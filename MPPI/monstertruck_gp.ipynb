{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d38533fa",
   "metadata": {},
   "source": [
    "# GP"
   ]
  },
  {
   "cell_type": "code",
   "id": "c826c79a",
   "metadata": {},
   "source": [
    "%pip install ipympl"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7496da55",
   "metadata": {},
   "source": [
    "# %matplotlib inline\n",
    "#%matplotlib widget\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ae0d6a1e",
   "metadata": {},
   "source": [
    "# Setup environment collect data"
   ]
  },
  {
   "cell_type": "code",
   "id": "c21e57c2",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------\n",
    "# MonsterTruck Flip — Environment setup (s = [phi_deg, phi_rate_deg])\n",
    "# --------------------------------------------------------------\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# If the class was defined in a previous cell, you don't need this import.\n",
    "# from monstertruck_env import MonsterTruckFlipEnvPitchSigned\n",
    "\n",
    "env = MonsterTruckFlipEnvPitchSigned(\n",
    "    xml_path=\"monstertruck.xml\",  # save your XML to this path\n",
    "    frame_skip=10,                # same step aggregation you used before\n",
    "    max_steps=2000,\n",
    "    render=False,                 # set True to open the GLFW viewer window\n",
    "    realtime=False,\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "obs, = (env.reset(),)  # initial observation: [phi_deg, phi_rate_deg]\n",
    "print(\"Initial obs:\", obs)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a0f2476c",
   "metadata": {},
   "source": [
    "# Dynamics from GP"
   ]
  },
  {
   "cell_type": "code",
   "id": "6b984c5a",
   "metadata": {},
   "source": [
    "# =========================================================\n",
    "# MountainCar: Phase plane with action color + dynamics field\n",
    "# - 2D (pos, vel) scatter colored by action\n",
    "# - Quiver arrows showing empirical mean (Δpos, Δvel) per bin\n",
    "# - Marginal histograms for position and velocity\n",
    "# =========================================================\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "SEED = 0\n",
    "TOTAL_STEPS = 100000\n",
    "BINS_POS = 35\n",
    "BINS_VEL = 35\n",
    "ARROW_STRIDE = 2        # thin-out arrows for readability\n",
    "USE_POLICY = [\"swing\", \"random\"]    # \"swing\" or \"random\"\n",
    "USE_BOTH_DATA = False\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "# ---------- simple policies ----------\n",
    "def random_policy(space):\n",
    "    low, high = space.low.item(), space.high.item()\n",
    "    return np.array([rng.uniform(low, high)], dtype=np.float32)\n",
    "\n",
    "def swing_up_policy(obs, space):\n",
    "    pos, vel = float(obs[0]), float(obs[1])\n",
    "    a = np.sign(vel)                       # push in direction of motion\n",
    "    #a = 0.95 * a + 0.15                    # slight right bias\n",
    "    low, high = space.low.item(), space.high.item()\n",
    "    return np.array([np.clip(a, low, high)], dtype=np.float32)\n",
    "\n",
    "# ---------- rollouts ----------\n",
    "def collect(n_steps=TOTAL_STEPS, policy=\"swing\"):\n",
    "    env = gym.make(\"MountainCarContinuous-v0\")\n",
    "    obs, _ = env.reset(seed=SEED)\n",
    "    pos, vel, act, dpos, dvel = [], [], [], [], []\n",
    "\n",
    "\n",
    "    if USE_BOTH_DATA:\n",
    "        for training in USE_POLICY:\n",
    "            for _ in range(n_steps):\n",
    "                if training==\"swing\":\n",
    "                    a = swing_up_policy(obs, env.action_space)\n",
    "                else:\n",
    "                    a = random_policy(env.action_space)\n",
    "                nxt, r, term, trunc, _ = env.step(a)\n",
    "\n",
    "                pos.append(float(obs[0])); vel.append(float(obs[1])); act.append(float(a[0]))\n",
    "                dpos.append(float(nxt[0] - obs[0])); dvel.append(float(nxt[1] - obs[1]))\n",
    "                obs = nxt\n",
    "                if term or trunc:\n",
    "                    obs, _ = env.reset()\n",
    "    else:\n",
    "        for _ in range(n_steps):\n",
    "            #a = swing_up_policy(obs, env.action_space)\n",
    "            a = random_policy(env.action_space)\n",
    "            nxt, r, term, trunc, _ = env.step(a)\n",
    "\n",
    "            pos.append(float(obs[0])); vel.append(float(obs[1])); act.append(float(a[0]))\n",
    "            dpos.append(float(nxt[0] - obs[0])); dvel.append(float(nxt[1] - obs[1]))\n",
    "            obs = nxt\n",
    "            if term or trunc:\n",
    "                obs, _ = env.reset()\n",
    "\n",
    "\n",
    "\n",
    "    env.close()\n",
    "    pos = np.asarray(pos); vel = np.asarray(vel); act = np.asarray(act)\n",
    "    dpos = np.asarray(dpos); dvel = np.asarray(dvel)\n",
    "    return pos, vel, act, dpos, dvel\n",
    "\n",
    "pos, vel, act, dpos, dvel = collect(policy=USE_POLICY)\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_axes([0.12, 0.12, 0.6, 0.6])\n",
    "\n",
    "norm = TwoSlopeNorm(vcenter=0.0, vmin=act.min(), vmax=act.max())\n",
    "pts = ax.scatter(pos, vel, c=act, s=12, cmap=\"coolwarm\", norm=norm)\n",
    "\n",
    "ax.set_xlabel(\"position\")\n",
    "ax.set_ylabel(\"velocity\")\n",
    "ax.set_title(\"MountainCar phase plane — action-colored points\")\n",
    "\n",
    "cb = plt.colorbar(pts, ax=ax)\n",
    "cb.set_label(\"action\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6877dd6a",
   "metadata": {},
   "source": [
    "# GP MODEL NEW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6eee1e",
   "metadata": {},
   "source": [
    "# CELL A"
   ]
  },
  {
   "cell_type": "code",
   "id": "330372d7",
   "metadata": {},
   "source": [
    "import numpy as np, torch, gpytorch\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# You already have these from your rollouts:\n",
    "# pos, vel, act, dpos, dvel  (all 1D numpy arrays of equal length)\n",
    "# If they are Python lists, np.asarray(...) is fine.\n",
    "\n",
    "X_np = np.column_stack([pos, vel, act]).astype(np.float32)   # (N,3)\n",
    "Y_np = np.column_stack([dpos, dvel]).astype(np.float32)      # (N,2)\n",
    "\n",
    "Xtr  = torch.tensor(X_np, device=device)\n",
    "Ytr  = torch.tensor(Y_np, device=device)\n",
    "\n",
    "# Per-dimension standardization (inputs & outputs)\n",
    "X_mu, X_std = Xtr.mean(0), Xtr.std(0).clamp_min(1e-6)\n",
    "Y_mu, Y_std = Ytr.mean(0), Ytr.std(0).clamp_min(1e-6)\n",
    "\n",
    "Xz = (Xtr - X_mu) / X_std              # (N,3)\n",
    "Yz = (Ytr - Y_mu) / Y_std              # (N,2)\n",
    "\n",
    "N = Xz.shape[0]\n",
    "print(\"N =\", N, \"| Xz:\", Xz.shape, \"| Yz:\", Yz.shape, \"| device:\", device)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "85e89a1a",
   "metadata": {},
   "source": [
    "# CELL B"
   ]
  },
  {
   "cell_type": "code",
   "id": "ce3a7391",
   "metadata": {},
   "source": [
    "import math\n",
    "from gpytorch.variational import (\n",
    "    VariationalStrategy, CholeskyVariationalDistribution, LMCVariationalStrategy\n",
    ")\n",
    "\n",
    "num_tasks   = 2      # Δpos, Δvel\n",
    "num_latents = 2      # latent GPs in LMC head (tune: 1–3)\n",
    "M           = 128    # inducing points (tune: 64–512)\n",
    "\n",
    "# Simple grid of inducing points in standardized space (you can KMeans instead)\n",
    "with torch.no_grad():\n",
    "    mins = Xz.min(0).values\n",
    "    maxs = Xz.max(0).values\n",
    "    g0 = torch.linspace(mins[0], maxs[0], int(M**(1/3))+2, device=device)\n",
    "    g1 = torch.linspace(mins[1], maxs[1], int(M**(1/3))+2, device=device)\n",
    "    g2 = torch.linspace(mins[2], maxs[2], int(M**(1/3))+2, device=device)\n",
    "    G  = torch.stack(torch.meshgrid(g0, g1, g2, indexing=\"ij\"), dim=-1).reshape(-1,3)\n",
    "    inducing_points = G[torch.linspace(0, len(G)-1, M).round().long()].contiguous()  # (M,3)\n",
    "\n",
    "# Model\n",
    "class LMC_MTSVGP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points, num_latents=2, num_tasks=2):\n",
    "        # batch over latents: (Q, M, D)\n",
    "        inducing_points = inducing_points.unsqueeze(0).expand(num_latents, -1, -1)\n",
    "        q_u = CholeskyVariationalDistribution(\n",
    "            inducing_points.size(-2),\n",
    "            batch_shape=torch.Size([num_latents])\n",
    "        )\n",
    "        base_vs = VariationalStrategy(\n",
    "            self, inducing_points, q_u, learn_inducing_locations=True\n",
    "        )\n",
    "        lmc_vs = LMCVariationalStrategy(\n",
    "            base_vs, num_tasks=num_tasks, num_latents=num_latents, latent_dim=-1\n",
    "        )\n",
    "        super().__init__(lmc_vs)\n",
    "\n",
    "        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([num_latents]))\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(batch_shape=torch.Size([num_latents])),\n",
    "            batch_shape=torch.Size([num_latents])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x  = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "model = LMC_MTSVGP(inducing_points.to(device), num_latents=num_latents, num_tasks=num_tasks).to(device)\n",
    "likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=num_tasks).to(device)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, \"M params (approx)\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "81afc81f",
   "metadata": {},
   "source": [
    "# CELL C"
   ]
  },
  {
   "cell_type": "code",
   "id": "469c3491",
   "metadata": {},
   "source": [
    "%pip install tqdm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "133e96e5",
   "metadata": {},
   "source": [
    "import tqdm\n",
    "\n",
    "model.train(); likelihood.train()\n",
    "\n",
    "opt = torch.optim.Adam([\n",
    "    {'params': model.parameters(), 'lr': 0.08},\n",
    "    {'params': likelihood.parameters(), 'lr': 0.05},\n",
    "])\n",
    "\n",
    "# Variational ELBO needs num_data for minibatches; here we do full-batch\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=Yz.size(0))\n",
    "\n",
    "E = 800   # epochs\n",
    "pbar = tqdm.tqdm(range(E))\n",
    "for e in pbar:\n",
    "    opt.zero_grad()\n",
    "    out = model(Xz)\n",
    "    loss = -mll(out, Yz)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if (e+1) % 100 == 0:\n",
    "        # Report per-task lengthscales / noise\n",
    "        ls = model.covar_module.base_kernel.lengthscale.detach().view(-1).mean().item()\n",
    "        noise = likelihood.task_noises.detach().mean().item() if hasattr(likelihood, \"task_noises\") else float('nan')\n",
    "        pbar.set_description(f\"ELBO {loss.item():.3f} | ls~{ls:.3f} | noise~{noise:.4f}\")\n",
    "\n",
    "model.eval(); likelihood.eval()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9d213f82",
   "metadata": {},
   "source": [
    "# CELL D"
   ]
  },
  {
   "cell_type": "code",
   "id": "66d6034c",
   "metadata": {},
   "source": [
    "@torch.no_grad()\n",
    "def gp_predict_delta(pos_batch, vel_batch, act_batch, sample=False):\n",
    "    \"\"\"\n",
    "    Inputs: 1D numpy or torch, same length (B,)\n",
    "    Returns:\n",
    "      mean_dpos, mean_dvel, std_dpos, std_dvel  (np arrays)\n",
    "      If sample=True, also returns one sampled (dpos, dvel) draw via rsample().\n",
    "    \"\"\"\n",
    "    if not torch.is_tensor(pos_batch):\n",
    "        Xb = torch.tensor(np.column_stack([pos_batch, vel_batch, act_batch]),\n",
    "                          dtype=torch.float32, device=device)\n",
    "    else:\n",
    "        Xb = torch.stack([pos_batch, vel_batch, act_batch], dim=-1).to(device).float()\n",
    "    Xb_z = (Xb - X_mu) / X_std\n",
    "\n",
    "    model.eval(); likelihood.eval()\n",
    "    with gpytorch.settings.fast_pred_var():\n",
    "        pred = likelihood(model(Xb_z))  # MultitaskMultivariateNormal with 2 outputs\n",
    "        mean = pred.mean                # (B, 2)\n",
    "        var  = pred.variance            # (B, 2)\n",
    "\n",
    "        # de-standardize\n",
    "        mean = mean * Y_std + Y_mu\n",
    "        std  = var.clamp_min(1e-12).sqrt() * Y_std\n",
    "\n",
    "        m_dp = mean[..., 0].detach().cpu().numpy()\n",
    "        m_dv = mean[..., 1].detach().cpu().numpy()\n",
    "        s_dp = std[..., 0].detach().cpu().numpy()\n",
    "        s_dv = std[..., 1].detach().cpu().numpy()\n",
    "\n",
    "        if not sample:\n",
    "            return m_dp, s_dp, m_dv, s_dv\n",
    "\n",
    "        # one reparameterized sample from the predictive (per point)\n",
    "        sample_mv = pred.rsample()                     # (B, 2)\n",
    "        sample_np = (sample_mv * Y_std + Y_mu).detach().cpu().numpy()\n",
    "        return m_dp, s_dp, m_dv, s_dv, sample_np[:,0], sample_np[:,1]\n",
    "\n",
    "def gp_step(state_batch, action_batch, dt=1.0, stochastic=False):\n",
    "    \"\"\"\n",
    "    Vectorized one-step model for MPPI:\n",
    "      state_batch: (B,2) [pos, vel]\n",
    "      action_batch: (B,) or (B,1)\n",
    "    Returns next_state mean (B,2) and optionally a sample (B,2) if stochastic=True.\n",
    "    \"\"\"\n",
    "    sb = torch.as_tensor(state_batch, device=device, dtype=torch.float32)\n",
    "    ab = torch.as_tensor(action_batch, device=device, dtype=torch.float32).view(-1)\n",
    "    pos_b, vel_b = sb[:,0], sb[:,1]\n",
    "\n",
    "    if stochastic:\n",
    "        m_dp, _, m_dv, _, s_dp_samp, s_dv_samp = gp_predict_delta(pos_b, vel_b, ab, sample=True)\n",
    "        dpos = s_dp_samp; dvel = s_dv_samp\n",
    "    else:\n",
    "        m_dp, _, m_dv, _ = gp_predict_delta(pos_b, vel_b, ab, sample=False)\n",
    "        dpos = m_dp; dvel = m_dv\n",
    "\n",
    "    nxt = np.stack([pos_b.detach().cpu().numpy() + dpos*dt,\n",
    "                    vel_b.detach().cpu().numpy() + dvel*dt], axis=-1)\n",
    "    return nxt\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a263382b",
   "metadata": {},
   "source": [
    "# CELL E"
   ]
  },
  {
   "cell_type": "code",
   "id": "ef3ecf11",
   "metadata": {},
   "source": [
    "# GP dynamics adapter for MPPI:\n",
    "# - Expects your trained multitask SVGP \"model\" and \"likelihood\"\n",
    "# - Uses your standardization stats: X_mu, X_std, Y_mu, Y_std  (from Cell A)\n",
    "# - Provides: step_batch(states, actions, stochastic=False)  -> next_states\n",
    "\n",
    "import numpy as np\n",
    "import torch, gpytorch\n",
    "\n",
    "class SVGPDynamics:\n",
    "    def __init__(self, model, likelihood, X_mu, X_std, Y_mu, Y_std, device=None):\n",
    "        self.model = model.eval()\n",
    "        self.lik   = likelihood.eval()\n",
    "        self.X_mu  = X_mu\n",
    "        self.X_std = X_std.clamp_min(1e-6)\n",
    "        self.Y_mu  = Y_mu\n",
    "        self.Y_std = Y_std.clamp_min(1e-6)\n",
    "        self.device = device or next(model.parameters()).device\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _predict_delta(self, Xq):  # Xq: (B,3) tensor on device\n",
    "        Xqz = (Xq - self.X_mu) / self.X_std\n",
    "        with gpytorch.settings.fast_pred_var():\n",
    "            post = self.lik(self.model(Xqz))              # multitask posterior\n",
    "        mean = post.mean * self.Y_std + self.Y_mu         # (B,2) de-standardized\n",
    "        var  = post.variance.clamp_min(1e-12) * (self.Y_std**2)\n",
    "        return mean, var\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step_batch(self, states, actions, stochastic=False, dt=1.0):\n",
    "        \"\"\"\n",
    "        states:  (B,2) [pos, vel]  (np or torch)\n",
    "        actions: (B,) or (B,1)     (np or torch)\n",
    "        returns next_states (B,2)  numpy\n",
    "        \"\"\"\n",
    "        if not torch.is_tensor(states):\n",
    "            S = torch.as_tensor(states,  dtype=torch.float32, device=self.device)\n",
    "        else:\n",
    "            S = states.to(self.device).float()\n",
    "        if not torch.is_tensor(actions):\n",
    "            A = torch.as_tensor(actions, dtype=torch.float32, device=self.device).view(-1, 1)\n",
    "        else:\n",
    "            A = actions.to(self.device).float().view(-1, 1)\n",
    "\n",
    "        Xq = torch.cat([S, A], dim=1)                     # (B,3)\n",
    "        mean, var = self._predict_delta(Xq)               # (B,2), (B,2)\n",
    "\n",
    "        if stochastic:\n",
    "            std = var.sqrt()\n",
    "            eps = torch.randn_like(std)\n",
    "            d = mean + eps * std                          # sampled delta\n",
    "        else:\n",
    "            d = mean                                      # mean delta\n",
    "\n",
    "        S_next = S + d * dt\n",
    "        return S_next.detach().cpu().numpy()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0b67ed84",
   "metadata": {},
   "source": [
    "# === Cell F — build the GP dynamics adapter to use inside GP_MPPI ===\n",
    "dyn = SVGPDynamics(\n",
    "    model, likelihood,      # trained multitask SVGP from Cells B–C\n",
    "    X_mu, X_std, Y_mu, Y_std,\n",
    "    device=device\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fe0e339a",
   "metadata": {},
   "source": [
    "# COST FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "id": "f30533bd",
   "metadata": {},
   "source": [
    "GOAL = 0.45\n",
    "NEAR_EPS = 0.03   # when x >= GOAL-NEAR_EPS, switch to \"hold mode\"\n",
    "\n",
    "def v_desired(p, vmax=0.06):\n",
    "    # swing-up bias when far: wants positive velocity toward goal\n",
    "    return vmax * np.tanh(5.0 * (GOAL - p))\n",
    "\n",
    "def running_cost_far(p, v, a):\n",
    "    # FAR zone: build momentum & go right\n",
    "    w_pos = 80.0\n",
    "    w_vel = 8.0\n",
    "    w_act = 0.12\n",
    "    pos_gap = np.maximum(0.0, GOAL - p)\n",
    "    v_err   = v - v_desired(p)\n",
    "    return w_pos*pos_gap**2 + w_vel*v_err**2 + w_act*a**2\n",
    "\n",
    "def running_cost_near(p, v, a):\n",
    "    # tighten these two knobs if you still overshoot\n",
    "    OVERSHOOT_BARRIER = 3_000.0   # barrier strength\n",
    "    OVERSHOOT_HARD    = 1_200.0   # quadratic one-sided penalty\n",
    "    \n",
    "    # Track x*=GOAL, v=0, a~0, and strongly punish overshoot (x > GOAL)\n",
    "    w_x   = 900.0\n",
    "    w_v   = 500.0\n",
    "    w_act = 0.05\n",
    "\n",
    "    # one-sided penalty only when x>GOAL\n",
    "    overshoot = np.maximum(0.0, p - GOAL)\n",
    "    quad_wall = OVERSHOOT_HARD * overshoot**2\n",
    "\n",
    "    # soft log barrier just beyond the goal (kicks in quickly after 0.45)\n",
    "    # add a small eps to avoid log(0)\n",
    "    eps = 1e-3\n",
    "    log_barrier = OVERSHOOT_BARRIER * np.log1p(overshoot/eps)\n",
    "\n",
    "    return w_x*(p-GOAL)**2 + w_v*(v**2) + w_act*(a**2) + quad_wall + log_barrier\n",
    "\n",
    "\n",
    "def step_cost(p, v, a):\n",
    "    # pick cost mode based on position\n",
    "    return np.where(p >= GOAL-NEAR_EPS,\n",
    "                    running_cost_near(p, v, a),\n",
    "                    running_cost_far(p, v, a))\n",
    "\n",
    "def terminal_cost(p):\n",
    "    # modest terminal bonus for arriving; main “stay” reward comes from NEAR cost\n",
    "    return np.where(p >= GOAL, -200.0, 0.0)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a1675f80",
   "metadata": {},
   "source": [
    "class GP_MPPI:\n",
    "    def __init__(self, horizon=80, n_samples=1024, lambda_=1.3,\n",
    "                 u_sigma=0.55, action_low=-1.0, action_high=1.0,\n",
    "                 sample_gp=False, seed=0, warm_start=True,\n",
    "                 smooth_weight=2.0,\n",
    "                 dynamics_adapter=None): \n",
    "        self.T = horizon\n",
    "        self.K = n_samples\n",
    "        self.lmb = lambda_\n",
    "        self.u_sigma_base = u_sigma\n",
    "        self.a_low, self.a_high = action_low, action_high\n",
    "        self.sample_gp = sample_gp\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.smooth_w = float(smooth_weight)\n",
    "        self.dyn = dynamics_adapter\n",
    "\n",
    "\n",
    "        self.U = np.zeros((self.T, 1), dtype=np.float32)\n",
    "        if warm_start:\n",
    "            # brief left push, then right push (helps swing-up)\n",
    "            self.U[:12, 0]  = -0.95\n",
    "            self.U[12:30, 0] = +0.95\n",
    "\n",
    "    def plan(self, s0):\n",
    "        p0, v0 = float(s0[0]), float(s0[1])\n",
    "\n",
    "        # reduce exploration if we are already near goal (helps holding)\n",
    "        near_now = (p0 >= GOAL - NEAR_EPS) and (abs(v0) < 0.02)\n",
    "        u_sigma  = self.u_sigma_base * (0.35 if near_now else 1.0)\n",
    "\n",
    "        eps = self.rng.normal(0.0, u_sigma, size=(self.K, self.T, 1)).astype(np.float32)\n",
    "        U_samples = np.clip(self.U[None, :, :] + eps, self.a_low, self.a_high)  # (K,T,1)\n",
    "\n",
    "        # rollout buffers\n",
    "        P = np.full((self.K,), p0, dtype=np.float32)\n",
    "        V = np.full((self.K,), v0, dtype=np.float32)\n",
    "        costs = np.zeros((self.K,), dtype=np.float32)\n",
    "\n",
    "        # previous action for smoothness (start from nominal u0)\n",
    "        u_prev = U_samples[:, 0, 0].copy()\n",
    "\n",
    "        # Track if/when we enter NEAR zone; after that, use NEAR cost\n",
    "        in_near = (P >= GOAL-NEAR_EPS)\n",
    "\n",
    "        for t in range(self.T):\n",
    "            At = U_samples[:, t, 0]                               # (K,)\n",
    "\n",
    "            # --- NEW: one batched GP step for all K rollouts at time t ---\n",
    "            assert self.dyn is not None, \"Pass dynamics_adapter=SVGPDynamics(...) to GP_MPPI\"\n",
    "            S_curr = np.stack([P, V], axis=1)                     # (K,2)\n",
    "            S_next = self.dyn.step_batch(S_curr, At,              # (K,2) next state\n",
    "                                        stochastic=self.sample_gp, dt=1.0)\n",
    "            P, V = S_next[:, 0].astype(np.float32), S_next[:, 1].astype(np.float32)\n",
    "\n",
    "\n",
    "            # once a trajectory is near, keep it in NEAR cost mode thereafter\n",
    "            in_near = np.logical_or(in_near, P >= GOAL-NEAR_EPS)\n",
    "\n",
    "            # running cost (zone-specific) + action smoothness\n",
    "            c_run = np.where(in_near, running_cost_near(P, V, At),\n",
    "                                       running_cost_far(P, V, At))\n",
    "            c_smooth = self.smooth_w * (At - u_prev)**2\n",
    "            costs += (c_run + c_smooth).astype(np.float32)\n",
    "\n",
    "            u_prev = At\n",
    "\n",
    "        # terminal bonus (small)\n",
    "        costs += terminal_cost(P).astype(np.float32)\n",
    "\n",
    "        # MPPI update\n",
    "        J = costs - costs.min()\n",
    "        w = np.exp(-J / self.lmb)\n",
    "        w /= (w.sum() + 1e-12)\n",
    "        dU = (w[:, None, None] * eps).sum(axis=0)                 # (T,1)\n",
    "        self.U = np.clip(self.U + dU, self.a_low, self.a_high)\n",
    "\n",
    "        u0 = float(self.U[0, 0])\n",
    "        self.U[:-1] = self.U[1:]\n",
    "        self.U[-1] = 0.0\n",
    "        return u0\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "34cd2978",
   "metadata": {},
   "source": [
    "# ---------- hold-at-goal wrapper ----------\n",
    "class HoldAtSetpointWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, x_star=0.45, pos_tol=0.001, v_tol=0.0003, hold_steps=12):\n",
    "        super().__init__(env)\n",
    "        self.x_star = float(x_star)\n",
    "        self.pos_tol = float(pos_tol)\n",
    "        self.v_tol = float(v_tol)\n",
    "        self.hold_steps_req = int(hold_steps)\n",
    "        self._hold = 0\n",
    "        # keep the original goal position for reference (Gym env stores it here)\n",
    "        self.goal_position = float(getattr(env.unwrapped, \"goal_position\", x_star))\n",
    "\n",
    "    def reset(self, **kw):\n",
    "        self._hold = 0\n",
    "        return self.env.reset(**kw)\n",
    "\n",
    "    def step(self, action):\n",
    "        s_next, r, terminated, truncated, info = self.env.step(action)\n",
    "        x, v = float(s_next[0]), float(s_next[1])\n",
    "        near = abs(x - self.x_star) <= self.pos_tol\n",
    "        slow = abs(v) <= self.v_tol\n",
    "        # count consecutive steps meeting both position + velocity tolerance\n",
    "        self._hold = self._hold + 1 if (near and slow) else 0\n",
    "        # only mark episode done if we've held for required steps\n",
    "        if terminated:\n",
    "            terminated = (self._hold >= self.hold_steps_req)\n",
    "        return s_next, r, terminated, truncated, info"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "76dd4f09",
   "metadata": {},
   "source": [
    "# === Live render in a separate window (frame + x/v/a) ===\n",
    "import sys\n",
    "# --- choose a GUI backend BEFORE importing pyplot ---\n",
    "import matplotlib\n",
    "try:\n",
    "    matplotlib.use(\"QtAgg\")   # needs PyQt5 or PySide6 installed\n",
    "except Exception:\n",
    "    matplotlib.use(\"TkAgg\")   # fallback if Qt isn't available\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# ---- goal band params (reuse your values) ----\n",
    "GOAL = 0.45\n",
    "NEAR_EPS = 0.03\n",
    "\n",
    "# ---- make env with rgb frames for Matplotlib window ----\n",
    "base_env = gym.make(\"MountainCarContinuous-v0\", render_mode=\"rgb_array\")\n",
    "env = HoldAtSetpointWrapper(base_env, x_star=GOAL, pos_tol=0.008, v_tol=0.003, hold_steps=12)\n",
    "obs, _ = env.reset(seed=0)\n",
    "\n",
    "# ---- controller (uses your GP_MPPI that calls dyn.step_batch) ----\n",
    "USE_STOCHASTIC_GP = False  # steadier holding near goal\n",
    "ctrl = GP_MPPI(\n",
    "    horizon=80, n_samples=128, lambda_=1.0,\n",
    "    u_sigma=0.6, action_low=-1.0, action_high=1.0,\n",
    "    sample_gp=USE_STOCHASTIC_GP, seed=0, warm_start=False,\n",
    "    smooth_weight=2.0, dynamics_adapter=dyn\n",
    ")\n",
    "\n",
    "# ---- figure: left = env frame, right = x/v/a time series ----\n",
    "plt.ion()  # interactive mode ON\n",
    "fig = plt.figure(figsize=(10, 10), dpi=120)\n",
    "gs  = fig.add_gridspec(nrows=3, ncols=2, width_ratios=[1.2, 1.5], height_ratios=[1,1,1])\n",
    "\n",
    "ax_img = fig.add_subplot(gs[:, 0])       # spans all rows\n",
    "ax_x   = fig.add_subplot(gs[0, 1])\n",
    "ax_v   = fig.add_subplot(gs[1, 1])\n",
    "ax_a   = fig.add_subplot(gs[2, 1])\n",
    "\n",
    "frame0 = env.render()\n",
    "im = ax_img.imshow(frame0, interpolation=\"nearest\")\n",
    "ax_img.set_axis_off()\n",
    "ax_img.set_title(\"MountainCar — window render (Matplotlib)\")\n",
    "\n",
    "# goal band and lines\n",
    "ax_x.axhspan(GOAL-NEAR_EPS, GOAL+NEAR_EPS, alpha=0.10)\n",
    "ax_x.axhline(GOAL, ls=\"--\", lw=1, c=\"k\")\n",
    "ax_v.axhline(0.0,   ls=\"--\", lw=1, c=\"k\")\n",
    "ax_a.axhline(0.0,   ls=\"--\", lw=1, c=\"k\")\n",
    "\n",
    "line_x, = ax_x.plot([], [], lw=1.8)\n",
    "line_v, = ax_v.plot([], [], lw=1.8)\n",
    "line_a, = ax_a.plot([], [], lw=1.8)\n",
    "ax_x.set_ylabel(\"position\")\n",
    "ax_v.set_ylabel(\"velocity\")\n",
    "ax_a.set_ylabel(\"action\"); ax_a.set_xlabel(\"timestep\")\n",
    "for ax in (ax_x, ax_v, ax_a):\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.show(block=False)  # open a separate window immediately\n",
    "\n",
    "# ---- rollout loop ----\n",
    "t_hist, x_hist, v_hist, a_hist, r_hist = [], [], [], [], []\n",
    "done, t, TMAX = False, 0, 600\n",
    "\n",
    "while not done and t < TMAX:\n",
    "    p, v = float(obs[0]), float(obs[1])\n",
    "    a_val = float(ctrl.plan((p, v)))\n",
    "    nxt, r, term, trunc, _ = env.step([a_val])\n",
    "    # done = term or trunc   # keep or restore if you want termination\n",
    "\n",
    "    # update histories\n",
    "    t_hist.append(t); x_hist.append(p); v_hist.append(v); a_hist.append(a_val); r_hist.append(r)\n",
    "    t_arr = np.asarray(t_hist)\n",
    "\n",
    "    # update plots\n",
    "    line_x.set_data(t_arr, np.asarray(x_hist))\n",
    "    line_v.set_data(t_arr, np.asarray(v_hist))\n",
    "    line_a.set_data(t_arr, np.asarray(a_hist))\n",
    "\n",
    "    # axes limits\n",
    "    ax_x.set_xlim(0, max(50, t))\n",
    "    ax_x.set_ylim(-1.2, 0.6)\n",
    "    ax_v.set_xlim(0, max(50, t))\n",
    "    ax_v.set_ylim(-0.1, 0.1)\n",
    "    ax_a.set_xlim(0, max(50, t))\n",
    "    ax_a.set_ylim(-1.1, 1.1)\n",
    "\n",
    "    # refresh env frame\n",
    "    im.set_data(env.render())\n",
    "    ax_x.set_title(f\"t={t}  x={p:+.3f}\")\n",
    "    ax_v.set_title(f\"v={v:+.3f}\")\n",
    "    ax_a.set_title(f\"a={a_val:+.3f}\")\n",
    "\n",
    "    # draw & process GUI events\n",
    "    fig.canvas.draw_idle()\n",
    "    fig.canvas.flush_events()\n",
    "    plt.pause(0.001)\n",
    "\n",
    "    obs = nxt\n",
    "    t += 1\n",
    "\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.close(fig)\n",
    "\n",
    "traj = np.column_stack([t_hist, x_hist, v_hist, a_hist, r_hist]).astype(np.float32)\n",
    "print(f\"Finished. Steps={len(traj)} | Reached/held goal≈{done} | Return≈{traj[:,4].sum():.2f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7883a5f7",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
