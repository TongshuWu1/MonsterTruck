{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================\n",
    "# Cell 1 — Imports + Custom Env (Continuous CartPole Swing-Up) + EDGE RESPAWN\n",
    "# + TensorFlow GPU setup (for GPflow/TF compute)\n",
    "# ============================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"pkg_resources is deprecated as an API.*\", category=UserWarning)\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from gymnasium.utils import seeding\n",
    "\n",
    "# ---- numpy bool8 compat ----\n",
    "if not hasattr(np, \"bool8\"):\n",
    "    np.bool8 = np.bool_\n",
    "\n",
    "# ============================================================\n",
    "# TensorFlow GPU setup (THIS is what makes TF/GPflow use GPU)\n",
    "# ============================================================\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")  # quieter TF logs (optional)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    # Prevent TF from grabbing all VRAM at startup\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(\"✅ TF version:\", tf.__version__)\n",
    "    print(\"✅ Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "    print(\"✅ GPUs:\", gpus)\n",
    "else:\n",
    "    print(\"⚠️ TF version:\", tf.__version__)\n",
    "    print(\"⚠️ Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "    print(\"⚠️ GPUs: [] (TensorFlow will run on CPU)\")\n",
    "\n",
    "# ============================================================\n",
    "# Dtypes for GPU-friendly pipeline\n",
    "# ============================================================\n",
    "# Env can stay float32; GP features should also be float32 to avoid slow casts.\n",
    "DTYPE_NP = np.float32\n",
    "\n",
    "# ============================================================\n",
    "# Angle helpers\n",
    "# ============================================================\n",
    "def wrap_pi(x):\n",
    "    return (x + np.pi) % (2 * np.pi) - np.pi\n",
    "\n",
    "def obs_to_state(obs):\n",
    "    \"\"\"\n",
    "    obs = [x, x_dot, theta, theta_dot]\n",
    "    Wrap theta to (-pi, pi] for stability.\n",
    "    \"\"\"\n",
    "    x, xdot, th, thdot = float(obs[0]), float(obs[1]), float(obs[2]), float(obs[3])\n",
    "    th = wrap_pi(th)\n",
    "    return x, xdot, th, thdot\n",
    "U_MIN, U_MAX = -1.0, 1.0\n",
    "\n",
    "def state_to_features(x, xdot, theta, thetadot, u,\n",
    "                      x_scale=2.4, v_scale=3.0, w_scale=8.0,\n",
    "                      dtype=DTYPE_NP):\n",
    "    \"\"\"\n",
    "    GP features (D=6), bounded:\n",
    "        [ tanh(x/x_scale),\n",
    "          tanh(xdot/v_scale),\n",
    "          sin(theta),\n",
    "          cos(theta),\n",
    "          tanh(thetadot/w_scale),\n",
    "          u ]\n",
    "    Returns float32 by default (GPU-friendly).\n",
    "    \"\"\"\n",
    "    x_feat = np.tanh(x / x_scale)\n",
    "    xdot_feat = np.tanh(xdot / v_scale)\n",
    "    w_feat = np.tanh(thetadot / w_scale)\n",
    "    return np.array(\n",
    "        [x_feat, xdot_feat, np.sin(theta), np.cos(theta), w_feat, float(u)],\n",
    "        dtype=dtype\n",
    "    )\n",
    "\n",
    "# ============================================================\n",
    "# Custom Continuous CartPole Swing-Up Env (CPU physics; that's OK)\n",
    "# ============================================================\n",
    "class ContinuousCartPoleSwingUpEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"rgb_array\", \"human\"], \"render_fps\": 50}\n",
    "\n",
    "    def __init__(self, render_mode=None, start_down=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # physics\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = self.masspole + self.masscart\n",
    "        self.length = 0.5\n",
    "        self.polemass_length = self.masspole * self.length\n",
    "\n",
    "        # control\n",
    "        self.force_mag = 30.0\n",
    "        self.tau = 0.02\n",
    "        self.min_action = -1.0\n",
    "        self.max_action = 1.0\n",
    "\n",
    "        # track limits\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        # reset mode\n",
    "        self.start_down = bool(start_down)\n",
    "\n",
    "        # render\n",
    "        self.render_mode = render_mode\n",
    "        self.state = None\n",
    "        self.np_random = None\n",
    "        self.seed()\n",
    "\n",
    "        # spaces\n",
    "        high = np.array(\n",
    "            [\n",
    "                self.x_threshold * 2,\n",
    "                np.finfo(np.float32).max,\n",
    "                np.finfo(np.float32).max,\n",
    "                np.finfo(np.float32).max,\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([self.min_action], dtype=np.float32),\n",
    "            high=np.array([self.max_action], dtype=np.float32),\n",
    "            shape=(1,),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def stepPhysics(self, force):\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "\n",
    "        temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (\n",
    "            self.length * (4.0 / 3.0 - self.masspole * costheta * costheta / self.total_mass)\n",
    "        )\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        x = x + self.tau * x_dot\n",
    "        x_dot = x_dot + self.tau * xacc\n",
    "        theta = theta + self.tau * theta_dot\n",
    "        theta_dot = theta_dot + self.tau * thetaacc\n",
    "        return (x, x_dot, theta, theta_dot)\n",
    "\n",
    "    def step(self, action):\n",
    "        action = np.asarray(action, dtype=np.float32).reshape(1,)\n",
    "        assert self.action_space.contains(action), f\"{action} invalid\"\n",
    "\n",
    "        u = float(action[0])\n",
    "        force = self.force_mag * u\n",
    "\n",
    "        self.state = self.stepPhysics(force)\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "\n",
    "        terminated = bool(x < -self.x_threshold or x > self.x_threshold)\n",
    "        truncated = False  # TimeLimit handles truncation\n",
    "\n",
    "        reward = (\n",
    "            +1.0 * math.cos(theta)\n",
    "            -0.01 * (x * x)\n",
    "            -0.001 * (x_dot * x_dot)\n",
    "            -0.001 * (theta_dot * theta_dot)\n",
    "            -0.001 * (u * u)\n",
    "        )\n",
    "\n",
    "        obs = np.array([x, x_dot, theta, theta_dot], dtype=np.float32)\n",
    "        info = dict(x=x, x_dot=x_dot, theta=theta, theta_dot=theta_dot, u=u)\n",
    "        return obs, float(reward), terminated, truncated, info\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "\n",
    "        x = float(self.np_random.uniform(low=-0.05, high=0.05))\n",
    "        x_dot = float(self.np_random.uniform(low=-0.05, high=0.05))\n",
    "        theta_dot = float(self.np_random.uniform(low=-0.05, high=0.05))\n",
    "\n",
    "        if self.start_down:\n",
    "            theta = float(math.pi + self.np_random.uniform(low=-0.10, high=0.10))\n",
    "        else:\n",
    "            theta = float(self.np_random.uniform(low=-0.10, high=0.10))\n",
    "\n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "        obs = np.array(self.state, dtype=np.float32)\n",
    "        info = {}\n",
    "        return obs, info\n",
    "\n",
    "    def render(self):\n",
    "        return None\n",
    "\n",
    "    def close(self):\n",
    "        return None\n",
    "\n",
    "# ============================================================\n",
    "# Edge respawn wrapper\n",
    "# ============================================================\n",
    "class EdgeRespawnWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, respawn_penalty=-2.0, reset_seed_mode=\"random\", seed=0):\n",
    "        super().__init__(env)\n",
    "        self.respawn_penalty = float(respawn_penalty)\n",
    "        self.reset_seed_mode = str(reset_seed_mode)\n",
    "        self._rng = np.random.default_rng(seed)\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            self._rng = np.random.default_rng(seed)\n",
    "        return self.env.reset(seed=seed, options=options)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        if terminated:\n",
    "            info = dict(info)\n",
    "            info[\"respawned\"] = True\n",
    "            reward = float(reward) + self.respawn_penalty\n",
    "\n",
    "            seed = int(self._rng.integers(0, 10**9)) if self.reset_seed_mode == \"random\" else None\n",
    "            obs, _ = self.env.reset(seed=seed)\n",
    "\n",
    "            terminated = False\n",
    "            truncated = False\n",
    "\n",
    "        return obs, float(reward), bool(terminated), bool(truncated), info\n",
    "\n",
    "# ============================================================\n",
    "# Environment factory\n",
    "# ============================================================\n",
    "MAX_EPISODE_STEPS = 600\n",
    "\n",
    "def make_env(\n",
    "    render_mode=None,\n",
    "    seed=0,\n",
    "    max_episode_steps=MAX_EPISODE_STEPS,\n",
    "    start_down=True,\n",
    "    edge_respawn=True,\n",
    "    respawn_penalty=-2.0,\n",
    "):\n",
    "    env = ContinuousCartPoleSwingUpEnv(render_mode=render_mode, start_down=start_down)\n",
    "    env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "    if edge_respawn:\n",
    "        env = EdgeRespawnWrapper(env, respawn_penalty=respawn_penalty, seed=seed)\n",
    "    env.reset(seed=seed)\n",
    "    return env\n",
    "\n",
    "# ============================================================\n",
    "# Sanity check\n",
    "# ============================================================\n",
    "env = make_env(render_mode=None, seed=0, start_down=True, edge_respawn=True)\n",
    "obs, _ = env.reset(seed=0)\n",
    "s = obs_to_state(obs)\n",
    "\n",
    "print(\"✅ Env ready (edge_respawn=True)\")\n",
    "print(\"obs:\", obs)\n",
    "print(\"state:\", s)\n",
    "print(\"action space:\", env.action_space)\n",
    "env.close()\n"
   ],
   "id": "6eb2d796ef722ca0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# ============================\n",
    "# Cell 2 — Render the RANDOM collection path (and collect X,Y)  ✅ CartPole version\n",
    "#\n",
    "# Fix:\n",
    "#   ✅ Adds render_cartpole_frame_from_state() (pure PIL) so the line:\n",
    "#        frame = render_cartpole_frame_from_state(...)\n",
    "#      actually works.\n",
    "#\n",
    "# What you get:\n",
    "#   - Runs random actions for n_steps (with resets as needed)\n",
    "#   - Collects executed transitions:\n",
    "#       X0: (N,6)  = [x_feat, xdot_feat, sinθ, cosθ, w_feat, u]\n",
    "#       Ydx0, Ydxdot0, Ydth0, Ydthdot0  (each (N,1))\n",
    "#   - Records frames (rgb) and displays an inline animation (JS HTML)\n",
    "#   - Plots trajectories:\n",
    "#       x(t), xdot(t), theta(t), thetadot(t), action(t)\n",
    "#       phase plot: x vs xdot\n",
    "#\n",
    "# IMPORTANT:\n",
    "#   - We DO NOT rely on env.render() / pygame.\n",
    "#   - We render from state using a self-contained PIL renderer below.\n",
    "#   - If your env does edge respawn and sets info[\"respawned\"]=True, we DROP that transition.\n",
    "# ============================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Pure-PIL renderer: (x, theta) -> RGB frame\n",
    "# ------------------------------------------------------------\n",
    "def render_cartpole_frame_from_state(\n",
    "    x, theta,\n",
    "    x_threshold=2.4,\n",
    "    W=720, H=450,\n",
    "    cart_width=70,\n",
    "    cart_height=35,\n",
    "    pole_length_px=160,\n",
    "    pole_width=10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Minimal CartPole render (rgb uint8) from state.\n",
    "    Coordinates:\n",
    "      - x in [-x_threshold, x_threshold] maps to screen track\n",
    "      - theta: 0 = upright, pi = down (standard)\n",
    "    \"\"\"\n",
    "    # background\n",
    "    img = Image.new(\"RGB\", (W, H), (245, 245, 245))\n",
    "    dr = ImageDraw.Draw(img)\n",
    "\n",
    "    # track line\n",
    "    y_track = int(0.72 * H)\n",
    "    dr.line([(0, y_track), (W, y_track)], fill=(210, 210, 210), width=4)\n",
    "\n",
    "    # map x -> screen px\n",
    "    # keep margins so cart stays visible\n",
    "    margin = 40\n",
    "    x_clamped = float(np.clip(x, -x_threshold, x_threshold))\n",
    "    x_norm = (x_clamped + x_threshold) / (2 * x_threshold)  # [0,1]\n",
    "    cx = int(margin + x_norm * (W - 2 * margin))\n",
    "    cy = y_track - cart_height // 2\n",
    "\n",
    "    # cart rectangle\n",
    "    x0 = cx - cart_width // 2\n",
    "    y0 = cy - cart_height // 2\n",
    "    x1 = cx + cart_width // 2\n",
    "    y1 = cy + cart_height // 2\n",
    "    dr.rounded_rectangle([x0, y0, x1, y1], radius=8, fill=(60, 90, 160), outline=(30, 30, 30), width=2)\n",
    "\n",
    "    # axle point on top of cart\n",
    "    ax = cx\n",
    "    ay = y0 + 6\n",
    "\n",
    "    # pole end point\n",
    "    # IMPORTANT: theta=0 is upright -> pole points UP, so use -cos/sin appropriately\n",
    "    # Standard cartpole uses theta from vertical; here we draw:\n",
    "    #   dx =  L * sin(theta)\n",
    "    #   dy = -L * cos(theta)\n",
    "    dx = pole_length_px * float(np.sin(theta))\n",
    "    dy = -pole_length_px * float(np.cos(theta))\n",
    "    px = ax + dx\n",
    "    py = ay + dy\n",
    "\n",
    "    # pole (as thick line)\n",
    "    dr.line([(ax, ay), (px, py)], fill=(180, 50, 50), width=pole_width)\n",
    "\n",
    "    # axle circle\n",
    "    r = 8\n",
    "    dr.ellipse([ax - r, ay - r, ax + r, ay + r], fill=(30, 30, 30))\n",
    "\n",
    "    return np.asarray(img, dtype=np.uint8)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Random collection (rendered)\n",
    "# ------------------------------------------------------------\n",
    "def collect_random_transitions_rendered_cartpole(\n",
    "    n_steps=500,\n",
    "    seed=0,\n",
    "    max_episode_steps=500,\n",
    "    start_down=True,\n",
    "    # rendering controls\n",
    "    frame_stride=2,\n",
    "    resize=(720, 450),\n",
    "    fps=20,\n",
    "    # edge-respawn handling (relies on your Cell 1 env.step() putting info[\"respawned\"]=True)\n",
    "    edge_respawn=True,\n",
    "    respawn_penalty=-2.0,\n",
    "    drop_respawn_transitions=True,\n",
    "    verbose=True,\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    env_vis = make_env(\n",
    "        render_mode=None,   # we render ourselves\n",
    "        seed=seed,\n",
    "        max_episode_steps=max_episode_steps,\n",
    "        start_down=start_down,\n",
    "        edge_respawn=edge_respawn,\n",
    "        respawn_penalty=respawn_penalty,\n",
    "    )\n",
    "\n",
    "    obs, info = env_vis.reset(seed=seed)\n",
    "    x, xdot, th, thdot = obs_to_state(obs)\n",
    "\n",
    "    X_list, Ydx_list, Ydxdot_list, Ydth_list, Ydthdot_list = [], [], [], [], []\n",
    "\n",
    "    traj_x, traj_xdot, traj_th, traj_thdot, traj_u = [], [], [], [], []\n",
    "    frames = []\n",
    "    dropped = 0\n",
    "    resets = 0\n",
    "\n",
    "    for t in range(n_steps):\n",
    "        u = float(rng.uniform(U_MIN, U_MAX))\n",
    "\n",
    "        obs2, reward, terminated, truncated, info = env_vis.step(np.array([u], dtype=np.float32))\n",
    "        x2, xdot2, th2, thdot2 = obs_to_state(obs2)\n",
    "\n",
    "        respawned = bool(isinstance(info, dict) and info.get(\"respawned\", False))\n",
    "        if respawned and drop_respawn_transitions:\n",
    "            dropped += 1\n",
    "        else:\n",
    "            X_list.append(state_to_features(x, xdot, th, thdot, u))\n",
    "            Ydx_list.append(x2 - x)\n",
    "            Ydxdot_list.append(xdot2 - xdot)\n",
    "            Ydth_list.append(wrap_pi(th2 - th))\n",
    "            Ydthdot_list.append(thdot2 - thdot)\n",
    "\n",
    "        traj_x.append(x)\n",
    "        traj_xdot.append(xdot)\n",
    "        traj_th.append(th)\n",
    "        traj_thdot.append(thdot)\n",
    "        traj_u.append(u)\n",
    "\n",
    "        if (t % frame_stride) == 0:\n",
    "            W, H = int(resize[0]), int(resize[1])\n",
    "            frame = render_cartpole_frame_from_state(x2, th2, x_threshold=2.4, W=W, H=H)\n",
    "            frames.append(frame)\n",
    "\n",
    "        x, xdot, th, thdot = x2, xdot2, th2, thdot2\n",
    "\n",
    "        if terminated or truncated:\n",
    "            resets += 1\n",
    "            obs, info = env_vis.reset(seed=int(seed + 123 + t))\n",
    "            x, xdot, th, thdot = obs_to_state(obs)\n",
    "            if verbose:\n",
    "                print(f\"[t={t:04d}] reset (terminated={terminated}, truncated={truncated})\")\n",
    "\n",
    "    env_vis.close()\n",
    "\n",
    "    X0 = np.asarray(X_list, dtype=np.float64)\n",
    "    Ydx0 = np.asarray(Ydx_list, dtype=np.float64).reshape(-1, 1)\n",
    "    Ydxdot0 = np.asarray(Ydxdot_list, dtype=np.float64).reshape(-1, 1)\n",
    "    Ydth0 = np.asarray(Ydth_list, dtype=np.float64).reshape(-1, 1)\n",
    "    Ydthdot0 = np.asarray(Ydthdot_list, dtype=np.float64).reshape(-1, 1)\n",
    "\n",
    "    traj = dict(\n",
    "        x=np.asarray(traj_x, dtype=np.float64),\n",
    "        xdot=np.asarray(traj_xdot, dtype=np.float64),\n",
    "        theta=np.asarray(traj_th, dtype=np.float64),\n",
    "        thetadot=np.asarray(traj_thdot, dtype=np.float64),\n",
    "        u=np.asarray(traj_u, dtype=np.float64),\n",
    "        dropped_respawn=int(dropped),\n",
    "        resets=int(resets),\n",
    "        steps=int(n_steps),\n",
    "        kept=int(X0.shape[0]),\n",
    "    )\n",
    "\n",
    "    # ----------------------------\n",
    "    # 1) show animation\n",
    "    # ----------------------------\n",
    "    if len(frames) > 0:\n",
    "        fig = plt.figure(figsize=(resize[0] / 100, resize[1] / 100), dpi=100)\n",
    "        plt.axis(\"off\")\n",
    "        im = plt.imshow(frames[0])\n",
    "\n",
    "        def animate_fn(i):\n",
    "            im.set_data(frames[i])\n",
    "            return [im]\n",
    "\n",
    "        ani = animation.FuncAnimation(\n",
    "            fig,\n",
    "            animate_fn,\n",
    "            frames=len(frames),\n",
    "            interval=1000 / float(fps),\n",
    "            blit=True\n",
    "        )\n",
    "        plt.close(fig)\n",
    "        display(HTML(ani.to_jshtml()))\n",
    "    else:\n",
    "        print(\"⚠️ No frames collected (check frame_stride / resize).\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # 2) trajectory plots\n",
    "    # ----------------------------\n",
    "    tgrid = np.arange(len(traj[\"x\"]))\n",
    "\n",
    "    plt.figure(figsize=(9, 3.8))\n",
    "    plt.plot(tgrid, traj[\"x\"], linewidth=2)\n",
    "    plt.xlabel(\"t\"); plt.ylabel(\"x\")\n",
    "    plt.title(\"Random collection: cart position x(t)\")\n",
    "    plt.grid(True, alpha=0.25); plt.show()\n",
    "\n",
    "    plt.figure(figsize=(9, 3.8))\n",
    "    plt.plot(tgrid, traj[\"xdot\"], linewidth=2)\n",
    "    plt.xlabel(\"t\"); plt.ylabel(\"xdot\")\n",
    "    plt.title(\"Random collection: cart velocity xdot(t)\")\n",
    "    plt.grid(True, alpha=0.25); plt.show()\n",
    "\n",
    "    plt.figure(figsize=(9, 3.8))\n",
    "    plt.plot(tgrid, traj[\"theta\"], linewidth=2)\n",
    "    plt.xlabel(\"t\"); plt.ylabel(\"theta (rad)\")\n",
    "    plt.title(\"Random collection: pole angle theta(t) (wrapped)\")\n",
    "    plt.grid(True, alpha=0.25); plt.show()\n",
    "\n",
    "    plt.figure(figsize=(9, 3.8))\n",
    "    plt.plot(tgrid, traj[\"thetadot\"], linewidth=2)\n",
    "    plt.xlabel(\"t\"); plt.ylabel(\"thetadot (rad/s)\")\n",
    "    plt.title(\"Random collection: pole angular velocity thetadot(t)\")\n",
    "    plt.grid(True, alpha=0.25); plt.show()\n",
    "\n",
    "    plt.figure(figsize=(9, 3.4))\n",
    "    plt.plot(tgrid, traj[\"u\"], linewidth=2)\n",
    "    plt.xlabel(\"t\"); plt.ylabel(\"u\")\n",
    "    plt.title(\"Random actions u(t)\")\n",
    "    plt.grid(True, alpha=0.25); plt.show()\n",
    "\n",
    "    plt.figure(figsize=(6.8, 5.2))\n",
    "    plt.scatter(traj[\"x\"], traj[\"xdot\"], s=10, alpha=0.55)\n",
    "    plt.xlabel(\"x\"); plt.ylabel(\"xdot\")\n",
    "    plt.title(\"Random collection path in state space (x vs xdot)\")\n",
    "    plt.grid(True, alpha=0.25); plt.show()\n",
    "\n",
    "    print(\"Collected X0 shape:\", X0.shape, \" (6D features)\")\n",
    "    print(\"Targets shapes:\", Ydx0.shape, Ydxdot0.shape, Ydth0.shape, Ydthdot0.shape)\n",
    "    print(f\"Kept={traj['kept']}  Dropped(respawn)={traj['dropped_respawn']}  Resets={traj['resets']}\")\n",
    "\n",
    "    return X0, Ydx0, Ydxdot0, Ydth0, Ydthdot0, frames, traj\n",
    "\n",
    "\n",
    "# ---- run it ----\n",
    "SEED = 0\n",
    "X0, Ydx0, Ydxdot0, Ydth0, Ydthdot0, frames0, traj0 = collect_random_transitions_rendered_cartpole(\n",
    "    n_steps=500,\n",
    "    seed=SEED,\n",
    "    max_episode_steps=500,\n",
    "    start_down=True,\n",
    "    frame_stride=2,\n",
    "    resize=(720, 450),\n",
    "    fps=20,\n",
    "    edge_respawn=True,\n",
    "    respawn_penalty=-2.0,\n",
    "    drop_respawn_transitions=True,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# Optional: quick distribution sanity\n",
    "import plotly.graph_objects as go\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=Ydth0.flatten(), nbinsx=60, name=\"Δtheta\"))\n",
    "fig.add_trace(go.Histogram(x=Ydthdot0.flatten(), nbinsx=60, name=\"Δthetadot\"))\n",
    "fig.update_layout(\n",
    "    title=\"Initial random dataset: distribution of Δtheta and Δtheta_dot\",\n",
    "    barmode=\"overlay\",\n",
    "    xaxis_title=\"delta value\",\n",
    "    yaxis_title=\"count\",\n",
    ")\n",
    "fig.update_traces(opacity=0.55)\n",
    "fig.show()\n"
   ],
   "id": "8595a10710cb7901",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ===========================\n",
    "# Cell 3 — OSGPR-VFE core (Streaming Sparse GP) + training + summaries + anchors\n",
    "#\n",
    "# Matches OUR pipeline:\n",
    "#   - GLOBAL sparse GP lives on an inducing set Z (usually Z_GLOBAL, size M, capped)\n",
    "#   - Each online update:\n",
    "#       (a) extract old posterior summary at Z_old\n",
    "#       (b) build a NEW OSGPR_VFE model on (X_new, Y_new) + old summary\n",
    "#       (c) train a bit, then cache for fast prediction\n",
    "#   - Anchors are selected FROM THE CURRENT inducing set Z (so you can reselect after updates)\n",
    "#\n",
    "# Provides:\n",
    "#   - batch_state_to_features(): (B,4)+(B,) -> (B,6)\n",
    "#   - OSGPR_VFE (single-output)\n",
    "#   - train_osgpr()\n",
    "#   - prior_summary(), extract_summary_from_model()\n",
    "#   - greedy_dopt_anchors_from_K()\n",
    "#   - rebuild_osgpr_from_old_summary(): returns (model_new, train_time, neg_obj)  ✅ fixes your unpack bug\n",
    "# ===========================\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gpflow\n",
    "\n",
    "from gpflow.inducing_variables import InducingPoints\n",
    "from gpflow.models import GPModel, InternalDataTrainingLossMixin\n",
    "from gpflow import covariances\n",
    "\n",
    "# ---- numerics ----\n",
    "gpflow.config.set_default_float(np.float64)\n",
    "gpflow.config.set_default_jitter(1e-6)\n",
    "tf.keras.backend.set_floatx(\"float64\")\n",
    "\n",
    "print(\"TF built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "try:\n",
    "    print(\"GPUs visible:\", tf.config.list_physical_devices(\"GPU\"))\n",
    "except Exception as e:\n",
    "    print(\"GPU query failed:\", e)\n",
    "\n",
    "DTYPE = gpflow.default_float()\n",
    "\n",
    "# ---------------------------\n",
    "# helpers\n",
    "# ---------------------------\n",
    "def sym_jitter(A, jitter=1e-6):\n",
    "    \"\"\"Make symmetric + add jitter (numpy).\"\"\"\n",
    "    A = np.asarray(A, dtype=np.float64)\n",
    "    A = 0.5 * (A + A.T)\n",
    "    A = A + float(jitter) * np.eye(A.shape[0], dtype=np.float64)\n",
    "    return A\n",
    "\n",
    "def finite_mask(*arrs):\n",
    "    \"\"\"Row-wise finite mask across arrays.\"\"\"\n",
    "    m = None\n",
    "    for a in arrs:\n",
    "        a = np.asarray(a)\n",
    "        mm = np.isfinite(a).all(axis=1) if a.ndim == 2 else np.isfinite(a)\n",
    "        m = mm if m is None else (m & mm)\n",
    "    return m\n",
    "\n",
    "def clone_kernel(kernel):\n",
    "    \"\"\"\n",
    "    Clone a GPflow kernel (to avoid variable-sharing across models).\n",
    "    gpflow.utilities.deepcopy exists in many versions; fallback to copy.deepcopy.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from gpflow.utilities import deepcopy as gf_deepcopy\n",
    "        return gf_deepcopy(kernel)\n",
    "    except Exception:\n",
    "        return copy.deepcopy(kernel)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Batch feature map (FAST) — used by MPPI later\n",
    "# ------------------------------------------------------------\n",
    "def batch_state_to_features(S, U, x_scale=2.4, v_scale=3.0, w_scale=8.0):\n",
    "    \"\"\"\n",
    "    Vectorized mapping from physical CartPole state to 6D GP features.\n",
    "\n",
    "    S: (B,4)  [x, xdot, theta, thetadot]\n",
    "    U: (B,)   action in [-1,1]\n",
    "    Returns:\n",
    "      Xfeat: (B,6) [tanh(x/xs), tanh(xdot/vs), sin(theta), cos(theta), tanh(thetadot/ws), u]\n",
    "    \"\"\"\n",
    "    S = np.asarray(S, dtype=np.float64)\n",
    "    U = np.asarray(U, dtype=np.float64).reshape(-1)\n",
    "    assert S.ndim == 2 and S.shape[1] == 4, \"S must be (B,4)\"\n",
    "    assert U.shape[0] == S.shape[0], \"U must match batch size\"\n",
    "\n",
    "    x     = S[:, 0]\n",
    "    xdot  = S[:, 1]\n",
    "    th    = S[:, 2]\n",
    "    thdot = S[:, 3]\n",
    "\n",
    "    Xf = np.empty((S.shape[0], 6), dtype=np.float64)\n",
    "    Xf[:, 0] = np.tanh(x / x_scale)\n",
    "    Xf[:, 1] = np.tanh(xdot / v_scale)\n",
    "    Xf[:, 2] = np.sin(th)\n",
    "    Xf[:, 3] = np.cos(th)\n",
    "    Xf[:, 4] = np.tanh(thdot / w_scale)\n",
    "    Xf[:, 5] = U\n",
    "    return Xf\n",
    "\n",
    "# ============================================================\n",
    "# OSGPR-VFE model — regression-only, single-output\n",
    "# ============================================================\n",
    "class OSGPR_VFE(GPModel, InternalDataTrainingLossMixin):\n",
    "    \"\"\"\n",
    "    Online Sparse Variational GP Regression (VFE), SINGLE-OUTPUT.\n",
    "\n",
    "    Provide:\n",
    "      - current batch data (X, Y)\n",
    "      - old summary q_old(u)=N(mu_old, Su_old) at Z_old\n",
    "      - Kaa_old = K(Z_old,Z_old) from old step\n",
    "      - new inducing Z (usually Z_GLOBAL; you MAY refresh Z over time, but size should be capped)\n",
    "\n",
    "    Includes:\n",
    "      - predict_f (correct but slower)\n",
    "      - build_predict_cache + predict_f_cached (FAST diag predictions)\n",
    "    \"\"\"\n",
    "    def __init__(self, data, kernel, mu_old, Su_old, Kaa_old, Z_old, Z, mean_function=None):\n",
    "        X, Y = gpflow.models.util.data_input_to_tensor(data)\n",
    "        self.X, self.Y = X, Y\n",
    "\n",
    "        likelihood = gpflow.likelihoods.Gaussian()\n",
    "        num_latent_gps = GPModel.calc_num_latent_gps_from_data(data, kernel, likelihood)\n",
    "        super().__init__(kernel, likelihood, mean_function, num_latent_gps)\n",
    "\n",
    "        Z = np.asarray(Z, dtype=np.float64)\n",
    "        assert Z.ndim == 2, \"Z must be (M, D)\"\n",
    "        self.inducing_variable = InducingPoints(Z)\n",
    "        gpflow.set_trainable(self.inducing_variable, False)\n",
    "        mu_old  = np.asarray(mu_old, dtype=np.float64).reshape(-1, 1)\n",
    "        Su_old  = sym_jitter(Su_old, 1e-6)\n",
    "        Kaa_old = sym_jitter(Kaa_old, 1e-6)\n",
    "        Z_old   = np.asarray(Z_old, dtype=np.float64)\n",
    "\n",
    "        self.mu_old  = tf.Variable(mu_old,  trainable=False, dtype=DTYPE)\n",
    "        self.Su_old  = tf.Variable(Su_old,  trainable=False, dtype=DTYPE)\n",
    "        self.Kaa_old = tf.Variable(Kaa_old, trainable=False, dtype=DTYPE)\n",
    "        self.Z_old   = tf.Variable(Z_old,   trainable=False, dtype=DTYPE)\n",
    "\n",
    "        if self.mean_function is None:\n",
    "            self.mean_function = gpflow.mean_functions.Zero()\n",
    "\n",
    "        # cache for fast predict\n",
    "        self._cache_ready = False\n",
    "        self._cache_Lb = None\n",
    "        self._cache_LD = None\n",
    "        self._cache_rhs = None\n",
    "\n",
    "    def _common_terms(self):\n",
    "        \"\"\"\n",
    "        Build common matrices used by both ELBO and prediction.\n",
    "\n",
    "        Z   : new inducing (Mb)\n",
    "        Za  : old inducing (Ma) == self.Z_old\n",
    "        X   : current batch inputs\n",
    "\n",
    "        Kbf = K(Z, X)    [Mb, N]\n",
    "        Kbb = K(Z, Z)    [Mb, Mb]\n",
    "        Kba = K(Z, Za)   [Mb, Ma]\n",
    "        \"\"\"\n",
    "        jitter = gpflow.utilities.to_default_float(1e-6)\n",
    "        sigma2 = self.likelihood.variance\n",
    "\n",
    "        Saa = self.Su_old  # [Ma,Ma]\n",
    "        ma  = self.mu_old  # [Ma,1]\n",
    "\n",
    "        Kbf = covariances.Kuf(self.inducing_variable, self.kernel, self.X)           # [Mb, N]\n",
    "        Kbb = covariances.Kuu(self.inducing_variable, self.kernel, jitter=jitter)   # [Mb, Mb]\n",
    "        Kba = covariances.Kuf(self.inducing_variable, self.kernel, self.Z_old)      # [Mb, Ma]\n",
    "\n",
    "        Kaa_cur = gpflow.utilities.add_noise_cov(self.kernel(self.Z_old), jitter)   # [Ma,Ma]\n",
    "        Kaa     = gpflow.utilities.add_noise_cov(self.Kaa_old, jitter)              # [Ma,Ma]\n",
    "\n",
    "        err = self.Y - self.mean_function(self.X)  # [N,1]\n",
    "\n",
    "        # c = Kbf*(Y/sigma2) + Kba*(Saa^{-1} ma)\n",
    "        Sainv_ma = tf.linalg.solve(Saa, ma)                                # [Ma,1]\n",
    "        c = tf.matmul(Kbf, self.Y / sigma2) + tf.matmul(Kba, Sainv_ma)     # [Mb,1]\n",
    "\n",
    "        # Cholesky(Kbb)\n",
    "        Lb = tf.linalg.cholesky(Kbb)                                       # [Mb,Mb]\n",
    "        Lbinv_c   = tf.linalg.triangular_solve(Lb, c,   lower=True)        # [Mb,1]\n",
    "        Lbinv_Kba = tf.linalg.triangular_solve(Lb, Kba, lower=True)        # [Mb,Ma]\n",
    "        Lbinv_Kbf = tf.linalg.triangular_solve(Lb, Kbf, lower=True) / tf.sqrt(sigma2)  # [Mb,N]\n",
    "\n",
    "        d1 = tf.matmul(Lbinv_Kbf, Lbinv_Kbf, transpose_b=True)             # [Mb,Mb]\n",
    "\n",
    "        # T = (Lb^{-1}Kba)^T  => [Ma,Mb]\n",
    "        T = tf.linalg.matrix_transpose(Lbinv_Kba)\n",
    "\n",
    "        # d2\n",
    "        LSa = tf.linalg.cholesky(Saa)\n",
    "        LSainv_T = tf.linalg.triangular_solve(LSa, T, lower=True)\n",
    "        d2 = tf.matmul(LSainv_T, LSainv_T, transpose_a=True)               # [Mb,Mb]\n",
    "\n",
    "        # d3\n",
    "        La = tf.linalg.cholesky(Kaa)\n",
    "        Lainv_T = tf.linalg.triangular_solve(La, T, lower=True)\n",
    "        d3 = tf.matmul(Lainv_T, Lainv_T, transpose_a=True)                 # [Mb,Mb]\n",
    "\n",
    "        Mb = self.inducing_variable.num_inducing\n",
    "        D = tf.eye(Mb, dtype=DTYPE) + d1 + d2 - d3\n",
    "        D = gpflow.utilities.add_noise_cov(D, jitter)\n",
    "        LD = tf.linalg.cholesky(D)\n",
    "\n",
    "        rhs = tf.linalg.triangular_solve(LD, Lbinv_c, lower=True)          # [Mb,1]\n",
    "\n",
    "        Qff_diag = tf.reduce_sum(tf.square(Lbinv_Kbf), axis=0)             # [N]\n",
    "\n",
    "        tf.debugging.assert_all_finite(Lb,  \"Lb has NaN/Inf\")\n",
    "        tf.debugging.assert_all_finite(LD,  \"LD has NaN/Inf\")\n",
    "        tf.debugging.assert_all_finite(rhs, \"rhs has NaN/Inf\")\n",
    "\n",
    "        return (Kbf, Kba, Kaa, Kaa_cur, La, Kbb, Lb, D, LD, Lbinv_Kba, rhs, err, Qff_diag)\n",
    "\n",
    "    def maximum_log_likelihood_objective(self):\n",
    "        sigma2 = self.likelihood.variance\n",
    "        N = tf.cast(tf.shape(self.X)[0], DTYPE)\n",
    "\n",
    "        Saa = self.Su_old\n",
    "        ma  = self.mu_old\n",
    "        Kfdiag = self.kernel(self.X, full_cov=False)\n",
    "\n",
    "        (Kbf, Kba, Kaa, Kaa_cur, La, Kbb, Lb, D, LD,\n",
    "         Lbinv_Kba, rhs, err, Qff_diag) = self._common_terms()\n",
    "\n",
    "        LSa = tf.linalg.cholesky(Saa)\n",
    "        Lainv_ma = tf.linalg.triangular_solve(LSa, ma, lower=True)\n",
    "\n",
    "        bound = -0.5 * N * np.log(2.0 * np.pi)\n",
    "        bound += -0.5 * tf.reduce_sum(tf.square(err)) / sigma2\n",
    "        bound += -0.5 * tf.reduce_sum(tf.square(Lainv_ma))\n",
    "        bound +=  0.5 * tf.reduce_sum(tf.square(rhs))\n",
    "\n",
    "        bound += -0.5 * N * tf.math.log(sigma2)\n",
    "        bound += -tf.reduce_sum(tf.math.log(tf.linalg.diag_part(LD)))\n",
    "\n",
    "        bound += -0.5 * tf.reduce_sum(Kfdiag) / sigma2\n",
    "        bound +=  0.5 * tf.reduce_sum(Qff_diag)\n",
    "\n",
    "        bound += tf.reduce_sum(tf.math.log(tf.linalg.diag_part(La)))\n",
    "        bound += -tf.reduce_sum(tf.math.log(tf.linalg.diag_part(LSa)))\n",
    "\n",
    "        # correction term involving Kaa_cur - Qaa\n",
    "        Kaadiff = Kaa_cur - tf.matmul(Lbinv_Kba, Lbinv_Kba, transpose_a=True)\n",
    "        Sainv_Kaadiff = tf.linalg.solve(Saa, Kaadiff)\n",
    "        Kainv_Kaadiff = tf.linalg.solve(Kaa, Kaadiff)\n",
    "\n",
    "        bound += -0.5 * tf.reduce_sum(\n",
    "            tf.linalg.diag_part(Sainv_Kaadiff) - tf.linalg.diag_part(Kainv_Kaadiff)\n",
    "        )\n",
    "        return bound\n",
    "\n",
    "    def predict_f(self, Xnew, full_cov=False):\n",
    "        jitter = gpflow.utilities.to_default_float(1e-6)\n",
    "\n",
    "        Kbs = covariances.Kuf(self.inducing_variable, self.kernel, Xnew)  # [Mb, Nnew]\n",
    "        (_, _, _, _, _, _, Lb, _, LD, _, rhs, _, _) = self._common_terms()\n",
    "\n",
    "        Lbinv_Kbs = tf.linalg.triangular_solve(Lb, Kbs, lower=True)\n",
    "        LDinv_Lbinv_Kbs = tf.linalg.triangular_solve(LD, Lbinv_Kbs, lower=True)\n",
    "        mean = tf.matmul(LDinv_Lbinv_Kbs, rhs, transpose_a=True)  # [Nnew,1]\n",
    "\n",
    "        if full_cov:\n",
    "            Kss = self.kernel(Xnew) + jitter * tf.eye(tf.shape(Xnew)[0], dtype=DTYPE)\n",
    "            var = (\n",
    "                Kss\n",
    "                - tf.matmul(Lbinv_Kbs, Lbinv_Kbs, transpose_a=True)\n",
    "                + tf.matmul(LDinv_Lbinv_Kbs, LDinv_Lbinv_Kbs, transpose_a=True)\n",
    "            )\n",
    "            return mean + self.mean_function(Xnew), var\n",
    "        else:\n",
    "            var = (\n",
    "                self.kernel(Xnew, full_cov=False)\n",
    "                - tf.reduce_sum(tf.square(Lbinv_Kbs), axis=0)\n",
    "                + tf.reduce_sum(tf.square(LDinv_Lbinv_Kbs), axis=0)\n",
    "            )\n",
    "            var = tf.maximum(var, tf.cast(1e-12, var.dtype))\n",
    "            return mean + self.mean_function(Xnew), var\n",
    "\n",
    "    def build_predict_cache(self):\n",
    "        \"\"\"Build cached matrices for fast predict_f_cached(). Call after training / after each update.\"\"\"\n",
    "        (_, _, _, _, _, _, Lb, _, LD, _, rhs, _, _) = self._common_terms()\n",
    "        self._cache_Lb = Lb\n",
    "        self._cache_LD = LD\n",
    "        self._cache_rhs = rhs\n",
    "        self._cache_ready = True\n",
    "\n",
    "    def predict_f_cached(self, Xnew, full_cov=False):\n",
    "        \"\"\"Fast diag prediction using cached Lb, LD, rhs.\"\"\"\n",
    "        if not self._cache_ready:\n",
    "            return self.predict_f(Xnew, full_cov=full_cov)\n",
    "\n",
    "        jitter = gpflow.utilities.to_default_float(1e-6)\n",
    "        Lb  = self._cache_Lb\n",
    "        LD  = self._cache_LD\n",
    "        rhs = self._cache_rhs\n",
    "\n",
    "        Kbs = covariances.Kuf(self.inducing_variable, self.kernel, Xnew)  # [Mb,Nnew]\n",
    "        Lbinv_Kbs = tf.linalg.triangular_solve(Lb, Kbs, lower=True)\n",
    "        LDinv_Lbinv_Kbs = tf.linalg.triangular_solve(LD, Lbinv_Kbs, lower=True)\n",
    "        mean = tf.matmul(LDinv_Lbinv_Kbs, rhs, transpose_a=True)\n",
    "\n",
    "        if full_cov:\n",
    "            Kss = self.kernel(Xnew) + jitter * tf.eye(tf.shape(Xnew)[0], dtype=DTYPE)\n",
    "            var = (\n",
    "                Kss\n",
    "                - tf.matmul(Lbinv_Kbs, Lbinv_Kbs, transpose_a=True)\n",
    "                + tf.matmul(LDinv_Lbinv_Kbs, LDinv_Lbinv_Kbs, transpose_a=True)\n",
    "            )\n",
    "            return mean + self.mean_function(Xnew), var\n",
    "        else:\n",
    "            var = (\n",
    "                self.kernel(Xnew, full_cov=False)\n",
    "                - tf.reduce_sum(tf.square(Lbinv_Kbs), axis=0)\n",
    "                + tf.reduce_sum(tf.square(LDinv_Lbinv_Kbs), axis=0)\n",
    "            )\n",
    "            var = tf.maximum(var, tf.cast(1e-12, var.dtype))\n",
    "            return mean + self.mean_function(Xnew), var\n",
    "\n",
    "# ----------------------------\n",
    "# training helper\n",
    "# ----------------------------\n",
    "def train_osgpr(model, iters=250, lr=0.02, clip_norm=10.0):\n",
    "    \"\"\"Adam optimize the negative ELBO.\"\"\"\n",
    "    opt = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "    @tf.function\n",
    "    def step():\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = -model.maximum_log_likelihood_objective()\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        if clip_norm is not None:\n",
    "            grads = [tf.clip_by_norm(g, clip_norm) if g is not None else None for g in grads]\n",
    "        opt.apply_gradients([(g, v) for g, v in zip(grads, model.trainable_variables) if g is not None])\n",
    "        return loss\n",
    "\n",
    "    t0 = time.time()\n",
    "    last = None\n",
    "    for _ in range(int(iters)):\n",
    "        last = step()\n",
    "    return float(time.time() - t0), float(last.numpy())\n",
    "\n",
    "# ----------------------------\n",
    "# summaries (to chain online)\n",
    "# ----------------------------\n",
    "def prior_summary(kernel, Z):\n",
    "    \"\"\"\n",
    "    Prior summary at inducing Z for the first model:\n",
    "      mu0 = 0\n",
    "      Su0 = Kzz\n",
    "      Kaa0 = Kzz\n",
    "    \"\"\"\n",
    "    Z = np.asarray(Z, dtype=np.float64)\n",
    "    Kzz = kernel.K(Z).numpy()\n",
    "    Kzz = sym_jitter(Kzz, 1e-6)\n",
    "    mu0 = np.zeros((Z.shape[0], 1), dtype=np.float64)\n",
    "    return mu0, Kzz, Kzz, Z\n",
    "\n",
    "def extract_summary_from_model(model):\n",
    "    \"\"\"\n",
    "    Extract q(u)=N(mu,Su) at model's current inducing Z plus Kaa=K(Z,Z).\n",
    "    \"\"\"\n",
    "    Z = model.inducing_variable.Z.numpy().astype(np.float64)\n",
    "\n",
    "    mu_tf, Sig_tf = model.predict_f(Z, full_cov=True)\n",
    "    mu = mu_tf.numpy().reshape(-1, 1)\n",
    "\n",
    "    Su = Sig_tf.numpy()\n",
    "    if Su.ndim == 3:\n",
    "        Su = Su[0]\n",
    "    Su = sym_jitter(Su, 1e-6)\n",
    "\n",
    "    Kaa = model.kernel.K(Z).numpy()\n",
    "    Kaa = sym_jitter(Kaa, 1e-6)\n",
    "    return mu, Su, Kaa, Z\n",
    "\n",
    "# ============================================================\n",
    "# Anchors: greedy D-opt (log-det) on Kzz\n",
    "# ============================================================\n",
    "def greedy_dopt_anchors_from_K(Kzz, m_anchors=24, lam=1e-6):\n",
    "    \"\"\"\n",
    "    Greedy log-det anchor selection on PSD Kzz using incremental Cholesky updates.\n",
    "    Returns indices of size m_anchors.\n",
    "    \"\"\"\n",
    "    K = np.asarray(Kzz, dtype=np.float64)\n",
    "    M = K.shape[0]\n",
    "    assert K.shape == (M, M)\n",
    "    K = sym_jitter(K, lam)\n",
    "\n",
    "    chosen = []\n",
    "    diag = np.clip(np.diag(K).copy(), 1e-12, None)\n",
    "    remaining = np.ones(M, dtype=bool)\n",
    "    L = None\n",
    "\n",
    "    for k in range(min(int(m_anchors), M)):\n",
    "        if k == 0:\n",
    "            i = int(np.argmax(diag))\n",
    "            chosen.append(i)\n",
    "            remaining[i] = False\n",
    "            L = np.array([[np.sqrt(diag[i])]], dtype=np.float64)\n",
    "            continue\n",
    "\n",
    "        S = np.array(chosen, dtype=np.int64)\n",
    "        Ks_all = K[np.ix_(S, np.arange(M))]     # (k,M)\n",
    "\n",
    "        v = np.linalg.solve(L, Ks_all)          # (k,M)\n",
    "        vn2 = np.sum(v * v, axis=0)             # (M,)\n",
    "        s2 = diag - vn2\n",
    "        s2 = np.where(remaining, s2, -np.inf)\n",
    "\n",
    "        i = int(np.argmax(s2))\n",
    "        if not np.isfinite(s2[i]) or s2[i] <= 1e-12:\n",
    "            cand = np.where(remaining)[0]\n",
    "            if len(cand) == 0:\n",
    "                break\n",
    "            i = int(cand[np.argmax(diag[cand])])\n",
    "            s2_i = max(diag[i], 1e-12)\n",
    "        else:\n",
    "            s2_i = float(s2[i])\n",
    "\n",
    "        chosen.append(i)\n",
    "        remaining[i] = False\n",
    "\n",
    "        kvec = K[np.ix_(S, [i])].reshape(-1, 1)  # (k,1)\n",
    "        w = np.linalg.solve(L, kvec)             # (k,1)\n",
    "        alpha = np.sqrt(max(s2_i, 1e-12))\n",
    "\n",
    "        L_new = np.zeros((k + 1, k + 1), dtype=np.float64)\n",
    "        L_new[:k, :k] = L\n",
    "        L_new[k, :k] = w.reshape(-1)\n",
    "        L_new[k, k] = alpha\n",
    "        L = L_new\n",
    "\n",
    "    return np.array(chosen, dtype=np.int64)\n",
    "\n",
    "# ============================================================\n",
    "# Online update builder (GLOBAL update step)\n",
    "# ============================================================\n",
    "def rebuild_osgpr_from_old_summary(\n",
    "    model_old,\n",
    "    X_new,\n",
    "    Y_new,\n",
    "    Z_new=None,\n",
    "    iters=120,\n",
    "    lr=0.02,\n",
    "    noise=1e-4,\n",
    "    freeze_kernel=False,\n",
    "    clip_norm=10.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a NEW OSGPR_VFE model using:\n",
    "      - old posterior summary extracted from model_old at its inducing Z_old\n",
    "      - new executed batch (X_new, Y_new)\n",
    "      - inducing set Z_new (defaults to model_old.Z; you may pass a refreshed Z here)\n",
    "\n",
    "    Returns:\n",
    "      model_new, train_time_sec, last_neg_obj\n",
    "    \"\"\"\n",
    "    # old summary\n",
    "    mu_old, Su_old, Kaa_old, Z_old = extract_summary_from_model(model_old)\n",
    "\n",
    "    # inducing set for the new model\n",
    "    if Z_new is None:\n",
    "        Z_use = Z_old\n",
    "    else:\n",
    "        Z_use = np.asarray(Z_new, dtype=np.float64)\n",
    "\n",
    "    # clone kernel to avoid variable-sharing surprises\n",
    "    k_new = clone_kernel(model_old.kernel)\n",
    "\n",
    "    m = OSGPR_VFE(\n",
    "        data=(np.asarray(X_new, dtype=np.float64), np.asarray(Y_new, dtype=np.float64)),\n",
    "        kernel=k_new,\n",
    "        mu_old=mu_old, Su_old=Su_old, Kaa_old=Kaa_old, Z_old=Z_old,\n",
    "        Z=Z_use,\n",
    "    )\n",
    "    m.likelihood.variance.assign(float(noise))\n",
    "\n",
    "    if freeze_kernel:\n",
    "        try:\n",
    "            m.kernel.variance.trainable = False\n",
    "            m.kernel.lengthscales.trainable = False\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    t_sec, neg = train_osgpr(m, iters=iters, lr=lr, clip_norm=clip_norm)\n",
    "    m.build_predict_cache()\n",
    "    return m, float(t_sec), float(neg)\n",
    "\n",
    "print(\"✅ OSGPR core + helpers ready (Cell 3 aligned to pipeline)\")\n"
   ],
   "id": "cfa6c764cdcd8d0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ===========================\n",
    "# Cell 4 — Train INITIAL GLOBAL OSGPR models (CartPole, 4 outputs)\n",
    "#   + GLOBAL evaluation plots (mean + uncertainty)\n",
    "#   + Anchor reselection utilities (for the new pipeline)\n",
    "#\n",
    "# New pipeline alignment:\n",
    "#   - Z_GLOBAL is FIXED capacity of global model\n",
    "#   - After EVERY global update, we will:\n",
    "#       (1) recompute ANCHOR_IDX from the UPDATED global model kernel on Z_GLOBAL\n",
    "#       (2) choose tube-based subset + anchors -> build local predictors for MPPI\n",
    "#\n",
    "# After training this cell gives:\n",
    "#   - m_dx, m_dxdot, m_dth, m_dthdot  (GLOBAL OSGPR models)\n",
    "#   - Z_GLOBAL  (fixed)\n",
    "#   - refresh_global_anchors()  -> updates ANCHOR_IDX using the current model kernel\n",
    "#   - Plot helpers: slice + surface for any model (mean & std)\n",
    "# ===========================\n",
    "\n",
    "import numpy as np\n",
    "import gpflow\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "SEED = 0\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Clean / finite filter\n",
    "# ----------------------------\n",
    "mask = finite_mask(X0, Ydx0, Ydxdot0, Ydth0, Ydthdot0)\n",
    "X0f       = X0[mask]\n",
    "Ydx0f     = Ydx0[mask]\n",
    "Ydxdot0f  = Ydxdot0[mask]\n",
    "Ydth0f    = Ydth0[mask]\n",
    "Ydthdot0f = Ydthdot0[mask]\n",
    "print(\"Data kept:\", X0f.shape[0], \"/\", X0.shape[0])\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Train/test split\n",
    "# ----------------------------\n",
    "N = X0f.shape[0]\n",
    "perm = rng.permutation(N)\n",
    "N_test = max(50, int(0.2 * N))\n",
    "test_idx  = perm[:N_test]\n",
    "train_idx = perm[N_test:]\n",
    "\n",
    "Xtr = X0f[train_idx]\n",
    "Xte = X0f[test_idx]\n",
    "\n",
    "Ydx_tr,     Ydx_te     = Ydx0f[train_idx],     Ydx0f[test_idx]\n",
    "Ydxdot_tr,  Ydxdot_te  = Ydxdot0f[train_idx],  Ydxdot0f[test_idx]\n",
    "Ydth_tr,    Ydth_te    = Ydth0f[train_idx],    Ydth0f[test_idx]\n",
    "Ydthdot_tr, Ydthdot_te = Ydthdot0f[train_idx], Ydthdot0f[test_idx]\n",
    "\n",
    "print(\"Train:\", Xtr.shape, \" Test:\", Xte.shape)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) GLOBAL inducing set (fixed capacity) -- FIXED\n",
    "# ----------------------------\n",
    "M_GLOBAL = 256  # Target capacity (128/256 recommended)\n",
    "N_avail  = Xtr.shape[0]\n",
    "\n",
    "if N_avail >= M_GLOBAL:\n",
    "    # Option A: We have enough data. Sample distinct points (standard).\n",
    "    idxZ = rng.choice(N_avail, size=M_GLOBAL, replace=False)\n",
    "    Z_GLOBAL = Xtr[idxZ].copy().astype(np.float64)\n",
    "else:\n",
    "    # Option B: Not enough data. Sample with replacement to fill buffer.\n",
    "    print(f\"⚠️ Warning: Train data ({N_avail}) < M_GLOBAL ({M_GLOBAL}). Sampling with replacement.\")\n",
    "    idxZ = rng.choice(N_avail, size=M_GLOBAL, replace=True)\n",
    "    Z_GLOBAL = Xtr[idxZ].copy().astype(np.float64)\n",
    "\n",
    "    # CRITICAL: Add jitter to duplicates.\n",
    "    # If Z contains exact duplicates, Kzz is singular -> Cholesky fails.\n",
    "    jitter_z = rng.standard_normal(Z_GLOBAL.shape) * 1e-6\n",
    "    Z_GLOBAL += jitter_z\n",
    "\n",
    "print(\"Z_GLOBAL:\", Z_GLOBAL.shape)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Kernels (6D ARD SE)\n",
    "# ----------------------------\n",
    "def make_kernel_6d():\n",
    "    # You can tune initial hyperparams; kernel may be frozen later in online loop for speed\n",
    "    return gpflow.kernels.SquaredExponential(\n",
    "        lengthscales=np.ones(6, dtype=np.float64),\n",
    "        variance=1.0\n",
    "    )\n",
    "\n",
    "k_dx     = make_kernel_6d()\n",
    "k_dxdot  = make_kernel_6d()\n",
    "k_dth    = make_kernel_6d()\n",
    "k_dthdot = make_kernel_6d()\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Build + train helper\n",
    "# ----------------------------\n",
    "def build_and_train_global_model(kernel, X, Y, Z_init, name,\n",
    "                                 iters=300, lr=0.02, noise=1e-4):\n",
    "    # first model uses prior summary at Z_GLOBAL\n",
    "    mu_old, Su_old, Kaa_old, Z_old = prior_summary(kernel, Z_init)\n",
    "\n",
    "    m = OSGPR_VFE(\n",
    "        data=(np.asarray(X, dtype=np.float64), np.asarray(Y, dtype=np.float64)),\n",
    "        kernel=kernel,\n",
    "        mu_old=mu_old, Su_old=Su_old, Kaa_old=Kaa_old, Z_old=Z_old,\n",
    "        Z=Z_init\n",
    "    )\n",
    "    m.likelihood.variance.assign(float(noise))\n",
    "\n",
    "    print(f\"\\nTraining {name} ...\")\n",
    "    t, neg = train_osgpr(m, iters=iters, lr=lr, clip_norm=10.0)\n",
    "    print(f\"{name} done | train={t:.2f}s | neg_obj={neg:.4f} | noise={float(m.likelihood.variance.numpy()):.2e}\")\n",
    "\n",
    "    m.build_predict_cache()\n",
    "    return m\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Train 4 global models\n",
    "# ----------------------------\n",
    "m_dx     = build_and_train_global_model(k_dx,     Xtr, Ydx_tr,     Z_GLOBAL, \"dx\",        iters=300, lr=0.02, noise=1e-4)\n",
    "m_dxdot  = build_and_train_global_model(k_dxdot,  Xtr, Ydxdot_tr,  Z_GLOBAL, \"dxdot\",     iters=300, lr=0.02, noise=1e-4)\n",
    "m_dth    = build_and_train_global_model(k_dth,    Xtr, Ydth_tr,    Z_GLOBAL, \"dtheta\",    iters=300, lr=0.02, noise=1e-4)\n",
    "m_dthdot = build_and_train_global_model(k_dthdot, Xtr, Ydthdot_tr, Z_GLOBAL, \"dthetadot\", iters=300, lr=0.02, noise=1e-4)\n",
    "\n",
    "print(\"\\n✅ Global models trained + caches ready\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Numeric eval (RMSE on held-out)\n",
    "# ----------------------------\n",
    "def rmse_on_test(model, Xte, Yte):\n",
    "    Xte = np.asarray(Xte, dtype=np.float64)\n",
    "    y = np.asarray(Yte, dtype=np.float64).reshape(-1)\n",
    "    if hasattr(model, \"predict_f_cached\"):\n",
    "        mu, _ = model.predict_f_cached(Xte, full_cov=False)\n",
    "    else:\n",
    "        mu, _ = model.predict_f(Xte, full_cov=False)\n",
    "    yhat = mu.numpy().reshape(-1)\n",
    "    return float(np.sqrt(np.mean((yhat - y) ** 2)))\n",
    "\n",
    "print(\"\\nHeld-out RMSE:\")\n",
    "print(\"  dx        :\", rmse_on_test(m_dx,     Xte, Ydx_te))\n",
    "print(\"  dxdot     :\", rmse_on_test(m_dxdot,  Xte, Ydxdot_te), \"   <-- Δxdot\")\n",
    "print(\"  dtheta    :\", rmse_on_test(m_dth,    Xte, Ydth_te))\n",
    "print(\"  dthetadot :\", rmse_on_test(m_dthdot, Xte, Ydthdot_te))\n",
    "\n",
    "# ============================================================\n",
    "# 7) Anchor reselection utilities (CRITICAL for new pipeline)\n",
    "# ============================================================\n",
    "def reselect_anchors_from_model(model, m_anchors=24, lam=1e-6):\n",
    "    \"\"\"\n",
    "    Anchors chosen on K(Z_GLOBAL,Z_GLOBAL) using the model's CURRENT kernel hyperparams.\n",
    "    This must be rerun after each global update if kernel changes OR if you want anchors to adapt.\n",
    "    \"\"\"\n",
    "    Z = model.inducing_variable.Z.numpy().astype(np.float64)\n",
    "    Kzz = model.kernel.K(Z).numpy()\n",
    "    Kzz = sym_jitter(Kzz, lam)\n",
    "    m_anchors = int(min(m_anchors, Z.shape[0]))\n",
    "    idx = greedy_dopt_anchors_from_K(Kzz, m_anchors=m_anchors, lam=lam)\n",
    "    return idx\n",
    "\n",
    "ANCHOR_M = min(24, int(Z_GLOBAL.shape[0]))\n",
    "ANCHOR_IDX = reselect_anchors_from_model(m_dxdot, m_anchors=ANCHOR_M, lam=1e-6)  # pick any of the 4 models\n",
    "print(\"ANCHOR_IDX:\", ANCHOR_IDX.shape, f\"(anchors={len(ANCHOR_IDX)})\")\n",
    "\n",
    "def refresh_global_anchors(m_anchors=24):\n",
    "    \"\"\"\n",
    "    Call this after global update (especially if you let kernel hyperparams move).\n",
    "    Uses m_dxdot by default; you can change to m_dx etc.\n",
    "    \"\"\"\n",
    "    global ANCHOR_IDX\n",
    "    ANCHOR_IDX = reselect_anchors_from_model(m_dxdot, m_anchors=m_anchors, lam=1e-6)\n",
    "    return ANCHOR_IDX\n",
    "\n",
    "# ============================================================\n",
    "# 8) Plot helpers (GLOBAL): mean + uncertainty\n",
    "# ============================================================\n",
    "def gp_predict_mu_std_fast(model, X):\n",
    "    X = np.asarray(X, dtype=np.float64)\n",
    "    if hasattr(model, \"predict_f_cached\"):\n",
    "        mu_tf, var_tf = model.predict_f_cached(X, full_cov=False)\n",
    "    else:\n",
    "        mu_tf, var_tf = model.predict_f(X, full_cov=False)\n",
    "    mu = mu_tf.numpy().reshape(-1)\n",
    "    var = var_tf.numpy().reshape(-1)\n",
    "    std = np.sqrt(np.maximum(var, 1e-12))\n",
    "    return mu, std\n",
    "\n",
    "def get_inducing_Z_np(model):\n",
    "    return model.inducing_variable.Z.numpy().astype(np.float64)\n",
    "\n",
    "# ---- Slice: sweep physical x, fixed xdot/theta/thetadot, compare u=+1/-1 ----\n",
    "def plot_slice_x_two_actions_cartpole(\n",
    "    model,\n",
    "    X_train, y_train,\n",
    "    title=\"Slice\",\n",
    "    y_label=\"Δy\",\n",
    "    xdot_fixed=0.0,\n",
    "    theta_fixed=0.0,\n",
    "    thetadot_fixed=0.0,\n",
    "    x_min=-2.4, x_max=2.4,\n",
    "    n_grid=260,\n",
    "    u_list=(+1.0, -1.0),\n",
    "    y_min=None, y_max=None,\n",
    "    x_tick_step=0.6,\n",
    "    y_tick_step=None,\n",
    "    show_data=True,\n",
    "    show_inducing=True,\n",
    "    tol_xdot_feat=0.12,\n",
    "    tol_theta=0.25,\n",
    "    tol_w_feat=0.12,\n",
    "    tol_u=0.10,\n",
    "):\n",
    "    x_grid = np.linspace(x_min, x_max, n_grid)\n",
    "    Z = get_inducing_Z_np(model)\n",
    "\n",
    "    curves = []\n",
    "    auto_ymin, auto_ymax = +np.inf, -np.inf\n",
    "\n",
    "    for u_fixed in u_list:\n",
    "        X_query = np.vstack([\n",
    "            state_to_features(x, xdot_fixed, theta_fixed, thetadot_fixed, u_fixed)\n",
    "            for x in x_grid\n",
    "        ]).astype(np.float64)\n",
    "        mu, std = gp_predict_mu_std_fast(model, X_query)\n",
    "        lo, hi = mu - 2 * std, mu + 2 * std\n",
    "        curves.append((u_fixed, mu, std, lo, hi))\n",
    "        auto_ymin, auto_ymax = min(auto_ymin, float(lo.min())), max(auto_ymax, float(hi.max()))\n",
    "\n",
    "    if y_min is None: y_min = auto_ymin\n",
    "    if y_max is None: y_max = auto_ymax\n",
    "\n",
    "    plt.figure(figsize=(9, 5))\n",
    "\n",
    "    X_train = np.asarray(X_train, dtype=np.float64)\n",
    "    y_train = np.asarray(y_train, dtype=np.float64).reshape(-1)\n",
    "\n",
    "    xdot_feat0 = np.tanh(xdot_fixed / 3.0)\n",
    "    w_feat0    = np.tanh(thetadot_fixed / 8.0)\n",
    "\n",
    "    for u_fixed, mu, std, lo, hi in curves:\n",
    "        plt.plot(x_grid, mu, lw=2.5, label=f\"mean (u={u_fixed:+.1f})\")\n",
    "        plt.fill_between(x_grid, lo, hi, alpha=0.18, label=f\"±2σ (u={u_fixed:+.1f})\")\n",
    "\n",
    "        if show_data:\n",
    "            th_train = np.arctan2(X_train[:, 2], X_train[:, 3])\n",
    "            mask = (\n",
    "                (np.abs(X_train[:, 1] - xdot_feat0) < tol_xdot_feat) &\n",
    "                (np.abs(wrap_pi(th_train - theta_fixed)) < tol_theta) &\n",
    "                (np.abs(X_train[:, 4] - w_feat0) < tol_w_feat) &\n",
    "                (np.abs(X_train[:, 5] - float(u_fixed)) < tol_u)\n",
    "            )\n",
    "            if np.sum(mask) > 0:\n",
    "                x_feat = np.clip(X_train[mask, 0], -0.999, 0.999)\n",
    "                x_phys = 2.4 * np.arctanh(x_feat)\n",
    "                plt.scatter(x_phys, y_train[mask], s=22, alpha=0.60,\n",
    "                            label=f\"data near slice (u≈{u_fixed:+.1f}, n={np.sum(mask)})\")\n",
    "\n",
    "        if show_inducing and (Z is not None):\n",
    "            thZ = np.arctan2(Z[:, 2], Z[:, 3])\n",
    "            maskZ = (\n",
    "                (np.abs(Z[:, 1] - xdot_feat0) < tol_xdot_feat) &\n",
    "                (np.abs(wrap_pi(thZ - theta_fixed)) < tol_theta) &\n",
    "                (np.abs(Z[:, 4] - w_feat0) < tol_w_feat) &\n",
    "                (np.abs(Z[:, 5] - float(u_fixed)) < tol_u)\n",
    "            )\n",
    "            if np.sum(maskZ) > 0:\n",
    "                Zsel = Z[maskZ]\n",
    "                muZ, _ = gp_predict_mu_std_fast(model, Zsel)\n",
    "                x_feat = np.clip(Zsel[:, 0], -0.999, 0.999)\n",
    "                x_phys = 2.4 * np.arctanh(x_feat)\n",
    "                plt.scatter(x_phys, muZ, marker=\"x\", s=70, linewidths=2.0,\n",
    "                            label=f\"inducing Z (u≈{u_fixed:+.1f}, M={np.sum(maskZ)})\")\n",
    "\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xlabel(\"Cart position x\")\n",
    "    plt.ylabel(y_label)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    if x_tick_step is not None:\n",
    "        ax.xaxis.set_major_locator(MultipleLocator(float(x_tick_step)))\n",
    "    if y_tick_step is not None:\n",
    "        ax.yaxis.set_major_locator(MultipleLocator(float(y_tick_step)))\n",
    "\n",
    "    plt.grid(True, alpha=0.25)\n",
    "    plt.title(title + f\"\\n(fixed: xdot={xdot_fixed:.2f}, theta={theta_fixed:.2f}, thetadot={thetadot_fixed:.2f})\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ---- 3D Surface: (x, xdot) -> mean Δ, colored by std ----\n",
    "def plot_surface_x_xdot_mean_colored_by_std(\n",
    "    model,\n",
    "    title=\"3D Surface (interactive)\",\n",
    "    z_label=\"Δy\",\n",
    "    theta_fixed=0.0,\n",
    "    thetadot_fixed=0.0,\n",
    "    u_fixed=+1.0,\n",
    "    x_min=-2.4, x_max=2.4,\n",
    "    xdot_min=-3.0, xdot_max=3.0,\n",
    "    n_grid=70,\n",
    "    show_inducing=True,\n",
    "    tol_theta=0.25,\n",
    "    tol_w_feat=0.12,\n",
    "    tol_u=0.10,\n",
    "):\n",
    "    x_grid = np.linspace(x_min, x_max, n_grid)\n",
    "    xd_grid = np.linspace(xdot_min, xdot_max, n_grid)\n",
    "    X, XD = np.meshgrid(x_grid, xd_grid)\n",
    "\n",
    "    X_feat = np.vstack([\n",
    "        state_to_features(x, xdot, theta_fixed, thetadot_fixed, u_fixed)\n",
    "        for x, xdot in zip(X.ravel(), XD.ravel())\n",
    "    ]).astype(np.float64)\n",
    "\n",
    "    mu, std = gp_predict_mu_std_fast(model, X_feat)\n",
    "    Mean = mu.reshape(X.shape)\n",
    "    Std  = std.reshape(X.shape)\n",
    "\n",
    "    surface = go.Surface(\n",
    "        x=X, y=XD, z=Mean,\n",
    "        surfacecolor=Std,\n",
    "        colorscale=\"Viridis\",\n",
    "        colorbar=dict(title=\"Std\"),\n",
    "        opacity=0.95,\n",
    "        showscale=True,\n",
    "        name=\"surface\"\n",
    "    )\n",
    "    traces = [surface]\n",
    "\n",
    "    if show_inducing:\n",
    "        Z = get_inducing_Z_np(model)\n",
    "        thZ = np.arctan2(Z[:, 2], Z[:, 3])\n",
    "        w_feat0 = np.tanh(thetadot_fixed / 8.0)\n",
    "        maskZ = (\n",
    "            (np.abs(wrap_pi(thZ - theta_fixed)) < tol_theta) &\n",
    "            (np.abs(Z[:, 4] - w_feat0) < tol_w_feat) &\n",
    "            (np.abs(Z[:, 5] - float(u_fixed)) < tol_u)\n",
    "        )\n",
    "        if np.sum(maskZ) > 0:\n",
    "            Zsel = Z[maskZ]\n",
    "            muZ, _ = gp_predict_mu_std_fast(model, Zsel)\n",
    "\n",
    "            x_feat = np.clip(Zsel[:, 0], -0.999, 0.999)\n",
    "            xdot_feat = np.clip(Zsel[:, 1], -0.999, 0.999)\n",
    "            x_phys = 2.4 * np.arctanh(x_feat)\n",
    "            xdot_phys = 3.0 * np.arctanh(xdot_feat)\n",
    "\n",
    "            traces.append(\n",
    "                go.Scatter3d(\n",
    "                    x=x_phys, y=xdot_phys, z=muZ,\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(size=4, color=\"red\", opacity=0.95),\n",
    "                    name=f\"inducing Z (near θ,θdot,u) | M={np.sum(maskZ)}\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "    fig = go.Figure(data=traces)\n",
    "    fig.update_layout(\n",
    "        title=f\"{title} | fixed u={u_fixed:+.1f}, theta={theta_fixed:.2f}, thetadot={thetadot_fixed:.2f}\",\n",
    "        scene=dict(\n",
    "            xaxis=dict(title=\"x\", range=[x_min, x_max]),\n",
    "            yaxis=dict(title=\"xdot\", range=[xdot_min, xdot_max]),\n",
    "            zaxis=dict(title=z_label),\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, b=0, t=50),\n",
    "        height=650\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "print(\"\\n✅ Cell 4 complete (trained global models + anchor refresh + plot helpers).\")"
   ],
   "id": "93b2207c4afbbe41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================\n",
    "# Cell 5 — Evaluate + Visualize GLOBAL GP (Reusable)\n",
    "#\n",
    "# What this cell provides:\n",
    "#   ✅ RMSE for dx, dxdot, dtheta, dthetadot on Xte\n",
    "#   ✅ Slice plot (mean ±2σ) for chosen output model (default: Δxdot)\n",
    "#   ✅ 3D Surface mean colored by std (default: Δxdot over x, xdot)\n",
    "#   ✅ NEW: 3D Surface of std alone (so you can SEE uncertainty shrink/grow)\n",
    "#\n",
    "# Assumes you ran:\n",
    "#   Cell 4 (trained global models + plot helpers already defined)\n",
    "#\n",
    "# Expected variables available:\n",
    "#   m_dx, m_dxdot, m_dth, m_dthdot\n",
    "#   Xte, Ydx_te, Ydxdot_te, Ydth_te, Ydthdot_te\n",
    "#   plot_slice_x_two_actions_cartpole()\n",
    "#   plot_surface_x_xdot_mean_colored_by_std()\n",
    "#   gp_predict_mu_std_fast()\n",
    "# ============================\n",
    "\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# ----------------------------\n",
    "# 0) RMSE helper\n",
    "# ----------------------------\n",
    "def rmse_np(yhat, y):\n",
    "    yhat = np.asarray(yhat).reshape(-1)\n",
    "    y = np.asarray(y).reshape(-1)\n",
    "    return float(np.sqrt(np.mean((yhat - y) ** 2)))\n",
    "\n",
    "# ----------------------------\n",
    "# 1) RMSE report\n",
    "# ----------------------------\n",
    "def print_global_rmse():\n",
    "    mu_dx, _     = gp_predict_mu_std_fast(m_dx,     Xte)\n",
    "    mu_dxdot, _  = gp_predict_mu_std_fast(m_dxdot,  Xte)\n",
    "    mu_dth, _    = gp_predict_mu_std_fast(m_dth,    Xte)\n",
    "    mu_dthdot, _ = gp_predict_mu_std_fast(m_dthdot, Xte)\n",
    "\n",
    "    print(\"=== Test RMSE (global models) ===\")\n",
    "    print(f\"dx        RMSE: {rmse_np(mu_dx,     Ydx_te):.6f}\")\n",
    "    print(f\"dxdot     RMSE: {rmse_np(mu_dxdot,  Ydxdot_te):.6f}   (Δxdot / delta v)\")\n",
    "    print(f\"dtheta    RMSE: {rmse_np(mu_dth,    Ydth_te):.6f}\")\n",
    "    print(f\"dthetadot RMSE: {rmse_np(mu_dthdot, Ydthdot_te):.6f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Std-only surface (uncertainty surface)\n",
    "# ----------------------------\n",
    "def plot_surface_x_xdot_std_only(\n",
    "    model,\n",
    "    title=\"3D Std surface (uncertainty only)\",\n",
    "    theta_fixed=0.0,\n",
    "    thetadot_fixed=0.0,\n",
    "    u_fixed=+1.0,\n",
    "    x_min=-2.4, x_max=2.4,\n",
    "    xdot_min=-3.0, xdot_max=3.0,\n",
    "    n_grid=70,\n",
    "):\n",
    "    x_grid = np.linspace(x_min, x_max, n_grid)\n",
    "    xd_grid = np.linspace(xdot_min, xdot_max, n_grid)\n",
    "    X, XD = np.meshgrid(x_grid, xd_grid)\n",
    "\n",
    "    X_feat = np.vstack([\n",
    "        state_to_features(x, xdot, theta_fixed, thetadot_fixed, u_fixed)\n",
    "        for x, xdot in zip(X.ravel(), XD.ravel())\n",
    "    ]).astype(np.float64)\n",
    "\n",
    "    _, std = gp_predict_mu_std_fast(model, X_feat)\n",
    "    Std = std.reshape(X.shape)\n",
    "\n",
    "    fig = go.Figure(\n",
    "        data=[\n",
    "            go.Surface(\n",
    "                x=X, y=XD, z=Std,\n",
    "                colorscale=\"Viridis\",\n",
    "                colorbar=dict(title=\"Std\"),\n",
    "                opacity=0.98,\n",
    "                showscale=True,\n",
    "                name=\"std surface\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        title=f\"{title} | fixed u={u_fixed:+.1f}, theta={theta_fixed:.2f}, thetadot={thetadot_fixed:.2f}\",\n",
    "        scene=dict(\n",
    "            xaxis=dict(title=\"x\", range=[x_min, x_max]),\n",
    "            yaxis=dict(title=\"xdot\", range=[xdot_min, xdot_max]),\n",
    "            zaxis=dict(title=\"std\"),\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, b=0, t=50),\n",
    "        height=650\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "# ----------------------------\n",
    "# 3) One-call global evaluation bundle (REUSABLE after updates)\n",
    "# ----------------------------\n",
    "def eval_and_plot_global(tag=\"GLOBAL (init)\",\n",
    "                         model_for_plots=None,\n",
    "                         theta_fixed=0.0,\n",
    "                         thetadot_fixed=0.0,\n",
    "                         u_fixed=+1.0):\n",
    "    \"\"\"\n",
    "    model_for_plots: choose which output model to visualize (default m_dxdot)\n",
    "    \"\"\"\n",
    "    if model_for_plots is None:\n",
    "        model_for_plots = m_dxdot\n",
    "\n",
    "    print(\"\\n==============================\")\n",
    "    print(f\"GLOBAL EVAL: {tag}\")\n",
    "    print(\"==============================\")\n",
    "    print_global_rmse()\n",
    "\n",
    "    # Slice: mean ±2σ, compare u=+1/-1\n",
    "    plot_slice_x_two_actions_cartpole(\n",
    "        model=model_for_plots,\n",
    "        X_train=Xtr,\n",
    "        y_train=Ydxdot_tr if (model_for_plots is m_dxdot) else Ydx_tr,\n",
    "        title=f\"{tag} slice: mean ±2σ (u=+1/-1) + inducing\",\n",
    "        y_label=\"Δxdot\" if (model_for_plots is m_dxdot) else \"Δx\",\n",
    "        xdot_fixed=0.0,\n",
    "        theta_fixed=theta_fixed,\n",
    "        thetadot_fixed=thetadot_fixed,\n",
    "        x_min=-2.4, x_max=2.4,\n",
    "        n_grid=260,\n",
    "        u_list=(+1.0, -1.0),\n",
    "    )\n",
    "\n",
    "    # Surface: mean colored by std (your current favorite)\n",
    "    plot_surface_x_xdot_mean_colored_by_std(\n",
    "        model=model_for_plots,\n",
    "        title=f\"{tag} surface: mean colored by std (+ inducing)\",\n",
    "        z_label=\"Δxdot\" if (model_for_plots is m_dxdot) else \"Δx\",\n",
    "        theta_fixed=theta_fixed,\n",
    "        thetadot_fixed=thetadot_fixed,\n",
    "        u_fixed=u_fixed,\n",
    "        x_min=-2.4, x_max=2.4,\n",
    "        xdot_min=-3.0, xdot_max=3.0,\n",
    "        n_grid=70,\n",
    "        show_inducing=True,\n",
    "    )\n",
    "\n",
    "    # NEW: std-only surface (uncertainty itself)\n",
    "    plot_surface_x_xdot_std_only(\n",
    "        model=model_for_plots,\n",
    "        title=f\"{tag} surface: std only (uncertainty)\",\n",
    "        theta_fixed=theta_fixed,\n",
    "        thetadot_fixed=thetadot_fixed,\n",
    "        u_fixed=u_fixed,\n",
    "        x_min=-2.4, x_max=2.4,\n",
    "        xdot_min=-3.0, xdot_max=3.0,\n",
    "        n_grid=70,\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# RUN ONCE FOR INITIAL GLOBAL\n",
    "# ----------------------------\n",
    "eval_and_plot_global(tag=\"GLOBAL (initial)\", model_for_plots=m_dxdot, u_fixed=+1.0)\n"
   ],
   "id": "e76267f1d5150253",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ===========================\n",
    "# Cell 6 — MPPI + Online OSGPR (FIXED global inducing) + Local subset (PALSGP-style)\n",
    "#   ✅ MULTI-RUN evaluation:\n",
    "#       - N_RUNS = 3\n",
    "#       - Each RUN has N_EPISODES_PER_RUN = 3 continuous episodes\n",
    "#       - Models KEEP training across episodes inside a run\n",
    "#       - BEFORE EACH RUN (including run 1): full reset to the same initial models + same initial Z_GLOBAL\n",
    "#\n",
    "#   ✅ Rendering:\n",
    "#       - ONLY 1 render per RUN (episode 0), others no render\n",
    "#       - BUT that 1 render is a SINGLE animation that stitches ALL episodes in the run\n",
    "#\n",
    "#   ✅ Timing curves (mean over runs):\n",
    "#       - training time per timestep (non-cumulative)\n",
    "#       - training time cumulative per timestep\n",
    "#       - prediction time per timestep (MPPI planning time)\n",
    "#       - wall time cumulative per timestep (EXCLUDING visualization)\n",
    "#\n",
    "#   ✅ Surfaces:\n",
    "#       - Plot surface after EACH RUN (global + local using last idx_sub)\n",
    "# ===========================\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML, display\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import gpflow\n",
    "import tensorflow as tf\n",
    "from scipy.stats import chi2\n",
    "from gpflow.utilities import parameter_dict, multiple_assign\n",
    "\n",
    "# ----------------------------\n",
    "# Enforce float64 (per your pipeline going forward)\n",
    "# ----------------------------\n",
    "try:\n",
    "    gpflow.config.set_default_float(np.float64)\n",
    "except Exception:\n",
    "    pass\n",
    "DTYPE_TF = gpflow.default_float()\n",
    "assert DTYPE_TF == tf.float64, f\"Expected float64 default, got {DTYPE_TF}. Set gpflow default float to float64 in earlier cells.\"\n",
    "\n",
    "# ----------------------------\n",
    "# GPU sanity / enforcement\n",
    "# ----------------------------\n",
    "REQUIRE_GPU = False\n",
    "LOG_DEVICE_PLACEMENT = False\n",
    "\n",
    "print(\"TF built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"GPUs visible:\", tf.config.list_physical_devices(\"GPU\"))\n",
    "print(\"Logical GPUs:\", tf.config.list_logical_devices(\"GPU\"))\n",
    "\n",
    "if LOG_DEVICE_PLACEMENT:\n",
    "    tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "if REQUIRE_GPU:\n",
    "    gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "    if len(gpus) == 0:\n",
    "        raise RuntimeError(\"REQUIRE_GPU=True but no GPU is visible to TensorFlow. Fix CUDA/TF install or set REQUIRE_GPU=False.\")\n",
    "\n",
    "# ----------------------------\n",
    "# Config (tune here)  (UNCHANGED: uses exactly your parameters)\n",
    "# ----------------------------\n",
    "HORIZON    = 40\n",
    "K_SAMPLES  = 128\n",
    "SIGMA      = 0.6\n",
    "LAMBDA     = 1.0\n",
    "\n",
    "UPDATE_EVERY = 30\n",
    "ITERS_UPDATE = 200\n",
    "LR_UPDATE    = 0.02\n",
    "NOISE_UPDATE = 1e-4\n",
    "\n",
    "M_GLOBAL = 256\n",
    "\n",
    "M_SUB           = 48\n",
    "DIVERSITY_FRAC  = 0.35\n",
    "\n",
    "ANCHOR_M   = 12\n",
    "ANCHOR_LAM = 1e-6\n",
    "\n",
    "X_BAND      = 0.60\n",
    "UPRIGHT_COS = 0.85\n",
    "HOLD_STEPS  = 200\n",
    "\n",
    "# Recording (visualization; excluded from runtime)\n",
    "RECORD_RGB_DEFAULT   = True\n",
    "RESIZE       = (720, 450)\n",
    "FPS          = 10\n",
    "FRAME_STRIDE = 2\n",
    "\n",
    "# Exploration schedule\n",
    "EXPLORE_STEPS = 1\n",
    "UNC_W_MAX     = 15.0\n",
    "UNC_W_MIN     = 0.0\n",
    "CENTER_W      = 1.0\n",
    "U_W           = 0.005\n",
    "UPRIGHT_W     = 2.0\n",
    "\n",
    "if \"U_MIN\" not in globals(): U_MIN = -1.0\n",
    "if \"U_MAX\" not in globals(): U_MAX = +1.0\n",
    "\n",
    "PLOT_EACH_UPDATE = False\n",
    "\n",
    "LOCAL_REBUILD_EVERY   = 10\n",
    "LOCAL_OVERLAP_THRESH  = 0.70\n",
    "\n",
    "TUBE_ALPHA      = 0.99\n",
    "TUBE_COV_EPS    = 1e-6\n",
    "TUBE_TIME_BINS  = 16\n",
    "TUBE_FALLBACK_CANDIDATES = 400\n",
    "\n",
    "STD_CMIN_FIXED = 0.0\n",
    "STD_FIXED_Q    = 0.5\n",
    "STD_MODE       = \"fixed\"\n",
    "STD_CMAX_FIXED = None\n",
    "\n",
    "PLOT_X_MIN, PLOT_X_MAX = -2.4, 2.4\n",
    "PLOT_V_MIN, PLOT_V_MAX = -3.0, 3.0\n",
    "PLOT_N_GRID            = 60\n",
    "PLOT_TH_FIXED          = 0.0\n",
    "PLOT_THDOT_FIXED       = 0.0\n",
    "PLOT_U_FIXED           = +1.0\n",
    "\n",
    "STORE_GLOBAL_SURFACES = True\n",
    "GLOBAL_SURF_HISTORY = []   # reset per RUN\n",
    "\n",
    "# ----------------------------\n",
    "# Safety: require dependencies from earlier cells\n",
    "# ----------------------------\n",
    "required = [\n",
    "    \"make_env\", \"obs_to_state\", \"wrap_pi\", \"state_to_features\",\n",
    "    \"batch_state_to_features\", \"OSGPR_VFE\", \"train_osgpr\",\n",
    "    \"extract_summary_from_model\", \"greedy_dopt_anchors_from_K\",\n",
    "    \"clone_kernel\",\n",
    "]\n",
    "missing = [k for k in required if k not in globals()]\n",
    "if len(missing) > 0:\n",
    "    raise NameError(f\"Cell 6 missing required symbols from earlier cells: {missing}\")\n",
    "\n",
    "if \"render_cartpole_frame_from_state\" not in globals():\n",
    "    print(\"⚠️ render_cartpole_frame_from_state not found. RGB recording will be disabled.\")\n",
    "    RECORD_RGB_DEFAULT = False\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: sym_jitter\n",
    "# ----------------------------\n",
    "def sym_jitter(A, jitter=1e-6):\n",
    "    A = np.asarray(A, dtype=np.float64)\n",
    "    A = 0.5 * (A + A.T)\n",
    "    return A + float(jitter) * np.eye(A.shape[0], dtype=np.float64)\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers: feature -> physical (for plots)\n",
    "# ----------------------------\n",
    "def feat_x_to_phys(x_feat, x_scale=2.4):\n",
    "    x_feat = np.asarray(x_feat, dtype=np.float64)\n",
    "    x_feat = np.clip(x_feat, -0.999, 0.999)\n",
    "    return x_scale * np.arctanh(x_feat)\n",
    "\n",
    "def feat_xdot_to_phys(xdot_feat, v_scale=3.0):\n",
    "    xdot_feat = np.asarray(xdot_feat, dtype=np.float64)\n",
    "    xdot_feat = np.clip(xdot_feat, -0.999, 0.999)\n",
    "    return v_scale * np.arctanh(xdot_feat)\n",
    "\n",
    "# ----------------------------\n",
    "# Precompute a fixed plot grid (same for all updates)\n",
    "# ----------------------------\n",
    "def build_fixed_plot_grid():\n",
    "    xg = np.linspace(PLOT_X_MIN, PLOT_X_MAX, PLOT_N_GRID)\n",
    "    vg = np.linspace(PLOT_V_MIN, PLOT_V_MAX, PLOT_N_GRID)\n",
    "    X, V = np.meshgrid(xg, vg)\n",
    "\n",
    "    Xfeat_grid = np.vstack([\n",
    "        state_to_features(x, xdot, PLOT_TH_FIXED, PLOT_THDOT_FIXED, PLOT_U_FIXED)\n",
    "        for x, xdot in zip(X.ravel(), V.ravel())\n",
    "    ]).astype(np.float64)\n",
    "    return X, V, Xfeat_grid\n",
    "\n",
    "PLOT_XMESH, PLOT_VMESH, PLOT_XFEAT_GRID = build_fixed_plot_grid()\n",
    "\n",
    "# ----------------------------\n",
    "# Global predict helpers (mean/std)\n",
    "# ----------------------------\n",
    "def gp_predict_mu_std_fast(model, X):\n",
    "    X = np.asarray(X, dtype=np.float64)\n",
    "    if hasattr(model, \"predict_f_cached\"):\n",
    "        mu_tf, var_tf = model.predict_f_cached(X, full_cov=False)\n",
    "    else:\n",
    "        mu_tf, var_tf = model.predict_f(X, full_cov=False)\n",
    "    mu = mu_tf.numpy().reshape(-1)\n",
    "    var = var_tf.numpy().reshape(-1)\n",
    "    std = np.sqrt(np.maximum(var, 1e-12))\n",
    "    return mu, std\n",
    "\n",
    "def pred_mean_cached(model, Xfeat):\n",
    "    Xfeat = np.asarray(Xfeat, dtype=np.float64)\n",
    "    if hasattr(model, \"predict_f_cached\"):\n",
    "        mu_tf, _ = model.predict_f_cached(Xfeat, full_cov=False)\n",
    "    else:\n",
    "        mu_tf, _ = model.predict_f(Xfeat, full_cov=False)\n",
    "    return mu_tf.numpy().reshape(-1)\n",
    "\n",
    "def make_global_predictors_bundle():\n",
    "    return (\n",
    "        lambda X: pred_mean_cached(m_dx, X),\n",
    "        lambda X: pred_mean_cached(m_dxdot, X),\n",
    "        lambda X: pred_mean_cached(m_dth, X),\n",
    "        lambda X: pred_mean_cached(m_dthdot, X),\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# Unified std scale utilities\n",
    "# ----------------------------\n",
    "def update_unified_std_scale(std_list, q=0.99, mode=\"fixed\"):\n",
    "    global STD_CMAX_FIXED\n",
    "    all_std = np.concatenate([np.asarray(s, dtype=np.float64).reshape(-1) for s in std_list if s is not None], axis=0)\n",
    "    if all_std.size == 0:\n",
    "        if STD_CMAX_FIXED is None:\n",
    "            STD_CMAX_FIXED = 1.0\n",
    "        return (STD_CMIN_FIXED, STD_CMAX_FIXED)\n",
    "\n",
    "    cand = float(np.quantile(all_std, q))\n",
    "    cand = max(cand, 1e-8)\n",
    "\n",
    "    if STD_CMAX_FIXED is None:\n",
    "        STD_CMAX_FIXED = cand\n",
    "    elif mode == \"grow_only\":\n",
    "        STD_CMAX_FIXED = max(STD_CMAX_FIXED, cand)\n",
    "\n",
    "    return (STD_CMIN_FIXED, STD_CMAX_FIXED)\n",
    "\n",
    "# ============================================================\n",
    "# CLEAN multi-head Z_GLOBAL refit\n",
    "# ============================================================\n",
    "def refit_Z_global_multihead(Z_global, Xnew, M_global, lam=1e-6, mode=\"mean\", normalize_traces=False):\n",
    "    Z_global = np.asarray(Z_global, dtype=np.float64)\n",
    "    Xnew = np.asarray(Xnew, dtype=np.float64)\n",
    "\n",
    "    Z_cand = np.vstack([Z_global, Xnew]).astype(np.float64, copy=False)\n",
    "    Zc_tf = tf.convert_to_tensor(Z_cand, dtype=tf.float64)\n",
    "\n",
    "    K_dx     = m_dx.kernel.K(Zc_tf).numpy().astype(np.float64, copy=False)\n",
    "    K_dxdot  = m_dxdot.kernel.K(Zc_tf).numpy().astype(np.float64, copy=False)\n",
    "    K_dth    = m_dth.kernel.K(Zc_tf).numpy().astype(np.float64, copy=False)\n",
    "    K_dthdot = m_dthdot.kernel.K(Zc_tf).numpy().astype(np.float64, copy=False)\n",
    "\n",
    "    if normalize_traces:\n",
    "        def _norm(K):\n",
    "            tr = float(np.trace(K))\n",
    "            return K / max(tr, 1e-12)\n",
    "        K_dx, K_dxdot, K_dth, K_dthdot = map(_norm, [K_dx, K_dxdot, K_dth, K_dthdot])\n",
    "\n",
    "    if mode == \"sum\":\n",
    "        K_agg = (K_dx + K_dxdot + K_dth + K_dthdot)\n",
    "    else:\n",
    "        K_agg = (K_dx + K_dxdot + K_dth + K_dthdot) / 4.0\n",
    "\n",
    "    idxZ = greedy_dopt_anchors_from_K(K_agg, m_anchors=int(M_global), lam=float(lam))\n",
    "    Z_new = np.asarray(Z_cand[np.asarray(idxZ, dtype=np.int64)], dtype=np.float64)\n",
    "\n",
    "    if Z_new.shape[0] != int(M_global):\n",
    "        Z_new = Z_new[:int(M_global)].copy()\n",
    "    return Z_new\n",
    "\n",
    "# ----------------------------\n",
    "# Online OSGPR update (Z fixed each update)\n",
    "# ----------------------------\n",
    "def osgpr_stream_update(model_old, X_new, Y_new, Z_new,\n",
    "                        iters=150, lr=0.02, noise=1e-4,\n",
    "                        freeze_kernel=False, clip_norm=10.0):\n",
    "    X_new = np.asarray(X_new, dtype=np.float64)\n",
    "    Y_new = np.asarray(Y_new, dtype=np.float64).reshape(-1, 1)\n",
    "\n",
    "    mu_old, Su_old, Kaa_old, Z_old = extract_summary_from_model(model_old)\n",
    "    Z_new = np.asarray(Z_new, dtype=np.float64)\n",
    "\n",
    "    k_new = clone_kernel(model_old.kernel)\n",
    "\n",
    "    m = OSGPR_VFE(\n",
    "        data=(X_new, Y_new),\n",
    "        kernel=k_new,\n",
    "        mu_old=mu_old, Su_old=Su_old, Kaa_old=Kaa_old, Z_old=Z_old,\n",
    "        Z=Z_new\n",
    "    )\n",
    "    m.likelihood.variance.assign(float(noise))\n",
    "\n",
    "    if freeze_kernel:\n",
    "        try:\n",
    "            m.kernel.variance.trainable = False\n",
    "            m.kernel.lengthscales.trainable = False\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    t_train, last_loss = train_osgpr(m, iters=iters, lr=lr, clip_norm=clip_norm)\n",
    "    m.build_predict_cache()\n",
    "\n",
    "    info = dict(\n",
    "        train_seconds=float(t_train),\n",
    "        last_neg_objective=float(last_loss),\n",
    "        M_new=int(m.inducing_variable.num_inducing),\n",
    "    )\n",
    "    return m, info\n",
    "\n",
    "# ============================================================\n",
    "# Anchors via aggregated kernel on Z_GLOBAL (multi-head)\n",
    "# ============================================================\n",
    "def compute_anchor_idx_dopt_from_Zglobal_multihead(Z_global, m_anchors=18, lam=1e-6, normalize_traces=True):\n",
    "    Z = np.asarray(Z_global, dtype=np.float64)\n",
    "    Ztf = tf.convert_to_tensor(Z, dtype=tf.float64)\n",
    "\n",
    "    K_dx     = m_dx.kernel.K(Ztf).numpy().astype(np.float64, copy=False)\n",
    "    K_dxdot  = m_dxdot.kernel.K(Ztf).numpy().astype(np.float64, copy=False)\n",
    "    K_dth    = m_dth.kernel.K(Ztf).numpy().astype(np.float64, copy=False)\n",
    "    K_dthdot = m_dthdot.kernel.K(Ztf).numpy().astype(np.float64, copy=False)\n",
    "\n",
    "    if normalize_traces:\n",
    "        def _norm(K):\n",
    "            tr = float(np.trace(K))\n",
    "            return K / max(tr, 1e-12)\n",
    "        K_dx, K_dxdot, K_dth, K_dthdot = map(_norm, [K_dx, K_dxdot, K_dth, K_dthdot])\n",
    "\n",
    "    K_agg = (K_dx + K_dxdot + K_dth + K_dthdot) / 4.0\n",
    "    return greedy_dopt_anchors_from_K(K_agg, m_anchors=int(m_anchors), lam=float(lam))\n",
    "\n",
    "# ----------------------------\n",
    "# Local subset predictor (NumPy SE-ARD) from global posterior at Z_sub\n",
    "# ----------------------------\n",
    "def se_ard_kernel_Kzx(Z, X, lengthscales, variance):\n",
    "    Z = np.asarray(Z, dtype=np.float64)\n",
    "    X = np.asarray(X, dtype=np.float64)\n",
    "    ls = np.asarray(lengthscales, dtype=np.float64).reshape(1, -1)\n",
    "    var = float(variance)\n",
    "\n",
    "    Zs = Z / ls\n",
    "    Xs = X / ls\n",
    "    z2 = np.sum(Zs * Zs, axis=1, keepdims=True)\n",
    "    x2 = np.sum(Xs * Xs, axis=1, keepdims=True).T\n",
    "    zx = Zs @ Xs.T\n",
    "    r2 = np.maximum(z2 + x2 - 2.0 * zx, 0.0)\n",
    "    return var * np.exp(-0.5 * r2)\n",
    "\n",
    "def build_subset_predictor_from_global(model_global, Z_global, idx_sub, jitter=1e-6):\n",
    "    idx_sub = np.asarray(idx_sub, dtype=np.int64)\n",
    "\n",
    "    Zg = np.asarray(Z_global, dtype=np.float64)\n",
    "    Z = Zg[idx_sub].copy()\n",
    "\n",
    "    mu_tf, Sig_tf = model_global.predict_f(Z, full_cov=True)\n",
    "    m = mu_tf.numpy().reshape(-1, 1)\n",
    "    S = Sig_tf.numpy()\n",
    "    if S.ndim == 3:\n",
    "        S = S[0]\n",
    "    S = sym_jitter(S, 1e-6)\n",
    "\n",
    "    ls  = model_global.kernel.lengthscales.numpy()\n",
    "    var = model_global.kernel.variance.numpy()\n",
    "\n",
    "    Kzz = se_ard_kernel_Kzx(Z, Z, ls, var)\n",
    "    Kzz = sym_jitter(Kzz, jitter=jitter)\n",
    "    L = np.linalg.cholesky(Kzz)\n",
    "\n",
    "    y = np.linalg.solve(L, m)\n",
    "    alpha = np.linalg.solve(L.T, y)\n",
    "\n",
    "    def predict(Xfeat):\n",
    "        Xfeat = np.asarray(Xfeat, dtype=np.float64)\n",
    "        Kzx = se_ard_kernel_Kzx(Z, Xfeat, ls, var)\n",
    "\n",
    "        W = np.linalg.solve(L, Kzx)\n",
    "        W = np.linalg.solve(L.T, W)\n",
    "\n",
    "        mu = (Kzx.T @ alpha).reshape(-1)\n",
    "\n",
    "        kxx = float(var) * np.ones((Xfeat.shape[0],), dtype=np.float64)\n",
    "        Qdiag = np.sum(Kzx * W, axis=0)\n",
    "        SW = S @ W\n",
    "        Sdiag = np.sum(W * SW, axis=0)\n",
    "\n",
    "        v = kxx - Qdiag + Sdiag\n",
    "        v = np.maximum(v, 1e-12)\n",
    "        return mu, v\n",
    "\n",
    "    return predict, Z\n",
    "\n",
    "def make_subset_predictors_bundle(idx_sub):\n",
    "    p_dx, _     = build_subset_predictor_from_global(m_dx,     Z_GLOBAL, idx_sub)\n",
    "    p_dxdot, _  = build_subset_predictor_from_global(m_dxdot,  Z_GLOBAL, idx_sub)\n",
    "    p_dth, _    = build_subset_predictor_from_global(m_dth,    Z_GLOBAL, idx_sub)\n",
    "    p_dthdot, _ = build_subset_predictor_from_global(m_dthdot, Z_GLOBAL, idx_sub)\n",
    "    return (\n",
    "        lambda X: p_dx(X)[0],\n",
    "        lambda X: p_dxdot(X)[0],\n",
    "        lambda X: p_dth(X)[0],\n",
    "        lambda X: p_dthdot(X)[0],\n",
    "    )\n",
    "\n",
    "def build_subset_uncertainty_fn(model_global, idx_sub):\n",
    "    pred, _ = build_subset_predictor_from_global(model_global, Z_GLOBAL, idx_sub)\n",
    "    def unc(Xfeat):\n",
    "        _, v = pred(Xfeat)\n",
    "        return v\n",
    "    return unc\n",
    "\n",
    "# ============================================================\n",
    "# Chi-square Mahalanobis tube selection helpers\n",
    "# ============================================================\n",
    "def normalize_nonnegative_weights(w, eps=1e-12):\n",
    "    w = np.asarray(w, dtype=np.float64).reshape(-1)\n",
    "    w = np.maximum(w, 0.0)\n",
    "    s = float(np.sum(w))\n",
    "    if s < eps:\n",
    "        return np.ones_like(w) / max(len(w), 1)\n",
    "    return w / s\n",
    "\n",
    "def compute_weighted_moments(rollout_inputs, rollout_weights):\n",
    "    X = np.asarray(rollout_inputs, dtype=np.float64)  # (K,H,D)\n",
    "    K, H, D = X.shape\n",
    "    w = normalize_nonnegative_weights(rollout_weights).reshape(K, 1, 1)\n",
    "\n",
    "    tube_mean = np.sum(w * X, axis=0)  # (H,D)\n",
    "    Xc = X - tube_mean[None, :, :]\n",
    "    tube_cov = np.einsum(\"khd,khe->hde\", (w * Xc), Xc)  # (H,D,D)\n",
    "    return tube_mean, tube_cov\n",
    "\n",
    "def min_mahalanobis_and_argmin(points_Z, tube_mean, tube_cov, cov_eps=1e-6):\n",
    "    Z = np.asarray(points_Z, dtype=np.float64)\n",
    "    mu = np.asarray(tube_mean, dtype=np.float64)\n",
    "    Sig = np.asarray(tube_cov, dtype=np.float64)\n",
    "\n",
    "    M, D = Z.shape\n",
    "    H = mu.shape[0]\n",
    "    I = np.eye(D, dtype=np.float64)\n",
    "\n",
    "    dmin = np.full((M,), np.inf, dtype=np.float64)\n",
    "    tmin = np.zeros((M,), dtype=np.int64)\n",
    "\n",
    "    for t in range(H):\n",
    "        St = Sig[t] + float(cov_eps) * I\n",
    "        L = np.linalg.cholesky(St)\n",
    "\n",
    "        diff = (Z - mu[t:t+1, :]).T  # (D,M)\n",
    "        y = np.linalg.solve(L, diff)\n",
    "        y2 = np.linalg.solve(L.T, y)\n",
    "        quad = np.sum(diff * y2, axis=0)  # (M,)\n",
    "\n",
    "        mask = quad < dmin\n",
    "        dmin[mask] = quad[mask]\n",
    "        tmin[mask] = t\n",
    "\n",
    "    return dmin, tmin\n",
    "\n",
    "def split_even_quota(total, num_bins):\n",
    "    num_bins = int(max(1, num_bins))\n",
    "    base = total // num_bins\n",
    "    rem = total - base * num_bins\n",
    "    q = np.full((num_bins,), base, dtype=np.int64)\n",
    "    if rem > 0:\n",
    "        q[:rem] += 1\n",
    "    return q\n",
    "\n",
    "def chi2_radius_sq(alpha, D):\n",
    "    alpha = float(alpha)\n",
    "    D = int(D)\n",
    "    alpha = min(max(alpha, 1e-6), 1.0 - 1e-12)\n",
    "    return float(chi2.ppf(alpha, df=D))\n",
    "\n",
    "def greedy_dopt_select_from_kernel(K, k, jitter=1e-6, log_eps=1e-300):\n",
    "    K = np.asarray(K, dtype=np.float64)\n",
    "    n = K.shape[0]\n",
    "    k = int(min(k, n))\n",
    "    if k <= 0:\n",
    "        return np.zeros((0,), dtype=np.int64)\n",
    "\n",
    "    diag = np.diag(K).copy() + float(jitter)\n",
    "    chosen = []\n",
    "    Lchol = None\n",
    "\n",
    "    for t in range(k):\n",
    "        if t == 0:\n",
    "            safe = np.maximum(diag, log_eps)\n",
    "            i = int(np.argmax(np.log(safe)))\n",
    "            chosen.append(i)\n",
    "            Lchol = np.sqrt(max(diag[i], log_eps)).reshape(1, 1)\n",
    "            continue\n",
    "\n",
    "        S = np.array(chosen, dtype=np.int64)\n",
    "\n",
    "        K_S = K[:, S]\n",
    "        v = np.linalg.solve(Lchol, K_S.T)\n",
    "        sq = np.sum(v * v, axis=0)\n",
    "        schur = diag - sq\n",
    "\n",
    "        schur[S] = -np.inf\n",
    "\n",
    "        schur_pos = np.maximum(schur, log_eps)\n",
    "        scores = np.log(schur_pos)\n",
    "        scores[S] = -np.inf\n",
    "\n",
    "        i = int(np.argmax(scores))\n",
    "        chosen.append(i)\n",
    "\n",
    "        k_iS = K[i, S].reshape(1, -1)\n",
    "        w = np.linalg.solve(Lchol, k_iS.T)\n",
    "        alpha2 = diag[i] - float(np.sum(w * w))\n",
    "        alpha2 = max(alpha2, log_eps)\n",
    "        alpha = np.sqrt(alpha2)\n",
    "\n",
    "        Lnew = np.zeros((t + 1, t + 1), dtype=np.float64)\n",
    "        Lnew[:t, :t] = Lchol\n",
    "        Lnew[t, :t] = w.reshape(-1)\n",
    "        Lnew[t, t] = alpha\n",
    "        Lchol = Lnew\n",
    "\n",
    "    return np.array(chosen, dtype=np.int64)\n",
    "\n",
    "def select_tube_subset(\n",
    "    Z_global,\n",
    "    kernel_for_dopt,\n",
    "    rollout_inputs,\n",
    "    rollout_weights,\n",
    "    total_subset_size=64,\n",
    "    anchor_idx=None,\n",
    "    time_bins=16,\n",
    "    cov_eps=1e-6,\n",
    "    dopt_jitter=1e-6,\n",
    "    fallback_candidates=400,\n",
    "    alpha=0.99,\n",
    "):\n",
    "    Zg = np.asarray(Z_global, dtype=np.float64)\n",
    "    M = Zg.shape[0]\n",
    "\n",
    "    if anchor_idx is None:\n",
    "        anchor_idx = np.zeros((0,), dtype=np.int64)\n",
    "    anchor_idx = np.unique(np.asarray(anchor_idx, dtype=np.int64))\n",
    "    anchor_idx = anchor_idx[(anchor_idx >= 0) & (anchor_idx < M)]\n",
    "\n",
    "    tube_mean, tube_cov = compute_weighted_moments(rollout_inputs, rollout_weights)\n",
    "    H, D = tube_mean.shape\n",
    "\n",
    "    chi2_thr = chi2_radius_sq(alpha, D)\n",
    "\n",
    "    dmin, tmin = min_mahalanobis_and_argmin(Zg, tube_mean, tube_cov, cov_eps=cov_eps)\n",
    "    tube_candidates_idx = np.where(dmin <= chi2_thr)[0].astype(np.int64)\n",
    "\n",
    "    remaining_budget = int(total_subset_size - anchor_idx.size)\n",
    "    remaining_budget = max(0, remaining_budget)\n",
    "\n",
    "    if tube_candidates_idx.size < remaining_budget:\n",
    "        L = int(min(max(fallback_candidates, remaining_budget), M))\n",
    "        tube_candidates_idx = np.argsort(dmin)[:L].astype(np.int64)\n",
    "\n",
    "    tube_pool = tube_candidates_idx[~np.isin(tube_candidates_idx, anchor_idx)]\n",
    "    tmin_pool = tmin[tube_pool]\n",
    "\n",
    "    selected = set(int(i) for i in anchor_idx)\n",
    "\n",
    "    if remaining_budget > 0 and tube_pool.size > 0:\n",
    "        B = int(max(1, min(time_bins, H)))\n",
    "        edges = np.linspace(0, H, B + 1, dtype=np.int64)\n",
    "        quotas = split_even_quota(remaining_budget, B)\n",
    "\n",
    "        for b in range(B):\n",
    "            if len(selected) >= total_subset_size:\n",
    "                break\n",
    "\n",
    "            a, c = int(edges[b]), int(edges[b + 1])\n",
    "            if c <= a:\n",
    "                continue\n",
    "\n",
    "            in_bin = (tmin_pool >= a) & (tmin_pool < c)\n",
    "            bin_pool = tube_pool[in_bin]\n",
    "            if bin_pool.size == 0:\n",
    "                continue\n",
    "\n",
    "            k_b = int(min(quotas[b], bin_pool.size, total_subset_size - len(selected)))\n",
    "            if k_b <= 0:\n",
    "                continue\n",
    "\n",
    "            Zb = Zg[bin_pool]\n",
    "            Kb = kernel_for_dopt.K(tf.convert_to_tensor(Zb, dtype=tf.float64)).numpy().astype(np.float64)\n",
    "            pick = greedy_dopt_select_from_kernel(Kb, k=k_b, jitter=dopt_jitter)\n",
    "\n",
    "            for gidx in bin_pool[pick]:\n",
    "                selected.add(int(gidx))\n",
    "\n",
    "    if len(selected) < total_subset_size:\n",
    "        need = int(total_subset_size - len(selected))\n",
    "        leftover = tube_pool[~np.isin(tube_pool, np.array(list(selected), dtype=np.int64))]\n",
    "        if leftover.size > 0 and need > 0:\n",
    "            Zl = Zg[leftover]\n",
    "            Kl = kernel_for_dopt.K(tf.convert_to_tensor(Zl, dtype=tf.float64)).numpy().astype(np.float64)\n",
    "            pick = greedy_dopt_select_from_kernel(Kl, k=min(need, leftover.size), jitter=dopt_jitter)\n",
    "            for gidx in leftover[pick]:\n",
    "                selected.add(int(gidx))\n",
    "\n",
    "    if len(selected) < total_subset_size:\n",
    "        need = int(total_subset_size - len(selected))\n",
    "        remaining = np.setdiff1d(np.arange(M, dtype=np.int64), np.array(list(selected), dtype=np.int64), assume_unique=False)\n",
    "        if remaining.size > 0:\n",
    "            order = remaining[np.argsort(dmin[remaining])]\n",
    "            for gidx in order[:need]:\n",
    "                selected.add(int(gidx))\n",
    "\n",
    "    subset_idx = np.array(sorted(selected), dtype=np.int64)[:total_subset_size]\n",
    "    return subset_idx, tube_mean, tube_cov, tube_candidates_idx, chi2_thr\n",
    "\n",
    "# ----------------------------\n",
    "# MPPI (NumPy) helpers (unchanged)\n",
    "# ----------------------------\n",
    "def exploration_weight(t, explore_steps=EXPLORE_STEPS, w_max=UNC_W_MAX, w_min=UNC_W_MIN):\n",
    "    if explore_steps <= 0:\n",
    "        return float(w_min)\n",
    "    a = np.clip(1.0 - float(t) / float(explore_steps), 0.0, 1.0)\n",
    "    return float(w_min + (w_max - w_min) * a)\n",
    "\n",
    "def stage_cost_cartpole(S, U, x_init, unc_bonus=None, unc_w=0.0):\n",
    "    th = S[:, 2]\n",
    "    x  = S[:, 0]\n",
    "    c = (\n",
    "        UPRIGHT_W * (1.0 - np.cos(th))\n",
    "        + CENTER_W * ((x - x_init) ** 2)\n",
    "        + U_W * (U ** 2)\n",
    "    )\n",
    "    if (unc_bonus is not None) and (unc_w > 0.0):\n",
    "        c = c - float(unc_w) * np.asarray(unc_bonus, dtype=np.float64)\n",
    "    return c\n",
    "\n",
    "def terminal_cost_hold_like(S, x_init):\n",
    "    th = S[:, 2]\n",
    "    x  = S[:, 0]\n",
    "    cT = 0.0 * th\n",
    "    good = (np.cos(th) >= UPRIGHT_COS) & (np.abs(x - x_init) <= X_BAND)\n",
    "    cT[good] -= 5.0\n",
    "    return cT\n",
    "\n",
    "def gp_dynamics_step_batch(S, U, pred_dx, pred_dxdot, pred_dth, pred_dthdot):\n",
    "    Xfeat = batch_state_to_features(S, U)\n",
    "    dx     = pred_dx(Xfeat)\n",
    "    dxdot  = pred_dxdot(Xfeat)\n",
    "    dth    = pred_dth(Xfeat)\n",
    "    dthdot = pred_dthdot(Xfeat)\n",
    "\n",
    "    S2 = np.empty_like(S)\n",
    "    S2[:, 0] = S[:, 0] + dx\n",
    "    S2[:, 1] = S[:, 1] + dxdot\n",
    "    S2[:, 2] = wrap_pi(S[:, 2] + dth)\n",
    "    S2[:, 3] = S[:, 3] + dthdot\n",
    "    return S2, Xfeat\n",
    "\n",
    "# ----------------------------\n",
    "# GPU MPPI (TF) — predictor uses EXPLICIT Z_GLOBAL (FIXED)\n",
    "# ----------------------------\n",
    "@tf.function\n",
    "def wrap_pi_tf(theta):\n",
    "    two_pi = tf.constant(2.0 * np.pi, dtype=DTYPE_TF)\n",
    "    pi = tf.constant(np.pi, dtype=DTYPE_TF)\n",
    "    return tf.math.floormod(theta + pi, two_pi) - pi\n",
    "\n",
    "@tf.function\n",
    "def batch_state_to_features_tf(S, U):\n",
    "    x = S[:, 0]\n",
    "    xdot = S[:, 1]\n",
    "    th = S[:, 2]\n",
    "    thdot = S[:, 3]\n",
    "    f0 = tf.tanh(x / tf.constant(2.4, dtype=DTYPE_TF))\n",
    "    f1 = tf.tanh(xdot / tf.constant(3.0, dtype=DTYPE_TF))\n",
    "    f2 = tf.sin(th)\n",
    "    f3 = tf.cos(th)\n",
    "    f4 = tf.tanh(thdot / tf.constant(8.0, dtype=DTYPE_TF))\n",
    "    f5 = U\n",
    "    return tf.stack([f0, f1, f2, f3, f4, f5], axis=1)\n",
    "\n",
    "@tf.function\n",
    "def se_ard_kernel_Kzx_tf(Z, X, lengthscales, variance):\n",
    "    ls = tf.reshape(lengthscales, (1, -1))\n",
    "    var = tf.cast(variance, DTYPE_TF)\n",
    "    Zs = Z / ls\n",
    "    Xs = X / ls\n",
    "    z2 = tf.reduce_sum(Zs * Zs, axis=1, keepdims=True)\n",
    "    x2 = tf.reduce_sum(Xs * Xs, axis=1, keepdims=True)\n",
    "    zx = tf.matmul(Zs, Xs, transpose_b=True)\n",
    "    r2 = tf.maximum(z2 + tf.transpose(x2) - 2.0 * zx, 0.0)\n",
    "    return var * tf.exp(-0.5 * r2)\n",
    "\n",
    "def build_subset_predictor_from_global_tf(model_global, Z_global, idx_sub):\n",
    "    idx_sub = tf.convert_to_tensor(np.asarray(idx_sub, dtype=np.int32))\n",
    "    Zg = tf.convert_to_tensor(np.asarray(Z_global, dtype=np.float64), dtype=DTYPE_TF)\n",
    "    Z  = tf.gather(Zg, idx_sub, axis=0)\n",
    "\n",
    "    muZ, SigZ = model_global.predict_f(Z, full_cov=True)\n",
    "    m = muZ\n",
    "    S = SigZ\n",
    "    if len(S.shape) == 3:\n",
    "        S = S[0]\n",
    "\n",
    "    ls  = tf.cast(model_global.kernel.lengthscales, DTYPE_TF)\n",
    "    var = tf.cast(model_global.kernel.variance, DTYPE_TF)\n",
    "\n",
    "    Kzz = se_ard_kernel_Kzx_tf(Z, Z, ls, var)\n",
    "    jitter = tf.cast(1e-6, DTYPE_TF)\n",
    "    Kzz = 0.5 * (Kzz + tf.transpose(Kzz)) + jitter * tf.eye(tf.shape(Kzz)[0], dtype=DTYPE_TF)\n",
    "    L = tf.linalg.cholesky(Kzz)\n",
    "\n",
    "    y = tf.linalg.triangular_solve(L, m, lower=True)\n",
    "    alpha = tf.linalg.triangular_solve(tf.transpose(L), y, lower=False)\n",
    "\n",
    "    @tf.function\n",
    "    def predict(Xfeat):\n",
    "        Xfeat = tf.cast(Xfeat, DTYPE_TF)\n",
    "        Kzx = se_ard_kernel_Kzx_tf(Z, Xfeat, ls, var)\n",
    "\n",
    "        w1 = tf.linalg.triangular_solve(L, Kzx, lower=True)\n",
    "        W  = tf.linalg.triangular_solve(tf.transpose(L), w1, lower=False)\n",
    "\n",
    "        mu = tf.reshape(tf.matmul(Kzx, alpha, transpose_a=True), (-1,))\n",
    "\n",
    "        kxx = tf.fill((tf.shape(Xfeat)[0],), tf.cast(var, DTYPE_TF))\n",
    "        Qdiag = tf.reduce_sum(Kzx * W, axis=0)\n",
    "        SW = tf.matmul(S, W)\n",
    "        Sdiag = tf.reduce_sum(W * SW, axis=0)\n",
    "\n",
    "        v = tf.maximum(kxx - Qdiag + Sdiag, tf.cast(1e-12, DTYPE_TF))\n",
    "        return mu, v\n",
    "\n",
    "    return predict, Z\n",
    "\n",
    "@tf.function\n",
    "def exploration_weight_tf(t):\n",
    "    explore_steps_f = tf.cast(EXPLORE_STEPS, DTYPE_TF)\n",
    "    t_f = tf.cast(t, DTYPE_TF)\n",
    "    a = tf.clip_by_value(1.0 - t_f / tf.maximum(explore_steps_f, 1.0), 0.0, 1.0)\n",
    "    return tf.cast(UNC_W_MIN, DTYPE_TF) + (tf.cast(UNC_W_MAX, DTYPE_TF) - tf.cast(UNC_W_MIN, DTYPE_TF)) * a\n",
    "\n",
    "@tf.function\n",
    "def stage_cost_cartpole_tf(S, U, x_init, unc_bonus=None, unc_w=0.0):\n",
    "    th = S[:, 2]\n",
    "    x  = S[:, 0]\n",
    "    c = (\n",
    "        tf.cast(UPRIGHT_W, DTYPE_TF) * (1.0 - tf.cos(th))\n",
    "        + tf.cast(CENTER_W, DTYPE_TF) * tf.square(x - x_init)\n",
    "        + tf.cast(U_W, DTYPE_TF) * tf.square(U)\n",
    "    )\n",
    "    if (unc_bonus is not None) and (unc_w > 0.0):\n",
    "        c = c - tf.cast(unc_w, DTYPE_TF) * tf.cast(unc_bonus, DTYPE_TF)\n",
    "    return c\n",
    "\n",
    "@tf.function\n",
    "def terminal_cost_hold_like_tf(S, x_init):\n",
    "    th = S[:, 2]\n",
    "    x  = S[:, 0]\n",
    "    cT = tf.zeros_like(th)\n",
    "    good = tf.logical_and(tf.cos(th) >= tf.cast(UPRIGHT_COS, DTYPE_TF),\n",
    "                          tf.abs(x - x_init) <= tf.cast(X_BAND, DTYPE_TF))\n",
    "    cT = tf.where(good, cT - tf.cast(5.0, DTYPE_TF), cT)\n",
    "    return cT\n",
    "\n",
    "def make_local_tf_predictors_bundle(idx_sub):\n",
    "    pred_dx_tf, _     = build_subset_predictor_from_global_tf(m_dx,     Z_GLOBAL, idx_sub)\n",
    "    pred_dxdot_tf, Zs = build_subset_predictor_from_global_tf(m_dxdot,  Z_GLOBAL, idx_sub)\n",
    "    pred_dth_tf, _    = build_subset_predictor_from_global_tf(m_dth,    Z_GLOBAL, idx_sub)\n",
    "    pred_dthdot_tf, _ = build_subset_predictor_from_global_tf(m_dthdot, Z_GLOBAL, idx_sub)\n",
    "\n",
    "    @tf.function\n",
    "    def dx_mu(X):     return pred_dx_tf(X)[0]\n",
    "    @tf.function\n",
    "    def dxdot_mu(X):  return pred_dxdot_tf(X)[0]\n",
    "    @tf.function\n",
    "    def dth_mu(X):    return pred_dth_tf(X)[0]\n",
    "    @tf.function\n",
    "    def thdot_mu(X):  return pred_dthdot_tf(X)[0]\n",
    "\n",
    "    @tf.function\n",
    "    def unc_var_dxdot(X):\n",
    "        _, v = pred_dxdot_tf(X)\n",
    "        return v\n",
    "\n",
    "    return (dx_mu, dxdot_mu, dth_mu, thdot_mu), unc_var_dxdot, Zs\n",
    "\n",
    "@tf.function\n",
    "def gp_dynamics_step_batch_local_tf(S, U, pred_bundle_tf):\n",
    "    pred_dx_tf, pred_dxdot_tf, pred_dth_tf, pred_dthdot_tf = pred_bundle_tf\n",
    "    Xfeat = batch_state_to_features_tf(S, U)\n",
    "    dx     = pred_dx_tf(Xfeat)\n",
    "    dxdot  = pred_dxdot_tf(Xfeat)\n",
    "    dth    = pred_dth_tf(Xfeat)\n",
    "    dthdot = pred_dthdot_tf(Xfeat)\n",
    "\n",
    "    S2 = tf.stack([\n",
    "        S[:, 0] + dx,\n",
    "        S[:, 1] + dxdot,\n",
    "        wrap_pi_tf(S[:, 2] + dth),\n",
    "        S[:, 3] + dthdot\n",
    "    ], axis=1)\n",
    "    return S2, Xfeat\n",
    "\n",
    "@tf.function\n",
    "def rollout_tube_features_local_tf(state0, u_seq, pred_bundle_tf):\n",
    "    H = tf.shape(u_seq)[0]\n",
    "    s = tf.identity(state0)\n",
    "    tube = tf.TensorArray(dtype=DTYPE_TF, size=H)\n",
    "\n",
    "    t = tf.constant(0)\n",
    "    def cond(t, s, tube): return t < H\n",
    "    def body(t, s, tube):\n",
    "        u = u_seq[t]\n",
    "        xfeat = batch_state_to_features_tf(tf.expand_dims(s, axis=0), tf.expand_dims(u, axis=0))[0]\n",
    "        tube = tube.write(t, xfeat)\n",
    "        s2, _ = gp_dynamics_step_batch_local_tf(tf.expand_dims(s, axis=0), tf.expand_dims(u, axis=0), pred_bundle_tf)\n",
    "        s = s2[0]\n",
    "        return t+1, s, tube\n",
    "\n",
    "    _, _, tube = tf.while_loop(cond, body, [t, s, tube], parallel_iterations=1)\n",
    "    return tube.stack()\n",
    "\n",
    "@tf.function\n",
    "def mppi_plan_gpu_local_tf(state0, x_init, u_mean0, t_global,\n",
    "                          pred_bundle_tf, unc_fn_tf,\n",
    "                          horizon=HORIZON, K=K_SAMPLES, sigma=SIGMA, lam=LAMBDA,\n",
    "                          base_seed=0):\n",
    "    H = tf.cast(horizon, tf.int32)\n",
    "    Kt = tf.cast(K, tf.int32)\n",
    "\n",
    "    seed = tf.stack([tf.cast(base_seed, tf.int32), tf.cast(t_global, tf.int32)], axis=0)\n",
    "    eps = tf.random.stateless_normal((Kt, H), seed=seed, mean=0.0, stddev=tf.cast(sigma, DTYPE_TF), dtype=DTYPE_TF)\n",
    "    U = tf.clip_by_value(u_mean0[None, :] + eps, tf.cast(U_MIN, DTYPE_TF), tf.cast(U_MAX, DTYPE_TF))\n",
    "\n",
    "    S = tf.tile(state0[None, :], [Kt, 1])\n",
    "    total_cost = tf.zeros((Kt,), dtype=DTYPE_TF)\n",
    "\n",
    "    Xta = tf.TensorArray(dtype=DTYPE_TF, size=H)\n",
    "    unc_w = exploration_weight_tf(t_global)\n",
    "\n",
    "    t = tf.constant(0, dtype=tf.int32)\n",
    "    def cond(t, S, total_cost, Xta): return t < H\n",
    "    def body(t, S, total_cost, Xta):\n",
    "        Ut = U[:, t]\n",
    "        S2, Xfeat = gp_dynamics_step_batch_local_tf(S, Ut, pred_bundle_tf)\n",
    "        Xta = Xta.write(t, Xfeat)\n",
    "        unc_bonus = unc_fn_tf(Xfeat)\n",
    "        total_cost = total_cost + stage_cost_cartpole_tf(S, Ut, x_init=tf.cast(x_init, DTYPE_TF),\n",
    "                                                         unc_bonus=unc_bonus, unc_w=unc_w)\n",
    "        return t+1, S2, total_cost, Xta\n",
    "\n",
    "    _, S, total_cost, Xta = tf.while_loop(cond, body, [t, S, total_cost, Xta], parallel_iterations=1)\n",
    "    total_cost = total_cost + terminal_cost_hold_like_tf(S, x_init=tf.cast(x_init, DTYPE_TF))\n",
    "\n",
    "    cmin = tf.reduce_min(total_cost)\n",
    "    w = tf.exp(-(total_cost - cmin) / tf.cast(lam, DTYPE_TF))\n",
    "    wsum = tf.reduce_sum(w) + tf.cast(1e-12, DTYPE_TF)\n",
    "\n",
    "    u_mean = u_mean0 + tf.reduce_sum(w[:, None] * eps, axis=0) / wsum\n",
    "    u_mean = tf.clip_by_value(u_mean, tf.cast(U_MIN, DTYPE_TF), tf.cast(U_MAX, DTYPE_TF))\n",
    "\n",
    "    tubeX = rollout_tube_features_local_tf(state0, u_mean, pred_bundle_tf)\n",
    "    XHKD = Xta.stack()\n",
    "    Xroll = tf.transpose(XHKD, perm=[1, 0, 2])\n",
    "\n",
    "    return u_mean[0], u_mean, tubeX, unc_w, Xroll, w\n",
    "\n",
    "def mppi_plan_gpu_local(state, x_init, u_init, t_global, pred_bundle_tf, unc_fn_tf, base_seed=0):\n",
    "    state0 = tf.convert_to_tensor(np.asarray(state, dtype=np.float64).reshape(4,), dtype=DTYPE_TF)\n",
    "    x0     = tf.convert_to_tensor(float(x_init), dtype=DTYPE_TF)\n",
    "    u0     = tf.convert_to_tensor(np.asarray(u_init, dtype=np.float64).reshape(-1,), dtype=DTYPE_TF)\n",
    "\n",
    "    dev = \"/GPU:0\" if len(tf.config.list_logical_devices(\"GPU\")) > 0 else \"/CPU:0\"\n",
    "    if REQUIRE_GPU: dev = \"/GPU:0\"\n",
    "\n",
    "    with tf.device(dev):\n",
    "        u_first, u_mean, tubeX, unc_w, Xroll, w = mppi_plan_gpu_local_tf(\n",
    "            state0, x0, u0, tf.convert_to_tensor(int(t_global), dtype=tf.int32),\n",
    "            pred_bundle_tf=pred_bundle_tf,\n",
    "            unc_fn_tf=unc_fn_tf,\n",
    "            horizon=HORIZON, K=K_SAMPLES, sigma=SIGMA, lam=LAMBDA,\n",
    "            base_seed=int(base_seed)\n",
    "        )\n",
    "\n",
    "    return float(u_first.numpy()), u_mean.numpy(), tubeX.numpy(), float(unc_w.numpy()), Xroll.numpy(), w.numpy()\n",
    "\n",
    "# ----------------------------\n",
    "# Visualization helpers (kept)\n",
    "# ----------------------------\n",
    "def plot_surface_global_or_local_dxdot(model_like, is_local=False, Z_overlay=None,\n",
    "                                      title=\"Surface\", std_clim=None,\n",
    "                                      Xmesh=PLOT_XMESH, Vmesh=PLOT_VMESH, Xfeat_grid=PLOT_XFEAT_GRID,\n",
    "                                      show_colorbar=True):\n",
    "    if not is_local:\n",
    "        mu, std = gp_predict_mu_std_fast(model_like, Xfeat_grid)\n",
    "        Mean = mu.reshape(Xmesh.shape)\n",
    "        Std  = std.reshape(Xmesh.shape)\n",
    "        Z = model_like.inducing_variable.Z.numpy()\n",
    "    else:\n",
    "        pred_local = model_like\n",
    "        mu, var = pred_local(Xfeat_grid)\n",
    "        Mean = mu.reshape(Xmesh.shape)\n",
    "        Std  = np.sqrt(np.maximum(var, 1e-12)).reshape(Xmesh.shape)\n",
    "        Z = Z_overlay\n",
    "\n",
    "    cmin = cmax = None\n",
    "    if std_clim is not None:\n",
    "        cmin, cmax = float(std_clim[0]), float(std_clim[1])\n",
    "\n",
    "    surface = go.Surface(\n",
    "        x=Xmesh, y=Vmesh, z=Mean,\n",
    "        surfacecolor=Std,\n",
    "        colorscale=\"Viridis\",\n",
    "        cmin=cmin, cmax=cmax,\n",
    "        colorbar=(dict(title=\"Std\") if show_colorbar else None),\n",
    "        showscale=bool(show_colorbar),\n",
    "        opacity=0.95,\n",
    "        name=\"surface\"\n",
    "    )\n",
    "    fig = go.Figure(data=[surface])\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        scene=dict(\n",
    "            xaxis=dict(title=\"x\", range=[PLOT_X_MIN, PLOT_X_MAX]),\n",
    "            yaxis=dict(title=\"xdot\", range=[PLOT_V_MIN, PLOT_V_MAX]),\n",
    "            zaxis=dict(title=\"Δxdot\"),\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, b=0, t=50),\n",
    "        height=620\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "def success_hold_update(state, x_init, hold_count):\n",
    "    x, xdot, th, thdot = state\n",
    "    upright = (np.cos(th) >= UPRIGHT_COS)\n",
    "    centered = (abs(x - x_init) <= X_BAND)\n",
    "    hold_count = (hold_count + 1) if (upright and centered) else 0\n",
    "    success = (hold_count >= HOLD_STEPS)\n",
    "    return hold_count, success, upright, centered\n",
    "\n",
    "# ============================================================\n",
    "# NEW: Create ONE animation per RUN (stitches ep0+ep1+ep2 frames)\n",
    "# ============================================================\n",
    "def display_run_animation_from_frames(frames, fps=FPS, resize=RESIZE):\n",
    "    if frames is None or len(frames) == 0:\n",
    "        print(\"⚠️ No frames captured for this run.\")\n",
    "        return None\n",
    "\n",
    "    fig = plt.figure(figsize=(resize[0]/100, resize[1]/100), dpi=100)\n",
    "    plt.axis(\"off\")\n",
    "    im = plt.imshow(frames[0])\n",
    "\n",
    "    def animate_fn(i):\n",
    "        im.set_data(frames[i])\n",
    "        return [im]\n",
    "\n",
    "    ani = animation.FuncAnimation(\n",
    "        fig, animate_fn,\n",
    "        frames=len(frames),\n",
    "        interval=1000 / float(fps),\n",
    "        blit=True\n",
    "    )\n",
    "    plt.close(fig)\n",
    "    html = HTML(ani.to_jshtml())\n",
    "    display(html)\n",
    "    return html\n",
    "\n",
    "# ============================================================\n",
    "# Single EPISODE runner (returns per-timestep timing arrays)\n",
    "#   NOTE: model & Z_GLOBAL are global; this function MUTATES them (training)\n",
    "#   CHANGE: does NOT display animation; returns frames if asked\n",
    "# ============================================================\n",
    "def run_one_episode_mppi_retrain_rgb_with_eval(\n",
    "    max_steps=600, seed=0, start_down=True, verbose=True,\n",
    "    use_gpu_mppi=True,\n",
    "    warmup_mppi=True,\n",
    "    record_rgb=True,\n",
    "    t_offset_in_run=0,\n",
    "    return_frames=False,  # NEW\n",
    "):\n",
    "    global m_dx, m_dxdot, m_dth, m_dthdot\n",
    "    global Z_GLOBAL\n",
    "    global STD_CMAX_FIXED\n",
    "\n",
    "    env = make_env(render_mode=None, seed=seed, max_episode_steps=max_steps, start_down=start_down)\n",
    "    obs, info = env.reset(seed=seed)\n",
    "    s = np.array(obs_to_state(obs), dtype=np.float64)\n",
    "    x_init = float(s[0])\n",
    "\n",
    "    # Ensure Z_GLOBAL fixed size\n",
    "    Z_GLOBAL = np.asarray(Z_GLOBAL, dtype=np.float64)\n",
    "    if Z_GLOBAL.shape[0] != int(M_GLOBAL):\n",
    "        if Z_GLOBAL.shape[0] > int(M_GLOBAL):\n",
    "            Z_GLOBAL = Z_GLOBAL[:int(M_GLOBAL)].copy()\n",
    "        else:\n",
    "            raise ValueError(f\"Z_GLOBAL has {Z_GLOBAL.shape[0]} points but M_GLOBAL={M_GLOBAL}.\")\n",
    "\n",
    "    # anchors\n",
    "    ANCHOR_IDX = compute_anchor_idx_dopt_from_Zglobal_multihead(\n",
    "        Z_GLOBAL, m_anchors=ANCHOR_M, lam=ANCHOR_LAM, normalize_traces=True\n",
    "    )\n",
    "\n",
    "    # init local\n",
    "    u_mean = np.zeros((HORIZON,), dtype=np.float64)\n",
    "    idx_sub = np.arange(min(M_SUB, M_GLOBAL), dtype=np.int64)\n",
    "\n",
    "    pred_local_np = make_subset_predictors_bundle(idx_sub)\n",
    "    unc_local_np  = build_subset_uncertainty_fn(m_dxdot, idx_sub)\n",
    "    pred_local_tf, unc_local_tf, _ = make_local_tf_predictors_bundle(idx_sub)\n",
    "\n",
    "    last_idx_sub = np.array(idx_sub, dtype=np.int64)\n",
    "    last_local_rebuild_t = 0\n",
    "\n",
    "    # timing arrays (length max_steps)\n",
    "    pred_time_step = np.zeros((max_steps,), dtype=np.float64)   # MPPI planning time per step\n",
    "    train_time_step = np.zeros((max_steps,), dtype=np.float64)  # training time per step (only nonzero on update steps)\n",
    "    wall_excl_vis_cum = np.zeros((max_steps,), dtype=np.float64)\n",
    "\n",
    "    # runtime excluding vis\n",
    "    t_wall_start = time.perf_counter()\n",
    "    vis_time_s = 0.0\n",
    "\n",
    "    # Warmup TF MPPI (compile), excluded from pred_time_step but still counts into wall time (as requested)\n",
    "    if use_gpu_mppi and warmup_mppi:\n",
    "        _ = mppi_plan_gpu_local(\n",
    "            state=s, x_init=x_init, u_init=u_mean, t_global=int(t_offset_in_run),\n",
    "            pred_bundle_tf=pred_local_tf, unc_fn_tf=unc_local_tf,\n",
    "            base_seed=seed\n",
    "        )\n",
    "\n",
    "    frames = []\n",
    "    total_reward = 0.0\n",
    "    hold_count = 0\n",
    "    updates = 0\n",
    "\n",
    "    Xbuf, ydx_buf, ydxdot_buf, ydth_buf, ydthdot_buf = [], [], [], [], []\n",
    "\n",
    "    last_rollout_inputs = None\n",
    "    last_rollout_weights = None\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        t_global = int(t_offset_in_run + t)\n",
    "\n",
    "        # ---------- MPPI plan (prediction time) ----------\n",
    "        t0 = time.perf_counter()\n",
    "        if use_gpu_mppi:\n",
    "            u0, u_mean, tubeX, unc_w, Xroll, wroll = mppi_plan_gpu_local(\n",
    "                state=s, x_init=x_init, u_init=u_mean, t_global=t_global,\n",
    "                pred_bundle_tf=pred_local_tf,\n",
    "                unc_fn_tf=unc_local_tf,\n",
    "                base_seed=seed\n",
    "            )\n",
    "        else:\n",
    "            raise RuntimeError(\"use_gpu_mppi=False path not wired in this multi-run cell (set use_gpu_mppi=True).\")\n",
    "        t1 = time.perf_counter()\n",
    "        pred_time_step[t] = (t1 - t0)\n",
    "\n",
    "        last_rollout_inputs = Xroll\n",
    "        last_rollout_weights = wroll\n",
    "\n",
    "        # ---------- tube subset selection ----------\n",
    "        idx_sub_cand, _, _, _, _ = select_tube_subset(\n",
    "            Z_global=Z_GLOBAL,\n",
    "            kernel_for_dopt=m_dxdot.kernel,\n",
    "            rollout_inputs=last_rollout_inputs,\n",
    "            rollout_weights=last_rollout_weights,\n",
    "            total_subset_size=M_SUB,\n",
    "            anchor_idx=ANCHOR_IDX,\n",
    "            time_bins=TUBE_TIME_BINS,\n",
    "            cov_eps=TUBE_COV_EPS,\n",
    "            dopt_jitter=1e-6,\n",
    "            fallback_candidates=TUBE_FALLBACK_CANDIDATES,\n",
    "            alpha=TUBE_ALPHA,\n",
    "        )\n",
    "\n",
    "        inter = np.intersect1d(last_idx_sub, idx_sub_cand)\n",
    "        overlap = float(len(inter)) / float(len(idx_sub_cand)) if len(idx_sub_cand) > 0 else 1.0\n",
    "\n",
    "        need_rebuild = ((t - last_local_rebuild_t) >= int(LOCAL_REBUILD_EVERY)) or (overlap < float(LOCAL_OVERLAP_THRESH))\n",
    "        if need_rebuild:\n",
    "            idx_sub = np.array(idx_sub_cand, dtype=np.int64)\n",
    "            pred_local_np = make_subset_predictors_bundle(idx_sub)\n",
    "            unc_local_np  = build_subset_uncertainty_fn(m_dxdot, idx_sub)\n",
    "            pred_local_tf, unc_local_tf, _ = make_local_tf_predictors_bundle(idx_sub)\n",
    "\n",
    "            last_idx_sub = np.array(idx_sub, dtype=np.int64)\n",
    "            last_local_rebuild_t = int(t)\n",
    "        else:\n",
    "            idx_sub = last_idx_sub\n",
    "\n",
    "        # ---------- env step ----------\n",
    "        obs2, r, terminated, truncated, info = env.step(np.array([u0], dtype=np.float32))\n",
    "        s2 = np.array(obs_to_state(obs2), dtype=np.float64)\n",
    "        total_reward += float(r)\n",
    "\n",
    "        respawned = bool(isinstance(info, dict) and info.get(\"respawned\", False))\n",
    "        if not respawned:\n",
    "            Xbuf.append(state_to_features(s[0], s[1], s[2], s[3], float(u0)))\n",
    "            ydx_buf.append([s2[0] - s[0]])\n",
    "            ydxdot_buf.append([s2[1] - s[1]])\n",
    "            ydth_buf.append([wrap_pi(s2[2] - s[2])])\n",
    "            ydthdot_buf.append([s2[3] - s[3]])\n",
    "\n",
    "        # ---------- render (excluded from wall) ----------\n",
    "        if record_rgb and RECORD_RGB_DEFAULT and (t % FRAME_STRIDE == 0):\n",
    "            tv0 = time.perf_counter()\n",
    "            W, H = int(RESIZE[0]), int(RESIZE[1])\n",
    "            frame = render_cartpole_frame_from_state(x=s2[0], theta=s2[2], x_threshold=2.4, W=W, H=H)\n",
    "            frames.append(frame)\n",
    "            vis_time_s += (time.perf_counter() - tv0)\n",
    "\n",
    "        # ---------- success tracking ----------\n",
    "        hold_count, success, upright, centered = success_hold_update(s2, x_init, hold_count)\n",
    "\n",
    "        if verbose and (t % 50 == 0):\n",
    "            print(f\"[t_global={t_global:04d}] u0={u0:+.2f}  unc_w={unc_w:.2f}  upright={upright} centered={centered} hold={hold_count}/{HOLD_STEPS}\")\n",
    "\n",
    "        # ---------- UPDATE (training time) ----------\n",
    "        if ((t + 1) % UPDATE_EVERY == 0) and (len(Xbuf) >= 10):\n",
    "            updates += 1\n",
    "\n",
    "            Xnew  = np.asarray(Xbuf, dtype=np.float64)\n",
    "            ydx   = np.asarray(ydx_buf, dtype=np.float64)\n",
    "            ydxd  = np.asarray(ydxdot_buf, dtype=np.float64)\n",
    "            ydth  = np.asarray(ydth_buf, dtype=np.float64)\n",
    "            ydthd = np.asarray(ydthdot_buf, dtype=np.float64)\n",
    "\n",
    "            # refit Z_GLOBAL\n",
    "            Z_GLOBAL = refit_Z_global_multihead(\n",
    "                Z_global=Z_GLOBAL,\n",
    "                Xnew=Xnew,\n",
    "                M_global=M_GLOBAL,\n",
    "                lam=ANCHOR_LAM,\n",
    "                mode=\"mean\",\n",
    "                normalize_traces=False,\n",
    "            )\n",
    "\n",
    "            # train 4 heads (measure training time precisely)\n",
    "            ttrain0 = time.perf_counter()\n",
    "            m_dx,     info_dx     = osgpr_stream_update(m_dx,     Xnew, ydx,   Z_GLOBAL, iters=ITERS_UPDATE, lr=LR_UPDATE, noise=NOISE_UPDATE, freeze_kernel=False)\n",
    "            m_dxdot,  info_dxdot  = osgpr_stream_update(m_dxdot,  Xnew, ydxd,  Z_GLOBAL, iters=ITERS_UPDATE, lr=LR_UPDATE, noise=NOISE_UPDATE, freeze_kernel=False)\n",
    "            m_dth,    info_dth    = osgpr_stream_update(m_dth,    Xnew, ydth,  Z_GLOBAL, iters=ITERS_UPDATE, lr=LR_UPDATE, noise=NOISE_UPDATE, freeze_kernel=False)\n",
    "            m_dthdot, info_dthdot = osgpr_stream_update(m_dthdot, Xnew, ydthd, Z_GLOBAL, iters=ITERS_UPDATE, lr=LR_UPDATE, noise=NOISE_UPDATE, freeze_kernel=False)\n",
    "            ttrain1 = time.perf_counter()\n",
    "\n",
    "            train_time_step[t] += float(ttrain1 - ttrain0)\n",
    "\n",
    "            # clear buffers\n",
    "            Xbuf, ydx_buf, ydxdot_buf, ydth_buf, ydthdot_buf = [], [], [], [], []\n",
    "\n",
    "            # reselect anchors\n",
    "            ANCHOR_IDX = compute_anchor_idx_dopt_from_Zglobal_multihead(\n",
    "                Z_GLOBAL, m_anchors=ANCHOR_M, lam=ANCHOR_LAM, normalize_traces=True\n",
    "            )\n",
    "\n",
    "            # force rebuild subset using last rollout distribution\n",
    "            idx_sub, _, _, _, _ = select_tube_subset(\n",
    "                Z_global=Z_GLOBAL,\n",
    "                kernel_for_dopt=m_dxdot.kernel,\n",
    "                rollout_inputs=last_rollout_inputs,\n",
    "                rollout_weights=last_rollout_weights,\n",
    "                total_subset_size=M_SUB,\n",
    "                anchor_idx=ANCHOR_IDX,\n",
    "                time_bins=TUBE_TIME_BINS,\n",
    "                cov_eps=TUBE_COV_EPS,\n",
    "                dopt_jitter=1e-6,\n",
    "                fallback_candidates=TUBE_FALLBACK_CANDIDATES,\n",
    "                alpha=TUBE_ALPHA,\n",
    "            )\n",
    "\n",
    "            pred_local_np = make_subset_predictors_bundle(idx_sub)\n",
    "            unc_local_np  = build_subset_uncertainty_fn(m_dxdot, idx_sub)\n",
    "            pred_local_tf, unc_local_tf, _ = make_local_tf_predictors_bundle(idx_sub)\n",
    "\n",
    "            last_idx_sub = np.array(idx_sub, dtype=np.int64)\n",
    "            last_local_rebuild_t = int(t)\n",
    "\n",
    "            # store global surfaces (excluded from timing; but we store only, no plot)\n",
    "            if STORE_GLOBAL_SURFACES:\n",
    "                muG, stdG = gp_predict_mu_std_fast(m_dxdot, PLOT_XFEAT_GRID)\n",
    "                MeanG = muG.reshape(PLOT_XMESH.shape)\n",
    "                StdG  = stdG.reshape(PLOT_XMESH.shape)\n",
    "                GLOBAL_SURF_HISTORY.append(dict(update=int(t_global), Mean=MeanG, Std=StdG))\n",
    "                _ = update_unified_std_scale([stdG.reshape(-1)], q=STD_FIXED_Q, mode=STD_MODE)\n",
    "\n",
    "        # ---------- wall time cumulative excl vis ----------\n",
    "        wall_excl_vis_cum[t] = max((time.perf_counter() - t_wall_start) - vis_time_s, 0.0)\n",
    "\n",
    "        # advance\n",
    "        s = s2\n",
    "\n",
    "        # terminate\n",
    "        if success or terminated or truncated:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    steps = int(t + 1)\n",
    "\n",
    "    stats = dict(\n",
    "        total_reward=float(total_reward),\n",
    "        steps=steps,\n",
    "        updates=int(updates),\n",
    "        frames=int(len(frames)),\n",
    "        x_init=float(x_init),\n",
    "        hold_steps=int(hold_count),\n",
    "        Z_global_size=int(len(Z_GLOBAL)),\n",
    "        std_cmax_fixed=float(STD_CMAX_FIXED) if STD_CMAX_FIXED is not None else None,\n",
    "        vis_time_s=float(vis_time_s),\n",
    "    )\n",
    "\n",
    "    frames_out = frames if (return_frames and record_rgb and RECORD_RGB_DEFAULT) else None\n",
    "\n",
    "    # Return per-step arrays trimmed to actual length + final idx_sub\n",
    "    return stats, frames_out, idx_sub, pred_time_step[:steps], train_time_step[:steps], wall_excl_vis_cum[:steps]\n",
    "\n",
    "# ============================================================\n",
    "# MULTI-RUN driver  (same parameters as your code)\n",
    "# ============================================================\n",
    "N_RUNS = 3\n",
    "N_EPISODES_PER_RUN = 3\n",
    "\n",
    "MAX_STEPS_PER_EP = 1000\n",
    "USE_GPU_MPPI = True\n",
    "START_DOWN = True\n",
    "VERBOSE = True\n",
    "\n",
    "# Snapshot INITIAL models & initial Z_GLOBAL ONCE (used to reset before each RUN)\n",
    "if \"Z_GLOBAL\" not in globals():\n",
    "    raise NameError(\"Z_GLOBAL must exist before Cell 6. Define initial inducing points in earlier cells.\")\n",
    "Z_GLOBAL_INIT = np.asarray(Z_GLOBAL, dtype=np.float64).copy()\n",
    "\n",
    "# Capture initial parameters for each model (safe reset)\n",
    "params_dx_init     = parameter_dict(m_dx)\n",
    "params_dxdot_init  = parameter_dict(m_dxdot)\n",
    "params_dth_init    = parameter_dict(m_dth)\n",
    "params_dthdot_init = parameter_dict(m_dthdot)\n",
    "\n",
    "def reset_models_and_globals_for_fresh_run():\n",
    "    global m_dx, m_dxdot, m_dth, m_dthdot\n",
    "    global Z_GLOBAL\n",
    "    global GLOBAL_SURF_HISTORY\n",
    "    global STD_CMAX_FIXED\n",
    "\n",
    "    # reset inducing points\n",
    "    Z_GLOBAL = Z_GLOBAL_INIT.copy()\n",
    "    if Z_GLOBAL.shape[0] != int(M_GLOBAL):\n",
    "        if Z_GLOBAL.shape[0] > int(M_GLOBAL):\n",
    "            Z_GLOBAL = Z_GLOBAL[:int(M_GLOBAL)].copy()\n",
    "        else:\n",
    "            raise ValueError(f\"Z_GLOBAL_INIT has {Z_GLOBAL.shape[0]} < M_GLOBAL={M_GLOBAL}.\")\n",
    "\n",
    "    # restore model params\n",
    "    multiple_assign(m_dx,     params_dx_init)\n",
    "    multiple_assign(m_dxdot,  params_dxdot_init)\n",
    "    multiple_assign(m_dth,    params_dth_init)\n",
    "    multiple_assign(m_dthdot, params_dthdot_init)\n",
    "\n",
    "    # rebuild caches\n",
    "    try: m_dx.build_predict_cache()\n",
    "    except Exception: pass\n",
    "    try: m_dxdot.build_predict_cache()\n",
    "    except Exception: pass\n",
    "    try: m_dth.build_predict_cache()\n",
    "    except Exception: pass\n",
    "    try: m_dthdot.build_predict_cache()\n",
    "    except Exception: pass\n",
    "\n",
    "    # reset per-run histories/scales\n",
    "    GLOBAL_SURF_HISTORY = []\n",
    "    STD_CMAX_FIXED = None\n",
    "\n",
    "# Storage for run-level metrics (arrays with variable lengths)\n",
    "run_pred_time = []\n",
    "run_train_time = []\n",
    "run_wall_cum = []\n",
    "run_rewards = []\n",
    "run_steps_total = []\n",
    "run_updates_total = []\n",
    "\n",
    "for run in range(N_RUNS):\n",
    "    print(f\"\\n==================== RUN {run+1}/{N_RUNS} (fresh reset) ====================\")\n",
    "    reset_models_and_globals_for_fresh_run()\n",
    "\n",
    "    # run-level concatenated arrays\n",
    "    pred_concat = []\n",
    "    train_concat = []\n",
    "    wall_concat = []\n",
    "    t_offset = 0\n",
    "    last_idx_sub = None\n",
    "\n",
    "    # ONE render per RUN: only capture frames from EP0, but animation stitches EP0+EP1+EP2 by reusing EP0 frames only?\n",
    "    # You asked: \"combine all episodes into a single render for the run\".\n",
    "    # To do that while still \"ONLY episode 0 renders\", we capture frames ONLY in episode 0.\n",
    "    # If you actually want visuals from ep1/ep2 too, set record_rgb=True for those eps as well.\n",
    "    frames_run = []   # NEW: frames for the single RUN animation\n",
    "\n",
    "    run_reward_sum = 0.0\n",
    "    run_updates_sum = 0\n",
    "    run_steps_sum = 0\n",
    "\n",
    "    for ep in range(N_EPISODES_PER_RUN):\n",
    "        record_rgb = (ep == 0)   # ✅ keep your constraint\n",
    "        warmup_mppi = (ep == 0)  # warmup once per run\n",
    "\n",
    "        print(f\"\\n--- RUN {run+1} EP {ep+1}/{N_EPISODES_PER_RUN} (t_offset={t_offset}) ---\")\n",
    "\n",
    "        stats_ep, frames_ep, idx_sub_ep, pred_t, train_t, wall_cum_t = run_one_episode_mppi_retrain_rgb_with_eval(\n",
    "            max_steps=MAX_STEPS_PER_EP,\n",
    "            seed=1000*run + ep,          # deterministic per run/ep\n",
    "            start_down=START_DOWN,\n",
    "            verbose=VERBOSE,\n",
    "            use_gpu_mppi=USE_GPU_MPPI,\n",
    "            warmup_mppi=warmup_mppi,\n",
    "            record_rgb=record_rgb,\n",
    "            t_offset_in_run=t_offset,\n",
    "            return_frames=True,\n",
    "        )\n",
    "\n",
    "        # collect frames ONLY from EP0 (per your rendering rule)\n",
    "        if record_rgb and (frames_ep is not None) and (len(frames_ep) > 0):\n",
    "            frames_run.extend(frames_ep)\n",
    "\n",
    "        last_idx_sub = idx_sub_ep\n",
    "\n",
    "        pred_concat.append(pred_t)\n",
    "        train_concat.append(train_t)\n",
    "        wall_concat.append(wall_cum_t)\n",
    "\n",
    "        run_reward_sum += stats_ep[\"total_reward\"]\n",
    "        run_updates_sum += stats_ep[\"updates\"]\n",
    "        run_steps_sum += stats_ep[\"steps\"]\n",
    "\n",
    "        t_offset += stats_ep[\"steps\"]\n",
    "\n",
    "    # concatenate (continuous timesteps across episodes in the run)\n",
    "    pred_run = np.concatenate(pred_concat, axis=0)\n",
    "    train_run = np.concatenate(train_concat, axis=0)\n",
    "    wall_run = np.concatenate(wall_concat, axis=0)\n",
    "\n",
    "    # recompute wall cumulative cleanly across concatenation (each episode’s wall_cum resets)\n",
    "    wall_incr = np.zeros_like(wall_run)\n",
    "    i0 = 0\n",
    "    for ep_arr in wall_concat:\n",
    "        ep_arr = np.asarray(ep_arr, dtype=np.float64)\n",
    "        if ep_arr.size > 0:\n",
    "            d = np.diff(np.concatenate([[0.0], ep_arr]))\n",
    "            wall_incr[i0:i0+len(d)] = d\n",
    "        i0 += len(ep_arr)\n",
    "    wall_cum_run = np.cumsum(np.maximum(wall_incr, 0.0))\n",
    "\n",
    "    # training cumulative\n",
    "    train_cum_run = np.cumsum(train_run)\n",
    "\n",
    "    # store run metrics\n",
    "    run_pred_time.append(pred_run)\n",
    "    run_train_time.append(train_run)\n",
    "    run_wall_cum.append(wall_cum_run)\n",
    "    run_rewards.append(run_reward_sum)\n",
    "    run_steps_total.append(len(pred_run))\n",
    "    run_updates_total.append(run_updates_sum)\n",
    "\n",
    "    # ==========================\n",
    "    # END OF RUN: SINGLE animation (EP0 frames) + surfaces\n",
    "    # ==========================\n",
    "    print(f\"\\n=== RUN {run+1}: single animation (episode 0 only, per your rule) ===\")\n",
    "    _ = display_run_animation_from_frames(frames_run, fps=FPS, resize=RESIZE)\n",
    "\n",
    "    print(f\"\\n=== RUN {run+1}: final model surfaces ===\")\n",
    "    std_clim = (STD_CMIN_FIXED, STD_CMAX_FIXED if STD_CMAX_FIXED is not None else 1.0)\n",
    "\n",
    "    plot_surface_global_or_local_dxdot(\n",
    "        model_like=m_dxdot,\n",
    "        is_local=False,\n",
    "        title=f\"RUN {run+1} FINAL — GLOBAL Δxdot (mean colored by Std)\",\n",
    "        std_clim=std_clim,\n",
    "        show_colorbar=True\n",
    "    )\n",
    "\n",
    "    # OPTIONAL: local surface using last_idx_sub (you requested global + local)\n",
    "    try:\n",
    "        pred_local_dxdot, Zsub_overlay = build_subset_predictor_from_global(m_dxdot, Z_GLOBAL, last_idx_sub)\n",
    "        plot_surface_global_or_local_dxdot(\n",
    "            model_like=pred_local_dxdot,\n",
    "            is_local=True,\n",
    "            Z_overlay=Zsub_overlay,\n",
    "            title=f\"RUN {run+1} FINAL — LOCAL Δxdot (subset mean colored by Std)\",\n",
    "            std_clim=std_clim,\n",
    "            show_colorbar=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ Local surface plot skipped due to error:\", repr(e))\n",
    "\n",
    "# ============================================================\n",
    "# Aggregate across runs: mean curves (pad with NaN)\n",
    "# ============================================================\n",
    "maxT = int(max(run_steps_total))\n",
    "def pad_to(arr, T):\n",
    "    out = np.full((T,), np.nan, dtype=np.float64)\n",
    "    out[:len(arr)] = arr\n",
    "    return out\n",
    "\n",
    "pred_mat  = np.vstack([pad_to(a, maxT) for a in run_pred_time])\n",
    "train_mat = np.vstack([pad_to(a, maxT) for a in run_train_time])\n",
    "wall_mat  = np.vstack([pad_to(a, maxT) for a in run_wall_cum])\n",
    "\n",
    "# cumulative training per run then mean\n",
    "train_cum_mat = np.vstack([pad_to(np.cumsum(a), maxT) for a in run_train_time])\n",
    "\n",
    "pred_mean  = np.nanmean(pred_mat, axis=0)\n",
    "train_mean = np.nanmean(train_mat, axis=0)\n",
    "train_cum_mean = np.nanmean(train_cum_mat, axis=0)\n",
    "wall_cum_mean  = np.nanmean(wall_mat, axis=0)\n",
    "\n",
    "t_axis = np.arange(maxT)\n",
    "\n",
    "# ============================================================\n",
    "# Plots (mean over runs)\n",
    "# ============================================================\n",
    "plt.figure(figsize=(10, 3.2))\n",
    "plt.plot(t_axis, train_mean)\n",
    "plt.xlabel(\"timestep (within run, concatenated episodes)\")\n",
    "plt.ylabel(\"training time (s) per step\")\n",
    "plt.title(\"Mean training time per timestep (non-cumulative) over runs\")\n",
    "plt.grid(True, alpha=0.25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 3.2))\n",
    "plt.plot(t_axis, train_cum_mean)\n",
    "plt.xlabel(\"timestep (within run, concatenated episodes)\")\n",
    "plt.ylabel(\"cumulative training time (s)\")\n",
    "plt.title(\"Mean cumulative training time over runs\")\n",
    "plt.grid(True, alpha=0.25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 3.2))\n",
    "plt.plot(t_axis, pred_mean)\n",
    "plt.xlabel(\"timestep (within run, concatenated episodes)\")\n",
    "plt.ylabel(\"prediction time (s) per step (MPPI planning)\")\n",
    "plt.title(\"Mean prediction (MPPI) time per timestep over runs\")\n",
    "plt.grid(True, alpha=0.25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 3.2))\n",
    "plt.plot(t_axis, wall_cum_mean)\n",
    "plt.xlabel(\"timestep (within run, concatenated episodes)\")\n",
    "plt.ylabel(\"wall time cumulative (s) (EXCLUDING visualization)\")\n",
    "plt.title(\"Mean wall time cumulative over runs (no visualization)\")\n",
    "plt.grid(True, alpha=0.25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# Summary stats\n",
    "# ============================================================\n",
    "print(\"\\n==================== SUMMARY ====================\")\n",
    "for r in range(N_RUNS):\n",
    "    print(f\"RUN {r+1}: total_steps={run_steps_total[r]}, total_reward_sum={run_rewards[r]:.3f}, updates_sum={run_updates_total[r]}\")\n",
    "print(\"================================================\\n\")\n"
   ],
   "id": "62cac5d7d902dc0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================\n",
    "# Cell 7 — BASELINE: Batch SVGP (4-head) + MPPI + Online retrain\n",
    "#\n",
    "# ✅ Matches your PLSSGP evaluation protocol:\n",
    "#   - N_RUNS = 3\n",
    "#   - Each run = 3 continuous episodes (model + dataset carry across episodes within run)\n",
    "#   - BEFORE each run (including run 1): full reset (fresh models, fresh data, fresh Z)\n",
    "#   - Same cost / success logic (upright+center hold)\n",
    "#   - One render per run (episode 0 only)\n",
    "#   - Logs per-timestep timings:\n",
    "#       * train_time_step (non-cumulative)\n",
    "#       * train_time_cum\n",
    "#       * pred_time_step (MPPI time)\n",
    "#       * wall_time_cum (excluding visualization)\n",
    "#   - Plots MEAN curves across runs (nan-mean over variable lengths)\n",
    "#   - Plots a GLOBAL surface after each run (Δxdot mean colored by std)\n",
    "#\n",
    "# Notes / assumptions:\n",
    "#   - Uses GPflow SVGP (Gaussian likelihood) per head.\n",
    "#   - Keeps inducing points size fixed at M_GLOBAL; updates variational params + hypers.\n",
    "#   - Uses TF MPPI (GPU-capable) with GLOBAL predictors (no local subset / no tube).\n",
    "# ============================\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML, display\n",
    "import plotly.graph_objects as go\n",
    "import gpflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# -----------------------\n",
    "# REQUIRED symbols from earlier cells\n",
    "# -----------------------\n",
    "required = [\n",
    "    \"make_env\", \"obs_to_state\", \"wrap_pi\",\n",
    "    \"state_to_features\", \"batch_state_to_features\",\n",
    "]\n",
    "missing = [k for k in required if k not in globals()]\n",
    "if len(missing) > 0:\n",
    "    raise NameError(f\"SVGP Cell 7 missing required symbols from earlier cells: {missing}\")\n",
    "\n",
    "if \"render_cartpole_frame_from_state\" not in globals() and \"render_cartpole_frame_from_state\" not in globals():\n",
    "    # your PLSSGP cell used render_cartpole_frame_from_state(...) (or render_cartpole_frame_from_state)\n",
    "    # we'll handle both names\n",
    "    pass\n",
    "\n",
    "# -----------------------\n",
    "# Dtypes (match your pipeline: float64)\n",
    "# -----------------------\n",
    "NP_DTYPE = np.float64\n",
    "TF_DTYPE = gpflow.default_float()\n",
    "if TF_DTYPE != tf.float64:\n",
    "    print(\"⚠️ gpflow.default_float() is not float64. You said float64 throughout; set gpflow.config.set_default_float(np.float64) in Cell 1.\")\n",
    "\n",
    "# -----------------------\n",
    "# Config defaults (prefer your globals if already defined)\n",
    "# -----------------------\n",
    "HORIZON    = int(globals().get(\"HORIZON\", 50))\n",
    "K_SAMPLES  = int(globals().get(\"K_SAMPLES\", 256))\n",
    "SIGMA      = float(globals().get(\"SIGMA\", 0.6))\n",
    "LAMBDA     = float(globals().get(\"LAMBDA\", 1.0))\n",
    "\n",
    "UPDATE_EVERY = int(globals().get(\"UPDATE_EVERY\", 30))\n",
    "ITERS_UPDATE = int(globals().get(\"ITERS_UPDATE\", 150))\n",
    "LR_UPDATE    = float(globals().get(\"LR_UPDATE\", 0.02))\n",
    "NOISE_UPDATE = float(globals().get(\"NOISE_UPDATE\", 1e-4))\n",
    "\n",
    "M_GLOBAL = int(globals().get(\"M_GLOBAL\", 256))\n",
    "\n",
    "# task/success (prefer your globals)\n",
    "X_BAND      = float(globals().get(\"X_BAND\", 0.70))\n",
    "UPRIGHT_COS = float(globals().get(\"UPRIGHT_COS\", 0.85))\n",
    "HOLD_STEPS  = int(globals().get(\"HOLD_STEPS\", 200))\n",
    "\n",
    "# exploration schedule (prefer your globals)\n",
    "EXPLORE_STEPS = int(globals().get(\"EXPLORE_STEPS\", 200))\n",
    "UNC_W_MAX     = float(globals().get(\"UNC_W_MAX\", 15.0))\n",
    "UNC_W_MIN     = float(globals().get(\"UNC_W_MIN\", 0.0))\n",
    "CENTER_W      = float(globals().get(\"CENTER_W\", 1.0))\n",
    "U_W           = float(globals().get(\"U_W\", 0.005))\n",
    "UPRIGHT_W     = float(globals().get(\"UPRIGHT_W\", 2.0))\n",
    "\n",
    "# action bounds (prefer earlier-cell globals)\n",
    "U_MIN = float(globals().get(\"U_MIN\", -1.0))\n",
    "U_MAX = float(globals().get(\"U_MAX\", +1.0))\n",
    "\n",
    "# rendering\n",
    "RECORD_RGB   = True\n",
    "RESIZE       = globals().get(\"RESIZE\", (720, 450))\n",
    "FPS          = int(globals().get(\"FPS\", 10))\n",
    "FRAME_STRIDE = int(globals().get(\"FRAME_STRIDE\", 2))\n",
    "\n",
    "# multi-run protocol\n",
    "N_RUNS = 3\n",
    "EPISODES_PER_RUN = 3\n",
    "MAX_STEPS_PER_EP = int(globals().get(\"MAX_STEPS_PER_EP\", 1000))\n",
    "START_DOWN = bool(globals().get(\"START_DOWN\", True))\n",
    "USE_GPU_MPPI = True\n",
    "WARMUP_MPPI = True\n",
    "VERBOSE = True\n",
    "\n",
    "# -----------------------\n",
    "# Fixed plot grid (for surface after each run)\n",
    "# -----------------------\n",
    "PLOT_X_MIN, PLOT_X_MAX = globals().get(\"PLOT_X_MIN\", -2.4), globals().get(\"PLOT_X_MAX\", 2.4)\n",
    "PLOT_V_MIN, PLOT_V_MAX = globals().get(\"PLOT_V_MIN\", -3.0), globals().get(\"PLOT_V_MAX\", 3.0)\n",
    "PLOT_N_GRID            = int(globals().get(\"PLOT_N_GRID\", 60))\n",
    "PLOT_TH_FIXED          = float(globals().get(\"PLOT_TH_FIXED\", 0.0))\n",
    "PLOT_THDOT_FIXED       = float(globals().get(\"PLOT_THDOT_FIXED\", 0.0))\n",
    "PLOT_U_FIXED           = float(globals().get(\"PLOT_U_FIXED\", +1.0))\n",
    "\n",
    "def build_fixed_plot_grid():\n",
    "    xg = np.linspace(PLOT_X_MIN, PLOT_X_MAX, PLOT_N_GRID)\n",
    "    vg = np.linspace(PLOT_V_MIN, PLOT_V_MAX, PLOT_N_GRID)\n",
    "    X, V = np.meshgrid(xg, vg)\n",
    "    Xfeat_grid = np.vstack([\n",
    "        state_to_features(x, xdot, PLOT_TH_FIXED, PLOT_THDOT_FIXED, PLOT_U_FIXED)\n",
    "        for x, xdot in zip(X.ravel(), V.ravel())\n",
    "    ]).astype(np.float64)\n",
    "    return X, V, Xfeat_grid\n",
    "\n",
    "PLOT_XMESH, PLOT_VMESH, PLOT_XFEAT_GRID = build_fixed_plot_grid()\n",
    "\n",
    "def gp_predict_mu_std(model, X):\n",
    "    Xtf = tf.convert_to_tensor(np.asarray(X, dtype=np.float64), dtype=TF_DTYPE)\n",
    "    mu_tf, var_tf = model.predict_f(Xtf, full_cov=False)\n",
    "    mu = mu_tf.numpy().reshape(-1)\n",
    "    var = var_tf.numpy().reshape(-1)\n",
    "    std = np.sqrt(np.maximum(var, 1e-12))\n",
    "    return mu, std\n",
    "\n",
    "def plot_global_surface_dxdot(model_dxdot, title=\"GLOBAL Δxdot surface\"):\n",
    "    mu, std = gp_predict_mu_std(model_dxdot, PLOT_XFEAT_GRID)\n",
    "    Mean = mu.reshape(PLOT_XMESH.shape)\n",
    "    Std  = std.reshape(PLOT_XMESH.shape)\n",
    "\n",
    "    fig = go.Figure(data=[\n",
    "        go.Surface(\n",
    "            x=PLOT_XMESH, y=PLOT_VMESH, z=Mean,\n",
    "            surfacecolor=Std,\n",
    "            colorscale=\"Viridis\",\n",
    "            showscale=True,\n",
    "            colorbar=dict(title=\"Std\"),\n",
    "            opacity=0.95\n",
    "        )\n",
    "    ])\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        scene=dict(\n",
    "            xaxis=dict(title=\"x\", range=[PLOT_X_MIN, PLOT_X_MAX]),\n",
    "            yaxis=dict(title=\"xdot\", range=[PLOT_V_MIN, PLOT_V_MAX]),\n",
    "            zaxis=dict(title=\"Δxdot\"),\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, b=0, t=50),\n",
    "        height=620\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "# -----------------------\n",
    "# Success check (same logic as your PLSSGP cell)\n",
    "# -----------------------\n",
    "def success_hold_update(state, x_init, hold_count):\n",
    "    x, xdot, th, thdot = state\n",
    "    upright = (np.cos(th) >= UPRIGHT_COS)\n",
    "    centered = (abs(x - x_init) <= X_BAND)\n",
    "    hold_count = (hold_count + 1) if (upright and centered) else 0\n",
    "    success = (hold_count >= HOLD_STEPS)\n",
    "    return hold_count, success, upright, centered\n",
    "\n",
    "# -----------------------\n",
    "# Cost + exploration weight (same functional form as your PLSSGP cell)\n",
    "# -----------------------\n",
    "@tf.function\n",
    "def exploration_weight_tf(t):\n",
    "    explore_steps_f = tf.cast(EXPLORE_STEPS, TF_DTYPE)\n",
    "    t_f = tf.cast(t, TF_DTYPE)\n",
    "    a = tf.clip_by_value(1.0 - t_f / tf.maximum(explore_steps_f, 1.0), 0.0, 1.0)\n",
    "    return tf.cast(UNC_W_MIN, TF_DTYPE) + (tf.cast(UNC_W_MAX, TF_DTYPE) - tf.cast(UNC_W_MIN, TF_DTYPE)) * a\n",
    "\n",
    "@tf.function\n",
    "def wrap_pi_tf(theta):\n",
    "    two_pi = tf.constant(2.0 * np.pi, dtype=TF_DTYPE)\n",
    "    pi = tf.constant(np.pi, dtype=TF_DTYPE)\n",
    "    return tf.math.floormod(theta + pi, two_pi) - pi\n",
    "\n",
    "@tf.function\n",
    "def batch_state_to_features_tf(S, U):\n",
    "    x = S[:, 0]\n",
    "    xdot = S[:, 1]\n",
    "    th = S[:, 2]\n",
    "    thdot = S[:, 3]\n",
    "\n",
    "    f0 = tf.tanh(x / tf.constant(2.4, dtype=TF_DTYPE))\n",
    "    f1 = tf.tanh(xdot / tf.constant(3.0, dtype=TF_DTYPE))\n",
    "    f2 = tf.sin(th)\n",
    "    f3 = tf.cos(th)\n",
    "    f4 = tf.tanh(thdot / tf.constant(8.0, dtype=TF_DTYPE))\n",
    "    f5 = U\n",
    "    return tf.stack([f0, f1, f2, f3, f4, f5], axis=1)\n",
    "\n",
    "@tf.function\n",
    "def stage_cost_cartpole_tf(S, U, x_init, unc_bonus=None, unc_w=0.0):\n",
    "    th = S[:, 2]\n",
    "    x  = S[:, 0]\n",
    "    c = (\n",
    "        tf.cast(UPRIGHT_W, TF_DTYPE) * (1.0 - tf.cos(th))\n",
    "        + tf.cast(CENTER_W, TF_DTYPE) * tf.square(x - x_init)\n",
    "        + tf.cast(U_W, TF_DTYPE) * tf.square(U)\n",
    "    )\n",
    "    if (unc_bonus is not None) and (unc_w > 0.0):\n",
    "        c = c - tf.cast(unc_w, TF_DTYPE) * tf.cast(unc_bonus, TF_DTYPE)\n",
    "    return c\n",
    "\n",
    "@tf.function\n",
    "def terminal_cost_hold_like_tf(S, x_init):\n",
    "    th = S[:, 2]\n",
    "    x  = S[:, 0]\n",
    "    cT = tf.zeros_like(th)\n",
    "    good = tf.logical_and(tf.cos(th) >= tf.cast(UPRIGHT_COS, TF_DTYPE),\n",
    "                          tf.abs(x - x_init) <= tf.cast(X_BAND, TF_DTYPE))\n",
    "    cT = tf.where(good, cT - tf.cast(5.0, TF_DTYPE), cT)\n",
    "    return cT\n",
    "\n",
    "# -----------------------\n",
    "# SVGP model factory (4 heads)\n",
    "# -----------------------\n",
    "def make_svgp_head(Z_init, kernel, noise=1e-4):\n",
    "    Z_init = np.asarray(Z_init, dtype=np.float64)\n",
    "    inducing = gpflow.inducing_variables.InducingPoints(tf.convert_to_tensor(Z_init, dtype=TF_DTYPE))\n",
    "    lik = gpflow.likelihoods.Gaussian(variance=float(noise))\n",
    "\n",
    "    # q_mu/q_sqrt will be sized to inducing points (M,1)\n",
    "    m = gpflow.models.SVGP(\n",
    "        kernel=kernel,\n",
    "        likelihood=lik,\n",
    "        inducing_variable=inducing,\n",
    "        num_latent_gps=1,\n",
    "        whiten=True\n",
    "    )\n",
    "    return m\n",
    "\n",
    "def make_four_head_svgp_models(Z_init):\n",
    "    # IMPORTANT: use independent kernel objects for each head (no shared variables)\n",
    "    # If you already have a kernel factory in earlier cells, you can swap it in here.\n",
    "    def _base_kernel():\n",
    "        # Default: SE-ARD over 6D feature space\n",
    "        return gpflow.kernels.SquaredExponential(lengthscales=np.ones((6,), dtype=np.float64), variance=1.0)\n",
    "\n",
    "    m_dx     = make_svgp_head(Z_init, _base_kernel(), noise=NOISE_UPDATE)\n",
    "    m_dxdot  = make_svgp_head(Z_init, _base_kernel(), noise=NOISE_UPDATE)\n",
    "    m_dth    = make_svgp_head(Z_init, _base_kernel(), noise=NOISE_UPDATE)\n",
    "    m_dthdot = make_svgp_head(Z_init, _base_kernel(), noise=NOISE_UPDATE)\n",
    "    return m_dx, m_dxdot, m_dth, m_dthdot\n",
    "\n",
    "# -----------------------\n",
    "# SVGP training step (batch retrain on ALL data so far in the run)\n",
    "# -----------------------\n",
    "def train_svgp_batch(model, X, Y, iters=150, lr=0.02, clip_norm=10.0):\n",
    "    Xtf = tf.convert_to_tensor(np.asarray(X, dtype=np.float64), dtype=TF_DTYPE)\n",
    "    Ytf = tf.convert_to_tensor(np.asarray(Y, dtype=np.float64).reshape(-1,1), dtype=TF_DTYPE)\n",
    "\n",
    "    opt = tf.optimizers.Adam(learning_rate=float(lr))\n",
    "\n",
    "    @tf.function\n",
    "    def step():\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = -model.elbo((Xtf, Ytf))\n",
    "        vars_ = model.trainable_variables\n",
    "        grads = tape.gradient(loss, vars_)\n",
    "        if clip_norm is not None:\n",
    "            grads, _ = tf.clip_by_global_norm(grads, clip_norm)\n",
    "        opt.apply_gradients(zip(grads, vars_))\n",
    "        return loss\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    last = None\n",
    "    for _ in range(int(iters)):\n",
    "        last = step()\n",
    "    t_train = time.perf_counter() - t0\n",
    "    return float(t_train), float(last.numpy() if last is not None else np.nan)\n",
    "\n",
    "# -----------------------\n",
    "# GLOBAL predictors for TF MPPI\n",
    "# -----------------------\n",
    "def make_global_tf_predictors_bundle_from_svgp(m_dx, m_dxdot, m_dth, m_dthdot):\n",
    "    @tf.function\n",
    "    def dx_mu(Xfeat):\n",
    "        mu, _ = m_dx.predict_f(Xfeat, full_cov=False)\n",
    "        return tf.reshape(mu, (-1,))\n",
    "\n",
    "    @tf.function\n",
    "    def dxdot_mu(Xfeat):\n",
    "        mu, _ = m_dxdot.predict_f(Xfeat, full_cov=False)\n",
    "        return tf.reshape(mu, (-1,))\n",
    "\n",
    "    @tf.function\n",
    "    def dth_mu(Xfeat):\n",
    "        mu, _ = m_dth.predict_f(Xfeat, full_cov=False)\n",
    "        return tf.reshape(mu, (-1,))\n",
    "\n",
    "    @tf.function\n",
    "    def thdot_mu(Xfeat):\n",
    "        mu, _ = m_dthdot.predict_f(Xfeat, full_cov=False)\n",
    "        return tf.reshape(mu, (-1,))\n",
    "\n",
    "    @tf.function\n",
    "    def dxdot_var(Xfeat):\n",
    "        _, v = m_dxdot.predict_f(Xfeat, full_cov=False)\n",
    "        return tf.reshape(v, (-1,))\n",
    "\n",
    "    return (dx_mu, dxdot_mu, dth_mu, thdot_mu), dxdot_var\n",
    "\n",
    "@tf.function\n",
    "def gp_dynamics_step_batch_tf(S, U, pred_bundle_tf):\n",
    "    pred_dx_tf, pred_dxdot_tf, pred_dth_tf, pred_dthdot_tf = pred_bundle_tf\n",
    "    Xfeat = batch_state_to_features_tf(S, U)\n",
    "    dx     = pred_dx_tf(Xfeat)\n",
    "    dxdot  = pred_dxdot_tf(Xfeat)\n",
    "    dth    = pred_dth_tf(Xfeat)\n",
    "    dthdot = pred_dthdot_tf(Xfeat)\n",
    "\n",
    "    S2 = tf.stack([\n",
    "        S[:, 0] + dx,\n",
    "        S[:, 1] + dxdot,\n",
    "        wrap_pi_tf(S[:, 2] + dth),\n",
    "        S[:, 3] + dthdot\n",
    "    ], axis=1)\n",
    "    return S2, Xfeat\n",
    "\n",
    "@tf.function\n",
    "def rollout_tube_features_tf(state0, u_seq, pred_bundle_tf):\n",
    "    # (kept for parity; baseline doesn't use tube selection)\n",
    "    H = tf.shape(u_seq)[0]\n",
    "    s = tf.identity(state0)\n",
    "    tube = tf.TensorArray(dtype=TF_DTYPE, size=H)\n",
    "\n",
    "    t = tf.constant(0)\n",
    "    def cond(t, s, tube): return t < H\n",
    "\n",
    "    def body(t, s, tube):\n",
    "        u = u_seq[t]\n",
    "        xfeat = batch_state_to_features_tf(tf.expand_dims(s, axis=0), tf.expand_dims(u, axis=0))[0]\n",
    "        tube = tube.write(t, xfeat)\n",
    "        s2, _ = gp_dynamics_step_batch_tf(tf.expand_dims(s, axis=0), tf.expand_dims(u, axis=0), pred_bundle_tf)\n",
    "        return t+1, s2[0], tube\n",
    "\n",
    "    _, _, tube = tf.while_loop(cond, body, [t, s, tube], parallel_iterations=1)\n",
    "    return tube.stack()\n",
    "\n",
    "@tf.function\n",
    "def mppi_plan_gpu_global_tf(state0, x_init, u_mean0, t_global,\n",
    "                           pred_bundle_tf, unc_var_fn_tf,\n",
    "                           horizon=HORIZON, K=K_SAMPLES, sigma=SIGMA, lam=LAMBDA,\n",
    "                           base_seed=0):\n",
    "    H = tf.cast(horizon, tf.int32)\n",
    "    Kt = tf.cast(K, tf.int32)\n",
    "\n",
    "    seed = tf.stack([tf.cast(base_seed, tf.int32), tf.cast(t_global, tf.int32)], axis=0)\n",
    "    eps = tf.random.stateless_normal((Kt, H), seed=seed, mean=0.0, stddev=tf.cast(sigma, TF_DTYPE), dtype=TF_DTYPE)\n",
    "    U = tf.clip_by_value(u_mean0[None, :] + eps, tf.cast(U_MIN, TF_DTYPE), tf.cast(U_MAX, TF_DTYPE))\n",
    "\n",
    "    S = tf.tile(state0[None, :], [Kt, 1])\n",
    "    total_cost = tf.zeros((Kt,), dtype=TF_DTYPE)\n",
    "\n",
    "    unc_w = exploration_weight_tf(t_global)\n",
    "\n",
    "    t = tf.constant(0, dtype=tf.int32)\n",
    "    def cond(t, S, total_cost): return t < H\n",
    "\n",
    "    def body(t, S, total_cost):\n",
    "        Ut = U[:, t]\n",
    "        S2, Xfeat = gp_dynamics_step_batch_tf(S, Ut, pred_bundle_tf)\n",
    "        unc_bonus = unc_var_fn_tf(Xfeat)\n",
    "        total_cost = total_cost + stage_cost_cartpole_tf(\n",
    "            S, Ut, x_init=tf.cast(x_init, TF_DTYPE),\n",
    "            unc_bonus=unc_bonus, unc_w=unc_w\n",
    "        )\n",
    "        return t+1, S2, total_cost\n",
    "\n",
    "    _, S, total_cost = tf.while_loop(cond, body, [t, S, total_cost], parallel_iterations=1)\n",
    "    total_cost = total_cost + terminal_cost_hold_like_tf(S, x_init=tf.cast(x_init, TF_DTYPE))\n",
    "\n",
    "    cmin = tf.reduce_min(total_cost)\n",
    "    w = tf.exp(-(total_cost - cmin) / tf.cast(lam, TF_DTYPE))\n",
    "    wsum = tf.reduce_sum(w) + tf.cast(1e-12, TF_DTYPE)\n",
    "\n",
    "    u_mean = u_mean0 + tf.reduce_sum(w[:, None] * eps, axis=0) / wsum\n",
    "    u_mean = tf.clip_by_value(u_mean, tf.cast(U_MIN, TF_DTYPE), tf.cast(U_MAX, TF_DTYPE))\n",
    "\n",
    "    tubeX = rollout_tube_features_tf(state0, u_mean, pred_bundle_tf)\n",
    "    return u_mean[0], u_mean, tubeX, unc_w\n",
    "\n",
    "def mppi_plan_gpu_global(state, x_init, u_init, t_global, pred_bundle_tf, unc_var_fn_tf, base_seed=0):\n",
    "    state0 = tf.convert_to_tensor(np.asarray(state, dtype=np.float64).reshape(4,), dtype=TF_DTYPE)\n",
    "    x0     = tf.convert_to_tensor(float(x_init), dtype=TF_DTYPE)\n",
    "    u0     = tf.convert_to_tensor(np.asarray(u_init, dtype=np.float64).reshape(-1,), dtype=TF_DTYPE)\n",
    "\n",
    "    dev = \"/GPU:0\" if len(tf.config.list_logical_devices(\"GPU\")) > 0 else \"/CPU:0\"\n",
    "    with tf.device(dev):\n",
    "        u_first, u_mean, tubeX, unc_w = mppi_plan_gpu_global_tf(\n",
    "            state0, x0, u0, tf.convert_to_tensor(int(t_global), dtype=tf.int32),\n",
    "            pred_bundle_tf=pred_bundle_tf,\n",
    "            unc_var_fn_tf=unc_var_fn_tf,\n",
    "            horizon=HORIZON, K=K_SAMPLES, sigma=SIGMA, lam=LAMBDA,\n",
    "            base_seed=int(base_seed)\n",
    "        )\n",
    "    return float(u_first.numpy()), u_mean.numpy(), tubeX.numpy(), float(unc_w.numpy())\n",
    "\n",
    "# -----------------------\n",
    "# Rendering helper compatibility\n",
    "# -----------------------\n",
    "def _render_frame_from_state(s):\n",
    "    W, H = int(RESIZE[0]), int(RESIZE[1])\n",
    "    if \"render_cartpole_frame_from_state\" in globals():\n",
    "        return render_cartpole_frame_from_state(x=s[0], theta=s[2], x_threshold=2.4, W=W, H=H)\n",
    "    if \"render_cartpole_frame_from_state\" in globals():\n",
    "        return render_cartpole_frame_from_state(x=s[0], theta=s[2], x_threshold=2.4, W=W, H=H)\n",
    "    raise NameError(\"No render_cartpole_frame_from_state(...) found; set RECORD_RGB=False or define the renderer in earlier cells.\")\n",
    "\n",
    "# ============================\n",
    "# RUNNER: one run = 3 episodes continuous\n",
    "# ============================\n",
    "def run_one_run_svgp(run_id, seed_base=0):\n",
    "    # ---- FRESH RUN RESET (critical!) ----\n",
    "    # Z init must exist from earlier cells\n",
    "    if \"Z_GLOBAL\" not in globals():\n",
    "        raise NameError(\"Z_GLOBAL must be defined in earlier cells (initial inducing points in feature space).\")\n",
    "    Z0 = np.asarray(Z_GLOBAL, dtype=np.float64)\n",
    "    if Z0.shape[0] < M_GLOBAL:\n",
    "        raise ValueError(f\"Z_GLOBAL has {Z0.shape[0]} points but M_GLOBAL={M_GLOBAL}. Provide at least {M_GLOBAL} points.\")\n",
    "    Z0 = Z0[:M_GLOBAL].copy()\n",
    "\n",
    "    # Fresh models per run\n",
    "    m_dx, m_dxdot, m_dth, m_dthdot = make_four_head_svgp_models(Z0)\n",
    "\n",
    "    # Fresh dataset per run (but accumulates across episodes within run)\n",
    "    Xall, ydx_all, ydxdot_all, ydth_all, ydthdot_all = [], [], [], [], []\n",
    "\n",
    "    # timing traces per timestep (for the whole run)\n",
    "    train_time_step = []\n",
    "    pred_time_step  = []\n",
    "    wall_time_cum_excl_vis = []\n",
    "    train_time_cum = []\n",
    "    pred_time_cum  = []\n",
    "\n",
    "    # 1 render per run (episode 0 only)\n",
    "    frames = []\n",
    "    html = None\n",
    "    vis_time_s_total = 0.0\n",
    "\n",
    "    # global cumulative counters\n",
    "    t_global = 0\n",
    "    total_reward_run = 0.0\n",
    "    updates_run = 0\n",
    "\n",
    "    # warmup compile once (optional)\n",
    "    pred_bundle_tf, unc_var_fn_tf = make_global_tf_predictors_bundle_from_svgp(m_dx, m_dxdot, m_dth, m_dthdot)\n",
    "    if USE_GPU_MPPI and WARMUP_MPPI:\n",
    "        s_dummy = np.array([0.0, 0.0, np.pi, 0.0], dtype=np.float64)\n",
    "        u_mean0 = np.zeros((HORIZON,), dtype=np.float64)\n",
    "        _ = mppi_plan_gpu_global(s_dummy, x_init=0.0, u_init=u_mean0, t_global=0,\n",
    "                                pred_bundle_tf=pred_bundle_tf, unc_var_fn_tf=unc_var_fn_tf,\n",
    "                                base_seed=seed_base + 1000*run_id)\n",
    "\n",
    "    t_wall_start = time.perf_counter()\n",
    "    wall_excl_vis = 0.0\n",
    "    train_cum = 0.0\n",
    "    pred_cum  = 0.0\n",
    "\n",
    "    # ---- 3 continuous episodes ----\n",
    "    for ep in range(EPISODES_PER_RUN):\n",
    "        seed = int(seed_base + 1000 * run_id + ep)\n",
    "        env = make_env(render_mode=None, seed=seed, max_episode_steps=MAX_STEPS_PER_EP, start_down=START_DOWN)\n",
    "        obs, info = env.reset(seed=seed)\n",
    "        s = np.array(obs_to_state(obs), dtype=np.float64)\n",
    "        x_init = float(s[0])\n",
    "\n",
    "        u_mean = np.zeros((HORIZON,), dtype=np.float64)\n",
    "        hold_count = 0\n",
    "        record_this_ep = (ep == 0) and RECORD_RGB\n",
    "\n",
    "        if VERBOSE:\n",
    "            print(f\"\\n[SVGP] Run {run_id+1}/{N_RUNS} | Episode {ep+1}/{EPISODES_PER_RUN} | seed={seed}\")\n",
    "\n",
    "        for t_ep in range(MAX_STEPS_PER_EP):\n",
    "            # ---- prediction / planning (time this) ----\n",
    "            t0 = time.perf_counter()\n",
    "            u0, u_mean, tubeX, unc_w = mppi_plan_gpu_global(\n",
    "                state=s, x_init=x_init, u_init=u_mean, t_global=t_global,\n",
    "                pred_bundle_tf=pred_bundle_tf, unc_var_fn_tf=unc_var_fn_tf,\n",
    "                base_seed=seed_base + 1000*run_id\n",
    "            )\n",
    "            t1 = time.perf_counter()\n",
    "            pred_dt = (t1 - t0)\n",
    "            pred_cum += pred_dt\n",
    "\n",
    "            # ---- step env ----\n",
    "            obs2, r, terminated, truncated, info = env.step(np.array([u0], dtype=np.float32))\n",
    "            s2 = np.array(obs_to_state(obs2), dtype=np.float64)\n",
    "            total_reward_run += float(r)\n",
    "\n",
    "            # buffer data (skip respawned if your env uses it)\n",
    "            respawned = bool(isinstance(info, dict) and info.get(\"respawned\", False))\n",
    "            if not respawned:\n",
    "                Xall.append(state_to_features(s[0], s[1], s[2], s[3], float(u0)))\n",
    "                ydx_all.append([s2[0] - s[0]])\n",
    "                ydxdot_all.append([s2[1] - s[1]])\n",
    "                ydth_all.append([wrap_pi(s2[2] - s[2])])\n",
    "                ydthdot_all.append([s2[3] - s[3]])\n",
    "\n",
    "            # ---- render (excluded from wall_excl_vis) ----\n",
    "            if record_this_ep and (t_global % FRAME_STRIDE == 0):\n",
    "                tv0 = time.perf_counter()\n",
    "                try:\n",
    "                    frames.append(_render_frame_from_state(s2))\n",
    "                except Exception:\n",
    "                    pass\n",
    "                vis_time_s_total += (time.perf_counter() - tv0)\n",
    "\n",
    "            # ---- success ----\n",
    "            hold_count, success, upright, centered = success_hold_update(s2, x_init, hold_count)\n",
    "\n",
    "            if VERBOSE and (t_global % 50 == 0):\n",
    "                print(f\"[t={t_global:04d}] u0={u0:+.2f} unc_w={unc_w:.2f} upright={upright} centered={centered} hold={hold_count}/{HOLD_STEPS}\")\n",
    "\n",
    "            # ---- online retrain (batch SVGP on ALL data so far) ----\n",
    "            train_dt = 0.0\n",
    "            if ((t_global + 1) % UPDATE_EVERY == 0) and (len(Xall) >= 10):\n",
    "                updates_run += 1\n",
    "\n",
    "                Xnp   = np.asarray(Xall, dtype=np.float64)\n",
    "                ydx   = np.asarray(ydx_all, dtype=np.float64)\n",
    "                ydxd  = np.asarray(ydxdot_all, dtype=np.float64)\n",
    "                ydth  = np.asarray(ydth_all, dtype=np.float64)\n",
    "                ydthd = np.asarray(ydthdot_all, dtype=np.float64)\n",
    "\n",
    "                # train each head (time total training)\n",
    "                tt0 = time.perf_counter()\n",
    "                t_dx,   _ = train_svgp_batch(m_dx,     Xnp, ydx,   iters=ITERS_UPDATE, lr=LR_UPDATE)\n",
    "                t_v,    _ = train_svgp_batch(m_dxdot,  Xnp, ydxd,  iters=ITERS_UPDATE, lr=LR_UPDATE)\n",
    "                t_th,   _ = train_svgp_batch(m_dth,    Xnp, ydth,  iters=ITERS_UPDATE, lr=LR_UPDATE)\n",
    "                t_thd,  _ = train_svgp_batch(m_dthdot, Xnp, ydthd, iters=ITERS_UPDATE, lr=LR_UPDATE)\n",
    "                train_dt = time.perf_counter() - tt0\n",
    "\n",
    "                train_cum += train_dt\n",
    "\n",
    "                # refresh TF predictor bundle (same objects, but safe)\n",
    "                pred_bundle_tf, unc_var_fn_tf = make_global_tf_predictors_bundle_from_svgp(m_dx, m_dxdot, m_dth, m_dthdot)\n",
    "\n",
    "            # ---- timing logs (per timestep in the run) ----\n",
    "            total_wall = time.perf_counter() - t_wall_start\n",
    "            wall_excl_vis = max(total_wall - vis_time_s_total, 0.0)\n",
    "\n",
    "            train_time_step.append(float(train_dt))\n",
    "            pred_time_step.append(float(pred_dt))\n",
    "            train_time_cum.append(float(train_cum))\n",
    "            pred_time_cum.append(float(pred_cum))\n",
    "            wall_time_cum_excl_vis.append(float(wall_excl_vis))\n",
    "\n",
    "            # advance\n",
    "            s = s2\n",
    "            t_global += 1\n",
    "\n",
    "            # stop episode\n",
    "            if success:\n",
    "                if VERBOSE:\n",
    "                    print(f\"✅ SUCCESS at global t={t_global-1} (held {HOLD_STEPS} steps)\")\n",
    "                break\n",
    "            if terminated or truncated:\n",
    "                if VERBOSE:\n",
    "                    print(f\"Episode ended at global t={t_global-1} (terminated={terminated}, truncated={truncated})\")\n",
    "                break\n",
    "\n",
    "        env.close()\n",
    "\n",
    "    # ---- one render per run ----\n",
    "    if RECORD_RGB and (len(frames) > 0):\n",
    "        tv0 = time.perf_counter()\n",
    "        fig = plt.figure(figsize=(RESIZE[0]/100, RESIZE[1]/100), dpi=100)\n",
    "        plt.axis(\"off\")\n",
    "        im = plt.imshow(frames[0])\n",
    "\n",
    "        def animate_fn(i):\n",
    "            im.set_data(frames[i])\n",
    "            return [im]\n",
    "\n",
    "        ani = animation.FuncAnimation(fig, animate_fn, frames=len(frames),\n",
    "                                      interval=1000 / float(FPS), blit=True)\n",
    "        plt.close(fig)\n",
    "        html = HTML(ani.to_jshtml())\n",
    "        display(html)\n",
    "        vis_time_s_total += (time.perf_counter() - tv0)\n",
    "\n",
    "    run_stats = dict(\n",
    "        run_id=int(run_id),\n",
    "        total_reward=float(total_reward_run),\n",
    "        total_steps=int(len(train_time_step)),\n",
    "        updates=int(updates_run),\n",
    "        vis_time_s=float(vis_time_s_total),\n",
    "        wall_excl_vis_s=float(wall_time_cum_excl_vis[-1] if len(wall_time_cum_excl_vis)>0 else 0.0),\n",
    "        train_cum_s=float(train_time_cum[-1] if len(train_time_cum)>0 else 0.0),\n",
    "        pred_cum_s=float(pred_time_cum[-1] if len(pred_time_cum)>0 else 0.0),\n",
    "    )\n",
    "\n",
    "    traces = dict(\n",
    "        train_time_step=np.asarray(train_time_step, dtype=np.float64),\n",
    "        train_time_cum=np.asarray(train_time_cum, dtype=np.float64),\n",
    "        pred_time_step=np.asarray(pred_time_step, dtype=np.float64),\n",
    "        pred_time_cum=np.asarray(pred_time_cum, dtype=np.float64),\n",
    "        wall_time_cum_excl_vis=np.asarray(wall_time_cum_excl_vis, dtype=np.float64),\n",
    "    )\n",
    "\n",
    "    models = dict(m_dx=m_dx, m_dxdot=m_dxdot, m_dth=m_dth, m_dthdot=m_dthdot)\n",
    "\n",
    "    return run_stats, traces, models, html\n",
    "\n",
    "# ============================\n",
    "# MULTI-RUN DRIVER (3 runs)\n",
    "# ============================\n",
    "all_stats = []\n",
    "all_traces = []\n",
    "all_models = []\n",
    "\n",
    "for r in range(N_RUNS):\n",
    "    stats_r, traces_r, models_r, html_r = run_one_run_svgp(run_id=r, seed_base=0)\n",
    "    all_stats.append(stats_r)\n",
    "    all_traces.append(traces_r)\n",
    "    all_models.append(models_r)\n",
    "\n",
    "    print(f\"\\n[SVGP] Run {r+1} stats:\", stats_r)\n",
    "\n",
    "    # surface after each run\n",
    "    plot_global_surface_dxdot(\n",
    "        models_r[\"m_dxdot\"],\n",
    "        title=f\"SVGP baseline — GLOBAL Δxdot after Run {r+1} (mean colored by std)\"\n",
    "    )\n",
    "\n",
    "# ============================\n",
    "# MEAN PLOTS ACROSS RUNS (nan-mean over variable lengths)\n",
    "# ============================\n",
    "def nanpad_to(T, L):\n",
    "    out = np.full((L,), np.nan, dtype=np.float64)\n",
    "    n = min(L, len(T))\n",
    "    out[:n] = T[:n]\n",
    "    return out\n",
    "\n",
    "maxL = max([len(tr[\"train_time_step\"]) for tr in all_traces]) if len(all_traces)>0 else 0\n",
    "if maxL == 0:\n",
    "    raise RuntimeError(\"No timesteps were recorded (did the env terminate immediately?).\")\n",
    "\n",
    "train_step_mat = np.vstack([nanpad_to(tr[\"train_time_step\"], maxL) for tr in all_traces])\n",
    "train_cum_mat  = np.vstack([nanpad_to(tr[\"train_time_cum\"], maxL) for tr in all_traces])\n",
    "pred_step_mat  = np.vstack([nanpad_to(tr[\"pred_time_step\"], maxL) for tr in all_traces])\n",
    "wall_cum_mat   = np.vstack([nanpad_to(tr[\"wall_time_cum_excl_vis\"], maxL) for tr in all_traces])\n",
    "\n",
    "train_step_mean = np.nanmean(train_step_mat, axis=0)\n",
    "train_cum_mean  = np.nanmean(train_cum_mat, axis=0)\n",
    "pred_step_mean  = np.nanmean(pred_step_mat, axis=0)\n",
    "wall_cum_mean   = np.nanmean(wall_cum_mat, axis=0)\n",
    "\n",
    "t_axis = np.arange(maxL)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(t_axis, train_step_mean)\n",
    "plt.title(\"SVGP baseline — mean TRAIN time per timestep (non-cumulative) across runs\")\n",
    "plt.xlabel(\"global timestep (within run)\")\n",
    "plt.ylabel(\"train time at step (s)\")\n",
    "plt.grid(True, alpha=0.25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(t_axis, train_cum_mean)\n",
    "plt.title(\"SVGP baseline — mean TRAIN time cumulative across runs\")\n",
    "plt.xlabel(\"global timestep (within run)\")\n",
    "plt.ylabel(\"cumulative train time (s)\")\n",
    "plt.grid(True, alpha=0.25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(t_axis, pred_step_mean)\n",
    "plt.title(\"SVGP baseline — mean PRED/MPPI time per timestep across runs\")\n",
    "plt.xlabel(\"global timestep (within run)\")\n",
    "plt.ylabel(\"prediction/planning time at step (s)\")\n",
    "plt.grid(True, alpha=0.25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(t_axis, wall_cum_mean)\n",
    "plt.title(\"SVGP baseline — mean WALL time cumulative (excluding visualization) across runs\")\n",
    "plt.xlabel(\"global timestep (within run)\")\n",
    "plt.ylabel(\"cumulative wall time excl. vis (s)\")\n",
    "plt.grid(True, alpha=0.25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n[SVGP baseline] Summary across runs:\")\n",
    "for s in all_stats:\n",
    "    print(s)\n"
   ],
   "id": "4660ba4ae3348423",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================\n",
    "# Cell — BASELINE: Exact GP (4-head) + MPPI + Online retrain (batch on ALL data so far)\n",
    "#   Mirrors your SVGP baseline protocol/logs.\n",
    "# ============================\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML, display\n",
    "import plotly.graph_objects as go\n",
    "import gpflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# -----------------------\n",
    "# REQUIRED symbols from earlier cells\n",
    "# -----------------------\n",
    "required = [\n",
    "    \"make_env\", \"obs_to_state\", \"wrap_pi\",\n",
    "    \"state_to_features\", \"batch_state_to_features\",\n",
    "]\n",
    "missing = [k for k in required if k not in globals()]\n",
    "if len(missing) > 0:\n",
    "    raise NameError(f\"ExactGP cell missing required symbols from earlier cells: {missing}\")\n",
    "\n",
    "# -----------------------\n",
    "# Dtypes (float64 everywhere)\n",
    "# -----------------------\n",
    "NP_DTYPE = np.float64\n",
    "TF_DTYPE = gpflow.default_float()\n",
    "if TF_DTYPE != tf.float64:\n",
    "    print(\"⚠️ gpflow.default_float() is not float64. Set gpflow.config.set_default_float(np.float64) in Cell 1.\")\n",
    "\n",
    "# -----------------------\n",
    "# Config defaults (prefer your globals if already defined)\n",
    "# -----------------------\n",
    "HORIZON    = int(globals().get(\"HORIZON\", 50))\n",
    "K_SAMPLES  = int(globals().get(\"K_SAMPLES\", 256))\n",
    "SIGMA      = float(globals().get(\"SIGMA\", 0.6))\n",
    "LAMBDA     = float(globals().get(\"LAMBDA\", 1.0))\n",
    "\n",
    "UPDATE_EVERY = int(globals().get(\"UPDATE_EVERY\", 30))\n",
    "\n",
    "# For Exact GP, DO NOT use \"iters=150\" like Adam loops.\n",
    "# Use a small number of L-BFGS iterations per update, otherwise it will be too slow.\n",
    "EXACT_SCIPY_ITERS = int(globals().get(\"EXACT_SCIPY_ITERS\", 20))\n",
    "\n",
    "NOISE_UPDATE = float(globals().get(\"NOISE_UPDATE\", 1e-4))\n",
    "\n",
    "# task/success\n",
    "X_BAND      = float(globals().get(\"X_BAND\", 0.70))\n",
    "UPRIGHT_COS = float(globals().get(\"UPRIGHT_COS\", 0.85))\n",
    "HOLD_STEPS  = int(globals().get(\"HOLD_STEPS\", 200))\n",
    "\n",
    "# exploration schedule\n",
    "EXPLORE_STEPS = int(globals().get(\"EXPLORE_STEPS\", 200))\n",
    "UNC_W_MAX     = float(globals().get(\"UNC_W_MAX\", 15.0))\n",
    "UNC_W_MIN     = float(globals().get(\"UNC_W_MIN\", 0.0))\n",
    "CENTER_W      = float(globals().get(\"CENTER_W\", 1.0))\n",
    "U_W           = float(globals().get(\"U_W\", 0.005))\n",
    "UPRIGHT_W     = float(globals().get(\"UPRIGHT_W\", 2.0))\n",
    "\n",
    "# action bounds\n",
    "U_MIN = float(globals().get(\"U_MIN\", -1.0))\n",
    "U_MAX = float(globals().get(\"U_MAX\", +1.0))\n",
    "\n",
    "# rendering\n",
    "RECORD_RGB   = True\n",
    "RESIZE       = globals().get(\"RESIZE\", (720, 450))\n",
    "FPS          = int(globals().get(\"FPS\", 10))\n",
    "FRAME_STRIDE = int(globals().get(\"FRAME_STRIDE\", 2))\n",
    "\n",
    "# multi-run protocol\n",
    "N_RUNS = 3\n",
    "EPISODES_PER_RUN = 3\n",
    "MAX_STEPS_PER_EP = int(globals().get(\"MAX_STEPS_PER_EP\", 1000))\n",
    "START_DOWN = bool(globals().get(\"START_DOWN\", True))\n",
    "USE_GPU_MPPI = True\n",
    "WARMUP_MPPI = True\n",
    "VERBOSE = True\n",
    "\n",
    "# -----------------------\n",
    "# Fixed plot grid (for surface after each run)\n",
    "# -----------------------\n",
    "PLOT_X_MIN, PLOT_X_MAX = globals().get(\"PLOT_X_MIN\", -2.4), globals().get(\"PLOT_X_MAX\", 2.4)\n",
    "PLOT_V_MIN, PLOT_V_MAX = globals().get(\"PLOT_V_MIN\", -3.0), globals().get(\"PLOT_V_MAX\", 3.0)\n",
    "PLOT_N_GRID            = int(globals().get(\"PLOT_N_GRID\", 60))\n",
    "PLOT_TH_FIXED          = float(globals().get(\"PLOT_TH_FIXED\", 0.0))\n",
    "PLOT_THDOT_FIXED       = float(globals().get(\"PLOT_THDOT_FIXED\", 0.0))\n",
    "PLOT_U_FIXED           = float(globals().get(\"PLOT_U_FIXED\", +1.0))\n",
    "\n",
    "def build_fixed_plot_grid():\n",
    "    xg = np.linspace(PLOT_X_MIN, PLOT_X_MAX, PLOT_N_GRID)\n",
    "    vg = np.linspace(PLOT_V_MIN, PLOT_V_MAX, PLOT_N_GRID)\n",
    "    X, V = np.meshgrid(xg, vg)\n",
    "    Xfeat_grid = np.vstack([\n",
    "        state_to_features(x, xdot, PLOT_TH_FIXED, PLOT_THDOT_FIXED, PLOT_U_FIXED)\n",
    "        for x, xdot in zip(X.ravel(), V.ravel())\n",
    "    ]).astype(np.float64)\n",
    "    return X, V, Xfeat_grid\n",
    "\n",
    "PLOT_XMESH, PLOT_VMESH, PLOT_XFEAT_GRID = build_fixed_plot_grid()\n",
    "\n",
    "def gp_predict_mu_std(model, X):\n",
    "    Xtf = tf.convert_to_tensor(np.asarray(X, dtype=np.float64), dtype=TF_DTYPE)\n",
    "    mu_tf, var_tf = model.predict_f(Xtf, full_cov=False)\n",
    "    mu = mu_tf.numpy().reshape(-1)\n",
    "    var = var_tf.numpy().reshape(-1)\n",
    "    std = np.sqrt(np.maximum(var, 1e-12))\n",
    "    return mu, std\n",
    "\n",
    "def plot_global_surface_dxdot(model_dxdot, title=\"GLOBAL Δxdot surface (Exact GP)\"):\n",
    "    mu, std = gp_predict_mu_std(model_dxdot, PLOT_XFEAT_GRID)\n",
    "    Mean = mu.reshape(PLOT_XMESH.shape)\n",
    "    Std  = std.reshape(PLOT_XMESH.shape)\n",
    "\n",
    "    fig = go.Figure(data=[\n",
    "        go.Surface(\n",
    "            x=PLOT_XMESH, y=PLOT_VMESH, z=Mean,\n",
    "            surfacecolor=Std,\n",
    "            colorscale=\"Viridis\",\n",
    "            showscale=True,\n",
    "            colorbar=dict(title=\"Std\"),\n",
    "            opacity=0.95\n",
    "        )\n",
    "    ])\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        scene=dict(\n",
    "            xaxis=dict(title=\"x\", range=[PLOT_X_MIN, PLOT_X_MAX]),\n",
    "            yaxis=dict(title=\"xdot\", range=[PLOT_V_MIN, PLOT_V_MAX]),\n",
    "            zaxis=dict(title=\"Δxdot\"),\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, b=0, t=50),\n",
    "        height=620\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "# -----------------------\n",
    "# Success check (same logic as your other cells)\n",
    "# -----------------------\n",
    "def success_hold_update(state, x_init, hold_count):\n",
    "    x, xdot, th, thdot = state\n",
    "    upright = (np.cos(th) >= UPRIGHT_COS)\n",
    "    centered = (abs(x - x_init) <= X_BAND)\n",
    "    hold_count = (hold_count + 1) if (upright and centered) else 0\n",
    "    success = (hold_count >= HOLD_STEPS)\n",
    "    return hold_count, success, upright, centered\n",
    "\n",
    "# -----------------------\n",
    "# Cost + exploration weight (same functional form as your SVGP cell)\n",
    "# -----------------------\n",
    "@tf.function\n",
    "def exploration_weight_tf(t):\n",
    "    explore_steps_f = tf.cast(EXPLORE_STEPS, TF_DTYPE)\n",
    "    t_f = tf.cast(t, TF_DTYPE)\n",
    "    a = tf.clip_by_value(1.0 - t_f / tf.maximum(explore_steps_f, 1.0), 0.0, 1.0)\n",
    "    return tf.cast(UNC_W_MIN, TF_DTYPE) + (tf.cast(UNC_W_MAX, TF_DTYPE) - tf.cast(UNC_W_MIN, TF_DTYPE)) * a\n",
    "\n",
    "@tf.function\n",
    "def wrap_pi_tf(theta):\n",
    "    two_pi = tf.constant(2.0 * np.pi, dtype=TF_DTYPE)\n",
    "    pi = tf.constant(np.pi, dtype=TF_DTYPE)\n",
    "    return tf.math.floormod(theta + pi, two_pi) - pi\n",
    "\n",
    "@tf.function\n",
    "def batch_state_to_features_tf(S, U):\n",
    "    x = S[:, 0]\n",
    "    xdot = S[:, 1]\n",
    "    th = S[:, 2]\n",
    "    thdot = S[:, 3]\n",
    "\n",
    "    f0 = tf.tanh(x / tf.constant(2.4, dtype=TF_DTYPE))\n",
    "    f1 = tf.tanh(xdot / tf.constant(3.0, dtype=TF_DTYPE))\n",
    "    f2 = tf.sin(th)\n",
    "    f3 = tf.cos(th)\n",
    "    f4 = tf.tanh(thdot / tf.constant(8.0, dtype=TF_DTYPE))\n",
    "    f5 = U\n",
    "    return tf.stack([f0, f1, f2, f3, f4, f5], axis=1)\n",
    "\n",
    "@tf.function\n",
    "def stage_cost_cartpole_tf(S, U, x_init, unc_bonus=None, unc_w=0.0):\n",
    "    th = S[:, 2]\n",
    "    x  = S[:, 0]\n",
    "    c = (\n",
    "        tf.cast(UPRIGHT_W, TF_DTYPE) * (1.0 - tf.cos(th))\n",
    "        + tf.cast(CENTER_W, TF_DTYPE) * tf.square(x - x_init)\n",
    "        + tf.cast(U_W, TF_DTYPE) * tf.square(U)\n",
    "    )\n",
    "    if (unc_bonus is not None) and (unc_w > 0.0):\n",
    "        c = c - tf.cast(unc_w, TF_DTYPE) * tf.cast(unc_bonus, TF_DTYPE)\n",
    "    return c\n",
    "\n",
    "@tf.function\n",
    "def terminal_cost_hold_like_tf(S, x_init):\n",
    "    th = S[:, 2]\n",
    "    x  = S[:, 0]\n",
    "    cT = tf.zeros_like(th)\n",
    "    good = tf.logical_and(tf.cos(th) >= tf.cast(UPRIGHT_COS, TF_DTYPE),\n",
    "                          tf.abs(x - x_init) <= tf.cast(X_BAND, TF_DTYPE))\n",
    "    cT = tf.where(good, cT - tf.cast(5.0, TF_DTYPE), cT)\n",
    "    return cT\n",
    "\n",
    "# -----------------------\n",
    "# Exact GP model factory (4 heads, GPR)\n",
    "# -----------------------\n",
    "def make_gpr_head(X0, Y0, kernel=None, noise=1e-4):\n",
    "    if kernel is None:\n",
    "        kernel = gpflow.kernels.SquaredExponential(lengthscales=np.ones((6,), dtype=np.float64), variance=1.0)\n",
    "\n",
    "    Xtf = tf.convert_to_tensor(np.asarray(X0, dtype=np.float64), dtype=TF_DTYPE)\n",
    "    Ytf = tf.convert_to_tensor(np.asarray(Y0, dtype=np.float64).reshape(-1,1), dtype=TF_DTYPE)\n",
    "\n",
    "    m = gpflow.models.GPR(data=(Xtf, Ytf), kernel=kernel, mean_function=None)\n",
    "    m.likelihood.variance.assign(float(noise))\n",
    "    return m\n",
    "\n",
    "def make_four_head_gpr_models(X0, ydx0, ydxdot0, ydth0, ydthdot0):\n",
    "    def _k():\n",
    "        return gpflow.kernels.SquaredExponential(lengthscales=np.ones((6,), dtype=np.float64), variance=1.0)\n",
    "    m_dx     = make_gpr_head(X0, ydx0,     kernel=_k(), noise=NOISE_UPDATE)\n",
    "    m_dxdot  = make_gpr_head(X0, ydxdot0,  kernel=_k(), noise=NOISE_UPDATE)\n",
    "    m_dth    = make_gpr_head(X0, ydth0,    kernel=_k(), noise=NOISE_UPDATE)\n",
    "    m_dthdot = make_gpr_head(X0, ydthdot0, kernel=_k(), noise=NOISE_UPDATE)\n",
    "    return m_dx, m_dxdot, m_dth, m_dthdot\n",
    "\n",
    "def set_gpr_data(model, X, Y):\n",
    "    Xtf = tf.convert_to_tensor(np.asarray(X, dtype=np.float64), dtype=TF_DTYPE)\n",
    "    Ytf = tf.convert_to_tensor(np.asarray(Y, dtype=np.float64).reshape(-1,1), dtype=TF_DTYPE)\n",
    "    model.data = (Xtf, Ytf)\n",
    "\n",
    "def train_gpr_scipy(model, maxiter=50):\n",
    "    # Optimize kernel hypers + noise (you can freeze things if desired)\n",
    "    opt = gpflow.optimizers.Scipy()\n",
    "    t0 = time.perf_counter()\n",
    "    res = opt.minimize(\n",
    "        model.training_loss,\n",
    "        variables=model.trainable_variables,\n",
    "        method=\"L-BFGS-B\",\n",
    "        options=dict(maxiter=int(maxiter), disp=False),\n",
    "    )\n",
    "    t_train = time.perf_counter() - t0\n",
    "    loss_val = float(model.training_loss().numpy())\n",
    "    return float(t_train), float(loss_val)\n",
    "\n",
    "# -----------------------\n",
    "# GLOBAL predictors for TF MPPI (Exact GP)\n",
    "# -----------------------\n",
    "def make_global_tf_predictors_bundle_from_gpr(m_dx, m_dxdot, m_dth, m_dthdot):\n",
    "    @tf.function\n",
    "    def dx_mu(Xfeat):\n",
    "        mu, _ = m_dx.predict_f(Xfeat, full_cov=False)\n",
    "        return tf.reshape(mu, (-1,))\n",
    "\n",
    "    @tf.function\n",
    "    def dxdot_mu(Xfeat):\n",
    "        mu, _ = m_dxdot.predict_f(Xfeat, full_cov=False)\n",
    "        return tf.reshape(mu, (-1,))\n",
    "\n",
    "    @tf.function\n",
    "    def dth_mu(Xfeat):\n",
    "        mu, _ = m_dth.predict_f(Xfeat, full_cov=False)\n",
    "        return tf.reshape(mu, (-1,))\n",
    "\n",
    "    @tf.function\n",
    "    def dthdot_mu(Xfeat):\n",
    "        mu, _ = m_dthdot.predict_f(Xfeat, full_cov=False)\n",
    "        return tf.reshape(mu, (-1,))\n",
    "\n",
    "    @tf.function\n",
    "    def dxdot_var(Xfeat):\n",
    "        _, v = m_dxdot.predict_f(Xfeat, full_cov=False)\n",
    "        return tf.reshape(v, (-1,))\n",
    "\n",
    "    return (dx_mu, dxdot_mu, dth_mu, dthdot_mu), dxdot_var\n",
    "\n",
    "@tf.function\n",
    "def gp_dynamics_step_batch_tf(S, U, pred_bundle_tf):\n",
    "    pred_dx_tf, pred_dxdot_tf, pred_dth_tf, pred_dthdot_tf = pred_bundle_tf\n",
    "    Xfeat = batch_state_to_features_tf(S, U)\n",
    "    dx     = pred_dx_tf(Xfeat)\n",
    "    dxdot  = pred_dxdot_tf(Xfeat)\n",
    "    dth    = pred_dth_tf(Xfeat)\n",
    "    dthdot = pred_dthdot_tf(Xfeat)\n",
    "\n",
    "    S2 = tf.stack([\n",
    "        S[:, 0] + dx,\n",
    "        S[:, 1] + dxdot,\n",
    "        wrap_pi_tf(S[:, 2] + dth),\n",
    "        S[:, 3] + dthdot\n",
    "    ], axis=1)\n",
    "    return S2, Xfeat\n",
    "\n",
    "@tf.function\n",
    "def rollout_tube_features_tf(state0, u_seq, pred_bundle_tf):\n",
    "    H = tf.shape(u_seq)[0]\n",
    "    s = tf.identity(state0)\n",
    "    tube = tf.TensorArray(dtype=TF_DTYPE, size=H)\n",
    "\n",
    "    t = tf.constant(0)\n",
    "    def cond(t, s, tube): return t < H\n",
    "\n",
    "    def body(t, s, tube):\n",
    "        u = u_seq[t]\n",
    "        xfeat = batch_state_to_features_tf(tf.expand_dims(s, axis=0), tf.expand_dims(u, axis=0))[0]\n",
    "        tube = tube.write(t, xfeat)\n",
    "        s2, _ = gp_dynamics_step_batch_tf(tf.expand_dims(s, axis=0), tf.expand_dims(u, axis=0), pred_bundle_tf)\n",
    "        return t+1, s2[0], tube\n",
    "\n",
    "    _, _, tube = tf.while_loop(cond, body, [t, s, tube], parallel_iterations=1)\n",
    "    return tube.stack()\n",
    "\n",
    "@tf.function\n",
    "def mppi_plan_gpu_global_tf(state0, x_init, u_mean0, t_global,\n",
    "                           pred_bundle_tf, unc_var_fn_tf,\n",
    "                           horizon=HORIZON, K=K_SAMPLES, sigma=SIGMA, lam=LAMBDA,\n",
    "                           base_seed=0):\n",
    "    H = tf.cast(horizon, tf.int32)\n",
    "    Kt = tf.cast(K, tf.int32)\n",
    "\n",
    "    seed = tf.stack([tf.cast(base_seed, tf.int32), tf.cast(t_global, tf.int32)], axis=0)\n",
    "    eps = tf.random.stateless_normal((Kt, H), seed=seed, mean=0.0, stddev=tf.cast(sigma, TF_DTYPE), dtype=TF_DTYPE)\n",
    "    U = tf.clip_by_value(u_mean0[None, :] + eps, tf.cast(U_MIN, TF_DTYPE), tf.cast(U_MAX, TF_DTYPE))\n",
    "\n",
    "    S = tf.tile(state0[None, :], [Kt, 1])\n",
    "    total_cost = tf.zeros((Kt,), dtype=TF_DTYPE)\n",
    "\n",
    "    unc_w = exploration_weight_tf(t_global)\n",
    "\n",
    "    t = tf.constant(0, dtype=tf.int32)\n",
    "    def cond(t, S, total_cost): return t < H\n",
    "\n",
    "    def body(t, S, total_cost):\n",
    "        Ut = U[:, t]\n",
    "        S2, Xfeat = gp_dynamics_step_batch_tf(S, Ut, pred_bundle_tf)\n",
    "        unc_bonus = unc_var_fn_tf(Xfeat)\n",
    "        total_cost = total_cost + stage_cost_cartpole_tf(\n",
    "            S, Ut, x_init=tf.cast(x_init, TF_DTYPE),\n",
    "            unc_bonus=unc_bonus, unc_w=unc_w\n",
    "        )\n",
    "        return t+1, S2, total_cost\n",
    "\n",
    "    _, S, total_cost = tf.while_loop(cond, body, [t, S, total_cost], parallel_iterations=1)\n",
    "    total_cost = total_cost + terminal_cost_hold_like_tf(S, x_init=tf.cast(x_init, TF_DTYPE))\n",
    "\n",
    "    cmin = tf.reduce_min(total_cost)\n",
    "    w = tf.exp(-(total_cost - cmin) / tf.cast(lam, TF_DTYPE))\n",
    "    wsum = tf.reduce_sum(w) + tf.cast(1e-12, TF_DTYPE)\n",
    "\n",
    "    u_mean = u_mean0 + tf.reduce_sum(w[:, None] * eps, axis=0) / wsum\n",
    "    u_mean = tf.clip_by_value(u_mean, tf.cast(U_MIN, TF_DTYPE), tf.cast(U_MAX, TF_DTYPE))\n",
    "\n",
    "    tubeX = rollout_tube_features_tf(state0, u_mean, pred_bundle_tf)\n",
    "    return u_mean[0], u_mean, tubeX, unc_w\n",
    "\n",
    "def mppi_plan_gpu_global(state, x_init, u_init, t_global, pred_bundle_tf, unc_var_fn_tf, base_seed=0):\n",
    "    state0 = tf.convert_to_tensor(np.asarray(state, dtype=np.float64).reshape(4,), dtype=TF_DTYPE)\n",
    "    x0     = tf.convert_to_tensor(float(x_init), dtype=TF_DTYPE)\n",
    "    u0     = tf.convert_to_tensor(np.asarray(u_init, dtype=np.float64).reshape(-1,), dtype=TF_DTYPE)\n",
    "\n",
    "    dev = \"/GPU:0\" if len(tf.config.list_logical_devices(\"GPU\")) > 0 else \"/CPU:0\"\n",
    "    with tf.device(dev):\n",
    "        u_first, u_mean, tubeX, unc_w = mppi_plan_gpu_global_tf(\n",
    "            state0, x0, u0, tf.convert_to_tensor(int(t_global), dtype=tf.int32),\n",
    "            pred_bundle_tf=pred_bundle_tf,\n",
    "            unc_var_fn_tf=unc_var_fn_tf,\n",
    "            horizon=HORIZON, K=K_SAMPLES, sigma=SIGMA, lam=LAMBDA,\n",
    "            base_seed=int(base_seed)\n",
    "        )\n",
    "    return float(u_first.numpy()), u_mean.numpy(), tubeX.numpy(), float(unc_w.numpy())\n",
    "\n",
    "# -----------------------\n",
    "# Rendering helper compatibility\n",
    "# -----------------------\n",
    "def _render_frame_from_state(s):\n",
    "    W, H = int(RESIZE[0]), int(RESIZE[1])\n",
    "    if \"render_cartpole_frame_from_state\" in globals():\n",
    "        return render_cartpole_frame_from_state(x=s[0], theta=s[2], x_threshold=2.4, W=W, H=H)\n",
    "    raise NameError(\"No render_cartpole_frame_from_state(...) found; set RECORD_RGB=False or define the renderer earlier.\")\n",
    "\n",
    "# ============================\n",
    "# RUNNER: one run = 3 episodes continuous\n",
    "# ============================\n",
    "def run_one_run_exactgp(run_id, seed_base=0):\n",
    "    # Seed data warm start: if you already have X0/Y0 from earlier cells, use them; else create tiny dummy.\n",
    "    if \"X0\" in globals() and \"Ydx0\" in globals():\n",
    "        X0 = np.asarray(globals()[\"X0\"], dtype=np.float64)\n",
    "        ydx0 = np.asarray(globals()[\"Ydx0\"], dtype=np.float64).reshape(-1,1)\n",
    "        ydxdot0 = np.asarray(globals()[\"Ydxdot0\"], dtype=np.float64).reshape(-1,1)\n",
    "        ydth0 = np.asarray(globals()[\"Ydth0\"], dtype=np.float64).reshape(-1,1)\n",
    "        ydthdot0 = np.asarray(globals()[\"Ydthdot0\"], dtype=np.float64).reshape(-1,1)\n",
    "    else:\n",
    "        # minimal seed to instantiate models\n",
    "        X0 = np.zeros((5, 6), dtype=np.float64)\n",
    "        ydx0 = np.zeros((5, 1), dtype=np.float64)\n",
    "        ydxdot0 = np.zeros((5, 1), dtype=np.float64)\n",
    "        ydth0 = np.zeros((5, 1), dtype=np.float64)\n",
    "        ydthdot0 = np.zeros((5, 1), dtype=np.float64)\n",
    "\n",
    "    # Fresh models per run\n",
    "    m_dx, m_dxdot, m_dth, m_dthdot = make_four_head_gpr_models(X0, ydx0, ydxdot0, ydth0, ydthdot0)\n",
    "\n",
    "    # Dataset per run (accumulates across episodes)\n",
    "    Xall, ydx_all, ydxdot_all, ydth_all, ydthdot_all = [], [], [], [], []\n",
    "    # Optionally start with seed\n",
    "    for i in range(X0.shape[0]):\n",
    "        Xall.append(X0[i])\n",
    "        ydx_all.append([float(ydx0[i])])\n",
    "        ydxdot_all.append([float(ydxdot0[i])])\n",
    "        ydth_all.append([float(ydth0[i])])\n",
    "        ydthdot_all.append([float(ydthdot0[i])])\n",
    "\n",
    "    train_time_step = []\n",
    "    pred_time_step  = []\n",
    "    wall_time_cum_excl_vis = []\n",
    "    train_time_cum = []\n",
    "    pred_time_cum  = []\n",
    "\n",
    "    frames = []\n",
    "    html = None\n",
    "    vis_time_s_total = 0.0\n",
    "\n",
    "    t_global = 0\n",
    "    total_reward_run = 0.0\n",
    "    updates_run = 0\n",
    "\n",
    "    pred_bundle_tf, unc_var_fn_tf = make_global_tf_predictors_bundle_from_gpr(m_dx, m_dxdot, m_dth, m_dthdot)\n",
    "\n",
    "    # Warmup compile once\n",
    "    if USE_GPU_MPPI and WARMUP_MPPI:\n",
    "        s_dummy = np.array([0.0, 0.0, np.pi, 0.0], dtype=np.float64)\n",
    "        u_mean0 = np.zeros((HORIZON,), dtype=np.float64)\n",
    "        _ = mppi_plan_gpu_global(\n",
    "            s_dummy, x_init=0.0, u_init=u_mean0, t_global=0,\n",
    "            pred_bundle_tf=pred_bundle_tf, unc_var_fn_tf=unc_var_fn_tf,\n",
    "            base_seed=seed_base + 1000*run_id\n",
    "        )\n",
    "\n",
    "    t_wall_start = time.perf_counter()\n",
    "    train_cum = 0.0\n",
    "    pred_cum  = 0.0\n",
    "\n",
    "    for ep in range(EPISODES_PER_RUN):\n",
    "        seed = int(seed_base + 1000 * run_id + ep)\n",
    "        env = make_env(render_mode=None, seed=seed, max_episode_steps=MAX_STEPS_PER_EP, start_down=START_DOWN)\n",
    "        obs, info = env.reset(seed=seed)\n",
    "        s = np.array(obs_to_state(obs), dtype=np.float64)\n",
    "        x_init = float(s[0])\n",
    "\n",
    "        u_mean = np.zeros((HORIZON,), dtype=np.float64)\n",
    "        hold_count = 0\n",
    "        record_this_ep = (ep == 0) and RECORD_RGB\n",
    "\n",
    "        if VERBOSE:\n",
    "            print(f\"\\n[ExactGP] Run {run_id+1}/{N_RUNS} | Episode {ep+1}/{EPISODES_PER_RUN} | seed={seed}\")\n",
    "\n",
    "        for t_ep in range(MAX_STEPS_PER_EP):\n",
    "            # planning time\n",
    "            t0 = time.perf_counter()\n",
    "            u0, u_mean, tubeX, unc_w = mppi_plan_gpu_global(\n",
    "                state=s, x_init=x_init, u_init=u_mean, t_global=t_global,\n",
    "                pred_bundle_tf=pred_bundle_tf, unc_var_fn_tf=unc_var_fn_tf,\n",
    "                base_seed=seed_base + 1000*run_id\n",
    "            )\n",
    "            pred_dt = time.perf_counter() - t0\n",
    "            pred_cum += pred_dt\n",
    "\n",
    "            # env step\n",
    "            obs2, r, terminated, truncated, info = env.step(np.array([u0], dtype=np.float32))\n",
    "            s2 = np.array(obs_to_state(obs2), dtype=np.float64)\n",
    "            total_reward_run += float(r)\n",
    "\n",
    "            # collect (skip respawns)\n",
    "            respawned = bool(isinstance(info, dict) and info.get(\"respawned\", False))\n",
    "            if not respawned:\n",
    "                Xall.append(state_to_features(s[0], s[1], s[2], s[3], float(u0)))\n",
    "                ydx_all.append([s2[0] - s[0]])\n",
    "                ydxdot_all.append([s2[1] - s[1]])\n",
    "                ydth_all.append([wrap_pi(s2[2] - s[2])])\n",
    "                ydthdot_all.append([s2[3] - s[3]])\n",
    "\n",
    "            # render\n",
    "            if record_this_ep and (t_global % FRAME_STRIDE == 0):\n",
    "                tv0 = time.perf_counter()\n",
    "                try:\n",
    "                    frames.append(_render_frame_from_state(s2))\n",
    "                except Exception:\n",
    "                    pass\n",
    "                vis_time_s_total += (time.perf_counter() - tv0)\n",
    "\n",
    "            # success\n",
    "            hold_count, success, upright, centered = success_hold_update(s2, x_init, hold_count)\n",
    "\n",
    "            if VERBOSE and (t_global % 50 == 0):\n",
    "                print(f\"[t={t_global:04d}] u0={u0:+.2f} unc_w={unc_w:.2f} upright={upright} centered={centered} hold={hold_count}/{HOLD_STEPS}\")\n",
    "\n",
    "            # online retrain (Exact GP on ALL data so far)\n",
    "            train_dt = 0.0\n",
    "            if ((t_global + 1) % UPDATE_EVERY == 0) and (len(Xall) >= 20):\n",
    "                updates_run += 1\n",
    "\n",
    "                Xnp   = np.asarray(Xall, dtype=np.float64)\n",
    "                ydx   = np.asarray(ydx_all, dtype=np.float64)\n",
    "                ydxd  = np.asarray(ydxdot_all, dtype=np.float64)\n",
    "                ydth  = np.asarray(ydth_all, dtype=np.float64)\n",
    "                ydthd = np.asarray(ydthdot_all, dtype=np.float64)\n",
    "\n",
    "                tt0 = time.perf_counter()\n",
    "\n",
    "                set_gpr_data(m_dx,     Xnp, ydx)\n",
    "                set_gpr_data(m_dxdot,  Xnp, ydxd)\n",
    "                set_gpr_data(m_dth,    Xnp, ydth)\n",
    "                set_gpr_data(m_dthdot, Xnp, ydthd)\n",
    "\n",
    "                # (optional) keep likelihood noise fixed\n",
    "                m_dx.likelihood.variance.assign(float(NOISE_UPDATE))\n",
    "                m_dxdot.likelihood.variance.assign(float(NOISE_UPDATE))\n",
    "                m_dth.likelihood.variance.assign(float(NOISE_UPDATE))\n",
    "                m_dthdot.likelihood.variance.assign(float(NOISE_UPDATE))\n",
    "\n",
    "                # optimize hypers\n",
    "                train_gpr_scipy(m_dx,     maxiter=EXACT_SCIPY_ITERS)\n",
    "                train_gpr_scipy(m_dxdot,  maxiter=EXACT_SCIPY_ITERS)\n",
    "                train_gpr_scipy(m_dth,    maxiter=EXACT_SCIPY_ITERS)\n",
    "                train_gpr_scipy(m_dthdot, maxiter=EXACT_SCIPY_ITERS)\n",
    "\n",
    "                train_dt = time.perf_counter() - tt0\n",
    "                train_cum += train_dt\n",
    "\n",
    "                pred_bundle_tf, unc_var_fn_tf = make_global_tf_predictors_bundle_from_gpr(m_dx, m_dxdot, m_dth, m_dthdot)\n",
    "\n",
    "            # timing logs\n",
    "            total_wall = time.perf_counter() - t_wall_start\n",
    "            wall_excl_vis = max(total_wall - vis_time_s_total, 0.0)\n",
    "\n",
    "            train_time_step.append(float(train_dt))\n",
    "            pred_time_step.append(float(pred_dt))\n",
    "            train_time_cum.append(float(train_cum))\n",
    "            pred_time_cum.append(float(pred_cum))\n",
    "            wall_time_cum_excl_vis.append(float(wall_excl_vis))\n",
    "\n",
    "            # advance\n",
    "            s = s2\n",
    "            t_global += 1\n",
    "\n",
    "            if success:\n",
    "                if VERBOSE:\n",
    "                    print(f\"✅ SUCCESS at global t={t_global-1} (held {HOLD_STEPS} steps)\")\n",
    "                break\n",
    "            if terminated or truncated:\n",
    "                if VERBOSE:\n",
    "                    print(f\"Episode ended at global t={t_global-1} (terminated={terminated}, truncated={truncated})\")\n",
    "                break\n",
    "\n",
    "        env.close()\n",
    "\n",
    "    # one render per run\n",
    "    if RECORD_RGB and (len(frames) > 0):\n",
    "        tv0 = time.perf_counter()\n",
    "        fig = plt.figure(figsize=(RESIZE[0]/100, RESIZE[1]/100), dpi=100)\n",
    "        plt.axis(\"off\")\n",
    "        im = plt.imshow(frames[0])\n",
    "\n",
    "        def animate_fn(i):\n",
    "            im.set_data(frames[i])\n",
    "            return [im]\n",
    "\n",
    "        ani = animation.FuncAnimation(fig, animate_fn, frames=len(frames),\n",
    "                                      interval=1000 / float(FPS), blit=True)\n",
    "        plt.close(fig)\n",
    "        html = HTML(ani.to_jshtml())\n",
    "        display(html)\n",
    "        vis_time_s_total += (time.perf_counter() - tv0)\n",
    "\n",
    "    run_stats = dict(\n",
    "        run_id=int(run_id),\n",
    "        total_reward=float(total_reward_run),\n",
    "        total_steps=int(len(train_time_step)),\n",
    "        updates=int(updates_run),\n",
    "        vis_time_s=float(vis_time_s_total),\n",
    "        wall_excl_vis_s=float(wall_time_cum_excl_vis[-1] if len(wall_time_cum_excl_vis)>0 else 0.0),\n",
    "        train_cum_s=float(train_time_cum[-1] if len(train_time_cum)>0 else 0.0),\n",
    "        pred_cum_s=float(pred_time_cum[-1] if len(pred_time_cum)>0 else 0.0),\n",
    "    )\n",
    "\n",
    "    traces = dict(\n",
    "        train_time_step=np.asarray(train_time_step, dtype=np.float64),\n",
    "        train_time_cum=np.asarray(train_time_cum, dtype=np.float64),\n",
    "        pred_time_step=np.asarray(pred_time_step, dtype=np.float64),\n",
    "        pred_time_cum=np.asarray(pred_time_cum, dtype=np.float64),\n",
    "        wall_time_cum_excl_vis=np.asarray(wall_time_cum_excl_vis, dtype=np.float64),\n",
    "    )\n",
    "\n",
    "    models = dict(m_dx=m_dx, m_dxdot=m_dxdot, m_dth=m_dth, m_dthdot=m_dthdot)\n",
    "\n",
    "    return run_stats, traces, models, html\n",
    "\n",
    "# ============================\n",
    "# MULTI-RUN DRIVER\n",
    "# ============================\n",
    "all_stats = []\n",
    "all_traces = []\n",
    "all_models = []\n",
    "\n",
    "for r in range(N_RUNS):\n",
    "    stats_r, traces_r, models_r, html_r = run_one_run_exactgp(run_id=r, seed_base=0)\n",
    "    all_stats.append(stats_r)\n",
    "    all_traces.append(traces_r)\n",
    "    all_models.append(models_r)\n",
    "\n",
    "    print(f\"\\n[ExactGP] Run {r+1} stats:\", stats_r)\n",
    "\n",
    "    plot_global_surface_dxdot(\n",
    "        models_r[\"m_dxdot\"],\n",
    "        title=f\"Exact GP baseline — GLOBAL Δxdot after Run {r+1} (mean colored by std)\"\n",
    "    )\n",
    "\n",
    "# ============================\n",
    "# MEAN PLOTS ACROSS RUNS\n",
    "# ============================\n",
    "def nanpad_to(T, L):\n",
    "    out = np.full((L,), np.nan, dtype=np.float64)\n",
    "    n = min(L, len(T))\n",
    "    out[:n] = T[:n]\n",
    "    return out\n",
    "\n",
    "maxL = max([len(tr[\"train_time_step\"]) for tr in all_traces]) if len(all_traces)>0 else 0\n",
    "if maxL == 0:\n",
    "    raise RuntimeError(\"No timesteps were recorded.\")\n",
    "\n",
    "train_step_mat = np.vstack([nanpad_to(tr[\"train_time_step\"], maxL) for tr in all_traces])\n",
    "train_cum_mat  = np.vstack([nanpad_to(tr[\"train_time_cum\"], maxL) for tr in all_traces])\n",
    "pred_step_mat  = np.vstack([nanpad_to(tr[\"pred_time_step\"], maxL) for tr in all_traces])\n",
    "wall_cum_mat   = np.vstack([nanpad_to(tr[\"wall_time_cum_excl_vis\"], maxL) for tr in all_traces])\n",
    "\n",
    "train_step_mean = np.nanmean(train_step_mat, axis=0)\n",
    "train_cum_mean  = np.nanmean(train_cum_mat, axis=0)\n",
    "pred_step_mean  = np.nanmean(pred_step_mat, axis=0)\n",
    "wall_cum_mean   = np.nanmean(wall_cum_mat, axis=0)\n",
    "\n",
    "t_axis = np.arange(maxL)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(t_axis, train_step_mean)\n",
    "plt.title(\"Exact GP baseline — mean TRAIN time per timestep (non-cumulative) across runs\")\n",
    "plt.xlabel(\"global timestep (within run)\")\n",
    "plt.ylabel(\"train time at step (s)\")\n",
    "plt.grid(True, alpha=0.25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(t_axis, train_cum_mean)\n",
    "plt.title(\"Exact GP baseline — mean TRAIN time cumulative across runs\")\n",
    "plt.xlabel(\"global timestep (within run)\")\n",
    "plt.ylabel(\"cumulative train time (s)\")\n",
    "plt.grid(True, alpha=0.25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(t_axis, pred_step_mean)\n",
    "plt.title(\"Exact GP baseline — mean PRED/MPPI time per timestep across runs\")\n",
    "plt.xlabel(\"global timestep (within run)\")\n",
    "plt.ylabel(\"prediction/planning time at step (s)\")\n",
    "plt.grid(True, alpha=0.25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(t_axis, wall_cum_mean)\n",
    "plt.title(\"Exact GP baseline — mean WALL time cumulative (excluding visualization) across runs\")\n",
    "plt.xlabel(\"global timestep (within run)\")\n",
    "plt.ylabel(\"cumulative wall time excl. vis (s)\")\n",
    "plt.grid(True, alpha=0.25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n[Exact GP baseline] Summary across runs:\")\n",
    "for s in all_stats:\n",
    "    print(s)\n"
   ],
   "id": "533b21d63b9795f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================\n",
    "# Cell 9 — BASELINE: Pure Streaming OSGPR-VFE (4-head) + MPPI (GLOBAL predictors)\n",
    "#\n",
    "# ✅ Pure OSGPR-VFE baseline meaning:\n",
    "#   - No local subset, no tube, no anchors\n",
    "#   - Fixed inducing size: Z0 = Z_GLOBAL[:M_GLOBAL] for the whole run\n",
    "#   - Stream update every UPDATE_EVERY using ONLY NEW data since last update (buffer-only)\n",
    "#\n",
    "# ✅ Protocol matches your SVGP/PLSSGP evaluation style:\n",
    "#   - N_RUNS = 3\n",
    "#   - Each run = EPISODES_PER_RUN continuous episodes (models carry across episodes in a run)\n",
    "#   - BEFORE each run: full reset to the SAME initial OSGPR models + SAME Z0\n",
    "#   - One render per run (episode 0 only)\n",
    "#   - Logs per-timestep timings:\n",
    "#       * train_time_step (non-cumulative)\n",
    "#       * train_time_cum\n",
    "#       * pred_time_step (MPPI planning time)\n",
    "#       * wall_time_cum_excl_vis (excluding visualization)\n",
    "#   - Plots mean curves across runs (nan-mean)\n",
    "#   - Plots a GLOBAL surface after each run (Δxdot mean colored by std)\n",
    "# ============================\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML, display\n",
    "import plotly.graph_objects as go\n",
    "import tensorflow as tf\n",
    "import gpflow\n",
    "\n",
    "# -----------------------\n",
    "# REQUIRED symbols from earlier cells\n",
    "# -----------------------\n",
    "required = [\n",
    "    \"make_env\", \"obs_to_state\", \"wrap_pi\",\n",
    "    \"state_to_features\", \"batch_state_to_features\",\n",
    "    \"render_cartpole_frame_from_state\",\n",
    "    \"Z_GLOBAL\",\n",
    "    \"osgpr_stream_update\",\n",
    "    \"m_dx\", \"m_dxdot\", \"m_dth\", \"m_dthdot\",  # we will use these as the \"reset template\"\n",
    "]\n",
    "missing = [k for k in required if k not in globals()]\n",
    "if len(missing) > 0:\n",
    "    raise NameError(f\"Cell 9 missing required symbols from earlier cells: {missing}\")\n",
    "\n",
    "# -----------------------\n",
    "# Dtypes (float64 throughout)\n",
    "# -----------------------\n",
    "NP_DTYPE = np.float64\n",
    "TF_DTYPE = gpflow.default_float()\n",
    "if TF_DTYPE != tf.float64:\n",
    "    print(\"⚠️ gpflow.default_float() is not float64. If you want float64 everywhere, set gpflow.config.set_default_float(np.float64) in Cell 1.\")\n",
    "\n",
    "# -----------------------\n",
    "# Config defaults (prefer your globals if already defined)\n",
    "# -----------------------\n",
    "HORIZON    = int(globals().get(\"HORIZON\", 50))\n",
    "K_SAMPLES  = int(globals().get(\"K_SAMPLES\", 256))\n",
    "SIGMA      = float(globals().get(\"SIGMA\", 0.6))\n",
    "LAMBDA     = float(globals().get(\"LAMBDA\", 1.0))\n",
    "\n",
    "UPDATE_EVERY = int(globals().get(\"UPDATE_EVERY\", 30))\n",
    "OSGPR_ITERS  = int(globals().get(\"OSGPR_ITERS\", 150))\n",
    "OSGPR_LR     = float(globals().get(\"OSGPR_LR\", 0.02))\n",
    "NOISE_UPDATE = float(globals().get(\"NOISE_UPDATE\", 1e-4))\n",
    "\n",
    "M_GLOBAL = int(globals().get(\"M_GLOBAL\", 256))\n",
    "\n",
    "# task/success\n",
    "X_BAND      = float(globals().get(\"X_BAND\", 0.70))\n",
    "UPRIGHT_COS = float(globals().get(\"UPRIGHT_COS\", 0.85))\n",
    "HOLD_STEPS  = int(globals().get(\"HOLD_STEPS\", 200))\n",
    "\n",
    "# exploration schedule (same as your SVGP baseline)\n",
    "EXPLORE_STEPS = int(globals().get(\"EXPLORE_STEPS\", 200))\n",
    "UNC_W_MAX     = float(globals().get(\"UNC_W_MAX\", 15.0))\n",
    "UNC_W_MIN     = float(globals().get(\"UNC_W_MIN\", 0.0))\n",
    "CENTER_W      = float(globals().get(\"CENTER_W\", 1.0))\n",
    "U_W           = float(globals().get(\"U_W\", 0.005))\n",
    "UPRIGHT_W     = float(globals().get(\"UPRIGHT_W\", 2.0))\n",
    "\n",
    "# action bounds\n",
    "U_MIN = float(globals().get(\"U_MIN\", -1.0))\n",
    "U_MAX = float(globals().get(\"U_MAX\", +1.0))\n",
    "\n",
    "# rendering\n",
    "RECORD_RGB   = True\n",
    "RESIZE       = globals().get(\"RESIZE\", (720, 450))\n",
    "FPS          = int(globals().get(\"FPS\", 10))\n",
    "FRAME_STRIDE = int(globals().get(\"FRAME_STRIDE\", 2))\n",
    "\n",
    "# multi-run protocol\n",
    "N_RUNS = 3\n",
    "EPISODES_PER_RUN = 3\n",
    "MAX_STEPS_PER_EP = int(globals().get(\"MAX_STEPS_PER_EP\", 1000))\n",
    "START_DOWN = bool(globals().get(\"START_DOWN\", True))\n",
    "USE_GPU_MPPI = True\n",
    "WARMUP_MPPI = True\n",
    "VERBOSE = True\n",
    "\n",
    "# Optional: whether to freeze hypers in OSGPR updates\n",
    "OSGPR_FREEZE_HYPERS = bool(globals().get(\"OSGPR_FREEZE_HYPERS\", True))\n",
    "\n",
    "# -----------------------\n",
    "# Fixed plot grid (for surface after each run)\n",
    "# -----------------------\n",
    "PLOT_X_MIN, PLOT_X_MAX = globals().get(\"PLOT_X_MIN\", -2.4), globals().get(\"PLOT_X_MAX\", 2.4)\n",
    "PLOT_V_MIN, PLOT_V_MAX = globals().get(\"PLOT_V_MIN\", -3.0), globals().get(\"PLOT_V_MAX\", 3.0)\n",
    "PLOT_N_GRID            = int(globals().get(\"PLOT_N_GRID\", 60))\n",
    "PLOT_TH_FIXED          = float(globals().get(\"PLOT_TH_FIXED\", 0.0))\n",
    "PLOT_THDOT_FIXED       = float(globals().get(\"PLOT_THDOT_FIXED\", 0.0))\n",
    "PLOT_U_FIXED           = float(globals().get(\"PLOT_U_FIXED\", +1.0))\n",
    "\n",
    "def build_fixed_plot_grid():\n",
    "    xg = np.linspace(PLOT_X_MIN, PLOT_X_MAX, PLOT_N_GRID)\n",
    "    vg = np.linspace(PLOT_V_MIN, PLOT_V_MAX, PLOT_N_GRID)\n",
    "    X, V = np.meshgrid(xg, vg)\n",
    "    Xfeat_grid = np.vstack([\n",
    "        state_to_features(x, xdot, PLOT_TH_FIXED, PLOT_THDOT_FIXED, PLOT_U_FIXED)\n",
    "        for x, xdot in zip(X.ravel(), V.ravel())\n",
    "    ]).astype(np.float64)\n",
    "    return X, V, Xfeat_grid\n",
    "\n",
    "PLOT_XMESH, PLOT_VMESH, PLOT_XFEAT_GRID = build_fixed_plot_grid()\n",
    "\n",
    "def gp_predict_mu_std(model, X):\n",
    "    Xtf = tf.convert_to_tensor(np.asarray(X, dtype=np.float64), dtype=TF_DTYPE)\n",
    "    mu_tf, var_tf = model.predict_f(Xtf, full_cov=False)\n",
    "    mu = mu_tf.numpy().reshape(-1)\n",
    "    var = var_tf.numpy().reshape(-1)\n",
    "    std = np.sqrt(np.maximum(var, 1e-12))\n",
    "    return mu, std\n",
    "\n",
    "def plot_global_surface_dxdot(model_dxdot, title=\"OSGPR baseline — GLOBAL Δxdot surface\"):\n",
    "    mu, std = gp_predict_mu_std(model_dxdot, PLOT_XFEAT_GRID)\n",
    "    Mean = mu.reshape(PLOT_XMESH.shape)\n",
    "    Std  = std.reshape(PLOT_XMESH.shape)\n",
    "\n",
    "    fig = go.Figure(data=[\n",
    "        go.Surface(\n",
    "            x=PLOT_XMESH, y=PLOT_VMESH, z=Mean,\n",
    "            surfacecolor=Std,\n",
    "            colorscale=\"Viridis\",\n",
    "            showscale=True,\n",
    "            colorbar=dict(title=\"Std\"),\n",
    "            opacity=0.95\n",
    "        )\n",
    "    ])\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        scene=dict(\n",
    "            xaxis=dict(title=\"x\", range=[PLOT_X_MIN, PLOT_X_MAX]),\n",
    "            yaxis=dict(title=\"xdot\", range=[PLOT_V_MIN, PLOT_V_MAX]),\n",
    "            zaxis=dict(title=\"Δxdot\"),\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, b=0, t=50),\n",
    "        height=620\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "# -----------------------\n",
    "# Success check (same logic)\n",
    "# -----------------------\n",
    "def success_hold_update(state, x_init, hold_count):\n",
    "    x, xdot, th, thdot = state\n",
    "    upright = (np.cos(th) >= UPRIGHT_COS)\n",
    "    centered = (abs(x - x_init) <= X_BAND)\n",
    "    hold_count = (hold_count + 1) if (upright and centered) else 0\n",
    "    success = (hold_count >= HOLD_STEPS)\n",
    "    return hold_count, success, upright, centered\n",
    "\n",
    "# -----------------------\n",
    "# Cost + exploration weight (same as your SVGP baseline)\n",
    "# -----------------------\n",
    "@tf.function\n",
    "def exploration_weight_tf(t):\n",
    "    explore_steps_f = tf.cast(EXPLORE_STEPS, TF_DTYPE)\n",
    "    t_f = tf.cast(t, TF_DTYPE)\n",
    "    a = tf.clip_by_value(1.0 - t_f / tf.maximum(explore_steps_f, 1.0), 0.0, 1.0)\n",
    "    return tf.cast(UNC_W_MIN, TF_DTYPE) + (tf.cast(UNC_W_MAX, TF_DTYPE) - tf.cast(UNC_W_MIN, TF_DTYPE)) * a\n",
    "\n",
    "@tf.function\n",
    "def wrap_pi_tf(theta):\n",
    "    two_pi = tf.constant(2.0 * np.pi, dtype=TF_DTYPE)\n",
    "    pi = tf.constant(np.pi, dtype=TF_DTYPE)\n",
    "    return tf.math.floormod(theta + pi, two_pi) - pi\n",
    "\n",
    "@tf.function\n",
    "def batch_state_to_features_tf(S, U):\n",
    "    x = S[:, 0]\n",
    "    xdot = S[:, 1]\n",
    "    th = S[:, 2]\n",
    "    thdot = S[:, 3]\n",
    "\n",
    "    f0 = tf.tanh(x / tf.constant(2.4, dtype=TF_DTYPE))\n",
    "    f1 = tf.tanh(xdot / tf.constant(3.0, dtype=TF_DTYPE))\n",
    "    f2 = tf.sin(th)\n",
    "    f3 = tf.cos(th)\n",
    "    f4 = tf.tanh(thdot / tf.constant(8.0, dtype=TF_DTYPE))\n",
    "    f5 = U\n",
    "    return tf.stack([f0, f1, f2, f3, f4, f5], axis=1)\n",
    "\n",
    "@tf.function\n",
    "def stage_cost_cartpole_tf(S, U, x_init, unc_bonus=None, unc_w=0.0):\n",
    "    th = S[:, 2]\n",
    "    x  = S[:, 0]\n",
    "    c = (\n",
    "        tf.cast(UPRIGHT_W, TF_DTYPE) * (1.0 - tf.cos(th))\n",
    "        + tf.cast(CENTER_W, TF_DTYPE) * tf.square(x - x_init)\n",
    "        + tf.cast(U_W, TF_DTYPE) * tf.square(U)\n",
    "    )\n",
    "    if (unc_bonus is not None) and (unc_w > 0.0):\n",
    "        c = c - tf.cast(unc_w, TF_DTYPE) * tf.cast(unc_bonus, TF_DTYPE)\n",
    "    return c\n",
    "\n",
    "@tf.function\n",
    "def terminal_cost_hold_like_tf(S, x_init):\n",
    "    th = S[:, 2]\n",
    "    x  = S[:, 0]\n",
    "    cT = tf.zeros_like(th)\n",
    "    good = tf.logical_and(tf.cos(th) >= tf.cast(UPRIGHT_COS, TF_DTYPE),\n",
    "                          tf.abs(x - x_init) <= tf.cast(X_BAND, TF_DTYPE))\n",
    "    cT = tf.where(good, cT - tf.cast(5.0, TF_DTYPE), cT)\n",
    "    return cT\n",
    "\n",
    "# -----------------------\n",
    "# GLOBAL predictors for TF MPPI\n",
    "# -----------------------\n",
    "def make_global_tf_predictors_bundle_from_models(m_dx, m_dxdot, m_dth, m_dthdot):\n",
    "    @tf.function\n",
    "    def dx_mu(Xfeat):\n",
    "        mu, _ = m_dx.predict_f(Xfeat, full_cov=False)\n",
    "        return tf.reshape(mu, (-1,))\n",
    "\n",
    "    @tf.function\n",
    "    def dxdot_mu(Xfeat):\n",
    "        mu, _ = m_dxdot.predict_f(Xfeat, full_cov=False)\n",
    "        return tf.reshape(mu, (-1,))\n",
    "\n",
    "    @tf.function\n",
    "    def dth_mu(Xfeat):\n",
    "        mu, _ = m_dth.predict_f(Xfeat, full_cov=False)\n",
    "        return tf.reshape(mu, (-1,))\n",
    "\n",
    "    @tf.function\n",
    "    def dthdot_mu(Xfeat):\n",
    "        mu, _ = m_dthdot.predict_f(Xfeat, full_cov=False)\n",
    "        return tf.reshape(mu, (-1,))\n",
    "\n",
    "    @tf.function\n",
    "    def dxdot_var(Xfeat):\n",
    "        _, v = m_dxdot.predict_f(Xfeat, full_cov=False)\n",
    "        return tf.reshape(v, (-1,))\n",
    "\n",
    "    return (dx_mu, dxdot_mu, dth_mu, dthdot_mu), dxdot_var\n",
    "\n",
    "@tf.function\n",
    "def gp_dynamics_step_batch_tf(S, U, pred_bundle_tf):\n",
    "    pred_dx_tf, pred_dxdot_tf, pred_dth_tf, pred_dthdot_tf = pred_bundle_tf\n",
    "    Xfeat = batch_state_to_features_tf(S, U)\n",
    "    dx     = pred_dx_tf(Xfeat)\n",
    "    dxdot  = pred_dxdot_tf(Xfeat)\n",
    "    dth    = pred_dth_tf(Xfeat)\n",
    "    dthdot = pred_dthdot_tf(Xfeat)\n",
    "\n",
    "    S2 = tf.stack([\n",
    "        S[:, 0] + dx,\n",
    "        S[:, 1] + dxdot,\n",
    "        wrap_pi_tf(S[:, 2] + dth),\n",
    "        S[:, 3] + dthdot\n",
    "    ], axis=1)\n",
    "    return S2, Xfeat\n",
    "\n",
    "@tf.function\n",
    "def mppi_plan_gpu_global_tf(state0, x_init, u_mean0, t_global,\n",
    "                           pred_bundle_tf, unc_var_fn_tf,\n",
    "                           horizon=HORIZON, K=K_SAMPLES, sigma=SIGMA, lam=LAMBDA,\n",
    "                           base_seed=0):\n",
    "    H = tf.cast(horizon, tf.int32)\n",
    "    Kt = tf.cast(K, tf.int32)\n",
    "\n",
    "    seed = tf.stack([tf.cast(base_seed, tf.int32), tf.cast(t_global, tf.int32)], axis=0)\n",
    "    eps = tf.random.stateless_normal((Kt, H), seed=seed, mean=0.0, stddev=tf.cast(sigma, TF_DTYPE), dtype=TF_DTYPE)\n",
    "    U = tf.clip_by_value(u_mean0[None, :] + eps, tf.cast(U_MIN, TF_DTYPE), tf.cast(U_MAX, TF_DTYPE))\n",
    "\n",
    "    S = tf.tile(state0[None, :], [Kt, 1])\n",
    "    total_cost = tf.zeros((Kt,), dtype=TF_DTYPE)\n",
    "\n",
    "    unc_w = exploration_weight_tf(t_global)\n",
    "\n",
    "    t = tf.constant(0, dtype=tf.int32)\n",
    "    def cond(t, S, total_cost): return t < H\n",
    "\n",
    "    def body(t, S, total_cost):\n",
    "        Ut = U[:, t]\n",
    "        S2, Xfeat = gp_dynamics_step_batch_tf(S, Ut, pred_bundle_tf)\n",
    "        unc_bonus = unc_var_fn_tf(Xfeat)\n",
    "        total_cost = total_cost + stage_cost_cartpole_tf(\n",
    "            S, Ut, x_init=tf.cast(x_init, TF_DTYPE),\n",
    "            unc_bonus=unc_bonus, unc_w=unc_w\n",
    "        )\n",
    "        return t+1, S2, total_cost\n",
    "\n",
    "    _, S, total_cost = tf.while_loop(cond, body, [t, S, total_cost], parallel_iterations=1)\n",
    "    total_cost = total_cost + terminal_cost_hold_like_tf(S, x_init=tf.cast(x_init, TF_DTYPE))\n",
    "\n",
    "    cmin = tf.reduce_min(total_cost)\n",
    "    w = tf.exp(-(total_cost - cmin) / tf.cast(lam, TF_DTYPE))\n",
    "    wsum = tf.reduce_sum(w) + tf.cast(1e-12, TF_DTYPE)\n",
    "\n",
    "    u_mean = u_mean0 + tf.reduce_sum(w[:, None] * eps, axis=0) / wsum\n",
    "    u_mean = tf.clip_by_value(u_mean, tf.cast(U_MIN, TF_DTYPE), tf.cast(U_MAX, TF_DTYPE))\n",
    "    return u_mean[0], u_mean, unc_w\n",
    "\n",
    "def mppi_plan_gpu_global(state, x_init, u_init, t_global, pred_bundle_tf, unc_var_fn_tf, base_seed=0):\n",
    "    state0 = tf.convert_to_tensor(np.asarray(state, dtype=np.float64).reshape(4,), dtype=TF_DTYPE)\n",
    "    x0     = tf.convert_to_tensor(float(x_init), dtype=TF_DTYPE)\n",
    "    u0     = tf.convert_to_tensor(np.asarray(u_init, dtype=np.float64).reshape(-1,), dtype=TF_DTYPE)\n",
    "\n",
    "    dev = \"/GPU:0\" if len(tf.config.list_logical_devices(\"GPU\")) > 0 else \"/CPU:0\"\n",
    "    with tf.device(dev):\n",
    "        u_first, u_mean, unc_w = mppi_plan_gpu_global_tf(\n",
    "            state0, x0, u0, tf.convert_to_tensor(int(t_global), dtype=tf.int32),\n",
    "            pred_bundle_tf=pred_bundle_tf,\n",
    "            unc_var_fn_tf=unc_var_fn_tf,\n",
    "            horizon=HORIZON, K=K_SAMPLES, sigma=SIGMA, lam=LAMBDA,\n",
    "            base_seed=int(base_seed)\n",
    "        )\n",
    "    return float(u_first.numpy()), u_mean.numpy(), float(unc_w.numpy())\n",
    "\n",
    "# -----------------------\n",
    "# Rendering helper\n",
    "# -----------------------\n",
    "def _render_frame_from_state(s):\n",
    "    W, H = int(RESIZE[0]), int(RESIZE[1])\n",
    "    return render_cartpole_frame_from_state(x=s[0], theta=s[2], x_threshold=2.4, W=W, H=H)\n",
    "\n",
    "# -----------------------\n",
    "# OSGPR update wrapper (fixed Z)\n",
    "# -----------------------\n",
    "def osgpr_update_fixedZ(model_old, Xnew, Ynew, Z0, iters, lr, freeze_hypers):\n",
    "    return osgpr_stream_update(\n",
    "        model_old,\n",
    "        np.asarray(Xnew, dtype=np.float64),\n",
    "        np.asarray(Ynew, dtype=np.float64),\n",
    "        np.asarray(Z0, dtype=np.float64),\n",
    "        iters=int(iters),\n",
    "        lr=float(lr),\n",
    "        freeze_hypers=bool(freeze_hypers),\n",
    "    )\n",
    "\n",
    "# ============================\n",
    "# IMPORTANT: snapshot \"initial models\" NOW as the reset template\n",
    "#   This guarantees each run starts from the same parameters.\n",
    "# ============================\n",
    "_BASE_m_dx     = copy.deepcopy(m_dx)\n",
    "_BASE_m_dxdot  = copy.deepcopy(m_dxdot)\n",
    "_BASE_m_dth    = copy.deepcopy(m_dth)\n",
    "_BASE_m_dthdot = copy.deepcopy(m_dthdot)\n",
    "\n",
    "# ============================\n",
    "# RUNNER: one run = 3 continuous episodes\n",
    "# ============================\n",
    "def run_one_run_osgpr_pure(run_id, seed_base=0):\n",
    "    # fixed inducing\n",
    "    Zg = np.asarray(Z_GLOBAL, dtype=np.float64)\n",
    "    if Zg.shape[0] < M_GLOBAL:\n",
    "        raise ValueError(f\"Z_GLOBAL has {Zg.shape[0]} points but M_GLOBAL={M_GLOBAL}. Provide at least {M_GLOBAL} points.\")\n",
    "    Z0 = Zg[:M_GLOBAL].copy()\n",
    "\n",
    "    # reset models for this run\n",
    "    m_dx     = copy.deepcopy(_BASE_m_dx)\n",
    "    m_dxdot  = copy.deepcopy(_BASE_m_dxdot)\n",
    "    m_dth    = copy.deepcopy(_BASE_m_dth)\n",
    "    m_dthdot = copy.deepcopy(_BASE_m_dthdot)\n",
    "\n",
    "    # fixed noise (optional): keep identical across methods\n",
    "    try:\n",
    "        m_dx.likelihood.variance.assign(float(NOISE_UPDATE))\n",
    "        m_dxdot.likelihood.variance.assign(float(NOISE_UPDATE))\n",
    "        m_dth.likelihood.variance.assign(float(NOISE_UPDATE))\n",
    "        m_dthdot.likelihood.variance.assign(float(NOISE_UPDATE))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # per-update buffer (streaming)\n",
    "    Xbuf, ydx_buf, ydxdot_buf, ydth_buf, ydthdot_buf = [], [], [], [], []\n",
    "\n",
    "    # timing traces per timestep\n",
    "    train_time_step = []\n",
    "    pred_time_step  = []\n",
    "    wall_time_cum_excl_vis = []\n",
    "    train_time_cum = []\n",
    "    pred_time_cum  = []\n",
    "\n",
    "    # one render per run (episode 0 only)\n",
    "    frames = []\n",
    "    html = None\n",
    "    vis_time_s_total = 0.0\n",
    "\n",
    "    # global counters\n",
    "    t_global = 0\n",
    "    total_reward_run = 0.0\n",
    "    updates_run = 0\n",
    "\n",
    "    # predictor bundle\n",
    "    pred_bundle_tf, unc_var_fn_tf = make_global_tf_predictors_bundle_from_models(m_dx, m_dxdot, m_dth, m_dthdot)\n",
    "\n",
    "    # warmup (compile)\n",
    "    if USE_GPU_MPPI and WARMUP_MPPI:\n",
    "        s_dummy = np.array([0.0, 0.0, np.pi, 0.0], dtype=np.float64)\n",
    "        u_mean0 = np.zeros((HORIZON,), dtype=np.float64)\n",
    "        _ = mppi_plan_gpu_global(\n",
    "            s_dummy, x_init=0.0, u_init=u_mean0, t_global=0,\n",
    "            pred_bundle_tf=pred_bundle_tf, unc_var_fn_tf=unc_var_fn_tf,\n",
    "            base_seed=seed_base + 1000*run_id\n",
    "        )\n",
    "\n",
    "    t_wall_start = time.perf_counter()\n",
    "    train_cum = 0.0\n",
    "    pred_cum  = 0.0\n",
    "\n",
    "    for ep in range(EPISODES_PER_RUN):\n",
    "        seed = int(seed_base + 1000 * run_id + ep)\n",
    "        env = make_env(render_mode=None, seed=seed, max_episode_steps=MAX_STEPS_PER_EP, start_down=START_DOWN)\n",
    "        obs, info = env.reset(seed=seed)\n",
    "        s = np.array(obs_to_state(obs), dtype=np.float64)\n",
    "        x_init = float(s[0])\n",
    "\n",
    "        u_mean = np.zeros((HORIZON,), dtype=np.float64)\n",
    "        hold_count = 0\n",
    "        record_this_ep = (ep == 0) and RECORD_RGB\n",
    "\n",
    "        if VERBOSE:\n",
    "            print(f\"\\n[OSGPR-PURE] Run {run_id+1}/{N_RUNS} | Episode {ep+1}/{EPISODES_PER_RUN} | seed={seed}\")\n",
    "\n",
    "        for t_ep in range(MAX_STEPS_PER_EP):\n",
    "            # ---- planning time ----\n",
    "            t0 = time.perf_counter()\n",
    "            u0, u_mean, unc_w = mppi_plan_gpu_global(\n",
    "                state=s, x_init=x_init, u_init=u_mean, t_global=t_global,\n",
    "                pred_bundle_tf=pred_bundle_tf, unc_var_fn_tf=unc_var_fn_tf,\n",
    "                base_seed=seed_base + 1000*run_id\n",
    "            )\n",
    "            pred_dt = time.perf_counter() - t0\n",
    "            pred_cum += pred_dt\n",
    "\n",
    "            # ---- env step ----\n",
    "            obs2, r, terminated, truncated, info = env.step(np.array([u0], dtype=np.float32))\n",
    "            s2 = np.array(obs_to_state(obs2), dtype=np.float64)\n",
    "            total_reward_run += float(r)\n",
    "\n",
    "            # ---- collect buffer (skip respawned) ----\n",
    "            respawned = bool(isinstance(info, dict) and info.get(\"respawned\", False))\n",
    "            if not respawned:\n",
    "                Xbuf.append(state_to_features(s[0], s[1], s[2], s[3], float(u0)))\n",
    "                ydx_buf.append([s2[0] - s[0]])\n",
    "                ydxdot_buf.append([s2[1] - s[1]])\n",
    "                ydth_buf.append([wrap_pi(s2[2] - s[2])])\n",
    "                ydthdot_buf.append([s2[3] - s[3]])\n",
    "\n",
    "            # ---- render (excluded) ----\n",
    "            if record_this_ep and (t_global % FRAME_STRIDE == 0):\n",
    "                tv0 = time.perf_counter()\n",
    "                try:\n",
    "                    frames.append(_render_frame_from_state(s2))\n",
    "                except Exception:\n",
    "                    pass\n",
    "                vis_time_s_total += (time.perf_counter() - tv0)\n",
    "\n",
    "            # ---- success ----\n",
    "            hold_count, success, upright, centered = success_hold_update(s2, x_init, hold_count)\n",
    "            if VERBOSE and (t_global % 50 == 0):\n",
    "                print(f\"[t={t_global:04d}] u0={u0:+.2f} unc_w={unc_w:.2f} upright={upright} centered={centered} hold={hold_count}/{HOLD_STEPS}\")\n",
    "\n",
    "            # ---- streaming OSGPR update (buffer only) ----\n",
    "            train_dt = 0.0\n",
    "            if ((t_global + 1) % UPDATE_EVERY == 0) and (len(Xbuf) >= 10):\n",
    "                updates_run += 1\n",
    "\n",
    "                Xnew = np.asarray(Xbuf, dtype=np.float64)\n",
    "                ydx  = np.asarray(ydx_buf, dtype=np.float64)\n",
    "                ydxd = np.asarray(ydxdot_buf, dtype=np.float64)\n",
    "                ydth = np.asarray(ydth_buf, dtype=np.float64)\n",
    "                ydthd= np.asarray(ydthdot_buf, dtype=np.float64)\n",
    "\n",
    "                tt0 = time.perf_counter()\n",
    "                m_dx     = osgpr_update_fixedZ(m_dx,     Xnew, ydx,  Z0, iters=OSGPR_ITERS, lr=OSGPR_LR, freeze_hypers=OSGPR_FREEZE_HYPERS)\n",
    "                m_dxdot  = osgpr_update_fixedZ(m_dxdot,  Xnew, ydxd, Z0, iters=OSGPR_ITERS, lr=OSGPR_LR, freeze_hypers=OSGPR_FREEZE_HYPERS)\n",
    "                m_dth    = osgpr_update_fixedZ(m_dth,    Xnew, ydth, Z0, iters=OSGPR_ITERS, lr=OSGPR_LR, freeze_hypers=OSGPR_FREEZE_HYPERS)\n",
    "                m_dthdot = osgpr_update_fixedZ(m_dthdot, Xnew, ydthd,Z0, iters=OSGPR_ITERS, lr=OSGPR_LR, freeze_hypers=OSGPR_FREEZE_HYPERS)\n",
    "                train_dt = time.perf_counter() - tt0\n",
    "                train_cum += train_dt\n",
    "\n",
    "                # clear buffers\n",
    "                Xbuf.clear(); ydx_buf.clear(); ydxdot_buf.clear(); ydth_buf.clear(); ydthdot_buf.clear()\n",
    "\n",
    "                # refresh predictor bundle\n",
    "                pred_bundle_tf, unc_var_fn_tf = make_global_tf_predictors_bundle_from_models(m_dx, m_dxdot, m_dth, m_dthdot)\n",
    "\n",
    "            # ---- timing logs ----\n",
    "            total_wall = time.perf_counter() - t_wall_start\n",
    "            wall_excl_vis = max(total_wall - vis_time_s_total, 0.0)\n",
    "\n",
    "            train_time_step.append(float(train_dt))\n",
    "            pred_time_step.append(float(pred_dt))\n",
    "            train_time_cum.append(float(train_cum))\n",
    "            pred_time_cum.append(float(pred_cum))\n",
    "            wall_time_cum_excl_vis.append(float(wall_excl_vis))\n",
    "\n",
    "            # advance\n",
    "            s = s2\n",
    "            t_global += 1\n",
    "\n",
    "            if success:\n",
    "                if VERBOSE:\n",
    "                    print(f\"✅ SUCCESS at global t={t_global-1} (held {HOLD_STEPS} steps)\")\n",
    "                break\n",
    "            if terminated or truncated:\n",
    "                if VERBOSE:\n",
    "                    print(f\"Episode ended at global t={t_global-1} (terminated={terminated}, truncated={truncated})\")\n",
    "                break\n",
    "\n",
    "        env.close()\n",
    "\n",
    "    # ---- one render per run ----\n",
    "    if RECORD_RGB and (len(frames) > 0):\n",
    "        tv0 = time.perf_counter()\n",
    "        fig = plt.figure(figsize=(RESIZE[0]/100, RESIZE[1]/100), dpi=100)\n",
    "        plt.axis(\"off\")\n",
    "        im = plt.imshow(frames[0])\n",
    "\n",
    "        def animate_fn(i):\n",
    "            im.set_data(frames[i])\n",
    "            return [im]\n",
    "\n",
    "        ani = animation.FuncAnimation(fig, animate_fn, frames=len(frames),\n",
    "                                      interval=1000 / float(FPS), blit=True)\n",
    "        plt.close(fig)\n",
    "        html = HTML(ani.to_jshtml())\n",
    "        display(html)\n",
    "        vis_time_s_total += (time.perf_counter() - tv0)\n",
    "\n",
    "    run_stats = dict(\n",
    "        run_id=int(run_id),\n",
    "        total_reward=float(total_reward_run),\n",
    "        total_steps=int(len(train_time_step)),\n",
    "        updates=int(updates_run),\n",
    "        vis_time_s=float(vis_time_s_total),\n",
    "        wall_excl_vis_s=float(wall_time_cum_excl_vis[-1] if len(wall_time_cum_excl_vis)>0 else 0.0),\n",
    "        train_cum_s=float(train_time_cum[-1] if len(train_time_cum)>0 else 0.0),\n",
    "        pred_cum_s=float(pred_time_cum[-1] if len(pred_time_cum)>0 else 0.0),\n",
    "    )\n",
    "\n",
    "    traces = dict(\n",
    "        train_time_step=np.asarray(train_time_step, dtype=np.float64),\n",
    "        train_time_cum=np.asarray(train_time_cum, dtype=np.float64),\n",
    "        pred_time_step=np.asarray(pred_time_step, dtype=np.float64),\n",
    "        pred_time_cum=np.asarray(pred_time_cum, dtype=np.float64),\n",
    "        wall_time_cum_excl_vis=np.asarray(wall_time_cum_excl_vis, dtype=np.float64),\n",
    "    )\n",
    "\n",
    "    models = dict(m_dx=m_dx, m_dxdot=m_dxdot, m_dth=m_dth, m_dthdot=m_dthdot)\n",
    "    return run_stats, traces, models, html\n",
    "\n",
    "# ============================\n",
    "# MULTI-RUN DRIVER\n",
    "# ============================\n",
    "all_stats = []\n",
    "all_traces = []\n",
    "all_models = []\n",
    "\n",
    "for r in range(N_RUNS):\n",
    "    stats_r, traces_r, models_r, html_r = run_one_run_osgpr_pure(run_id=r, seed_base=0)\n",
    "    all_stats.append(stats_r)\n",
    "    all_traces.append(traces_r)\n",
    "    all_models.append(models_r)\n",
    "\n",
    "    print(f\"\\n[OSGPR-PURE] Run {r+1} stats:\", stats_r)\n",
    "\n",
    "    # surface after each run\n",
    "    plot_global_surface_dxdot(\n",
    "        models_r[\"m_dxdot\"],\n",
    "        title=f\"OSGPR-PURE baseline — GLOBAL Δxdot after Run {r+1} (mean colored by std)\"\n",
    "    )\n",
    "\n",
    "# ============================\n",
    "# MEAN PLOTS ACROSS RUNS (nan-mean over variable lengths)\n",
    "# ============================\n",
    "def nanpad_to(T, L):\n",
    "    out = np.full((L,), np.nan, dtype=np.float64)\n",
    "    n = min(L, len(T))\n",
    "    out[:n] = T[:n]\n",
    "    return out\n",
    "\n",
    "maxL = max([len(tr[\"train_time_step\"]) for tr in all_traces]) if len(all_traces)>0 else 0\n",
    "if maxL == 0:\n",
    "    raise RuntimeError(\"No timesteps were recorded (did the env terminate immediately?).\")\n",
    "\n",
    "train_step_mat = np.vstack([nanpad_to(tr[\"train_time_step\"], maxL) for tr in all_traces])\n",
    "train_cum_mat  = np.vstack([nanpad_to(tr[\"train_time_cum\"], maxL) for tr in all_traces])\n",
    "pred_step_mat  = np.vstack([nanpad_to(tr[\"pred_time_step\"], maxL) for tr in all_traces])\n",
    "wall_cum_mat   = np.vstack([nanpad_to(tr[\"wall_time_cum_excl_vis\"], maxL) for tr in all_traces])\n",
    "\n",
    "train_step_mean = np.nanmean(train_step_mat, axis=0)\n",
    "train_cum_mean  = np.nanmean(train_cum_mat, axis=0)\n",
    "pred_step_mean  = np.nanmean(pred_step_mat, axis=0)\n",
    "wall_cum_mean   = np.nanmean(wall_cum_mat, axis=0)\n",
    "\n",
    "t_axis = np.arange(maxL)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(t_axis, train_step_mean)\n",
    "plt.title(\"OSGPR-PURE baseline — mean TRAIN time per timestep (non-cumulative) across runs\")\n",
    "plt.xlabel(\"global timestep (within run)\")\n",
    "plt.ylabel(\"train time at step (s)\")\n",
    "plt.grid(True, alpha=0.25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(t_axis, train_cum_mean)\n",
    "plt.title(\"OSGPR-PURE baseline — mean TRAIN time cumulative across runs\")\n",
    "plt.xlabel(\"global timestep (within run)\")\n",
    "plt.ylabel(\"cumulative train time (s)\")\n",
    "plt.grid(True, alpha=0.25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(t_axis, pred_step_mean)\n",
    "plt.title(\"OSGPR-PURE baseline — mean PRED/MPPI time per timestep across runs\")\n",
    "plt.xlabel(\"global timestep (within run)\")\n",
    "plt.ylabel(\"prediction/planning time at step (s)\")\n",
    "plt.grid(True, alpha=0.25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(t_axis, wall_cum_mean)\n",
    "plt.title(\"OSGPR-PURE baseline — mean WALL time cumulative (excluding visualization)\")\n",
    "plt.xlabel(\"global timestep (within run)\")\n",
    "plt.ylabel(\"cumulative wall time excl. vis (s)\")\n",
    "plt.grid(True, alpha=0.25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n[OSGPR-PURE baseline] Summary across runs:\")\n",
    "for s in all_stats:\n",
    "    print(s)\n"
   ],
   "id": "60ef6aca073e6151",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "13fec251cc6ac4d8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
